<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="es" xml:lang="es"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Modelos lineales generalizados</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="02_GLM_files/libs/clipboard/clipboard.min.js"></script>
<script src="02_GLM_files/libs/quarto-html/quarto.js"></script>
<script src="02_GLM_files/libs/quarto-html/popper.min.js"></script>
<script src="02_GLM_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="02_GLM_files/libs/quarto-html/anchor.min.js"></script>
<link href="02_GLM_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="02_GLM_files/libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="02_GLM_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="02_GLM_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="02_GLM_files/libs/bootstrap/bootstrap-a1ff8711b79ae3724c050874b28d9907.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="style.css">
</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Tabla de contenidos</h2>
   
  <ul>
  <li><a href="#la-familia-exponencial" id="toc-la-familia-exponencial" class="nav-link active" data-scroll-target="#la-familia-exponencial"><span class="header-section-number">1</span> La familia exponencial</a>
  <ul class="collapse">
  <li><a href="#ejemplos" id="toc-ejemplos" class="nav-link" data-scroll-target="#ejemplos"><span class="header-section-number">1.1</span> Ejemplos</a>
  <ul class="collapse">
  <li><a href="#distribución-bernoulli" id="toc-distribución-bernoulli" class="nav-link" data-scroll-target="#distribución-bernoulli"><span class="header-section-number">1.1.1</span> Distribución Bernoulli</a></li>
  <li><a href="#distribución-gaussiana" id="toc-distribución-gaussiana" class="nav-link" data-scroll-target="#distribución-gaussiana"><span class="header-section-number">1.1.2</span> Distribución Gaussiana</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#construcción-de-glms" id="toc-construcción-de-glms" class="nav-link" data-scroll-target="#construcción-de-glms"><span class="header-section-number">2</span> Construcción de GLMs</a>
  <ul class="collapse">
  <li><a href="#mínimos-cuadrados-ordinarios" id="toc-mínimos-cuadrados-ordinarios" class="nav-link" data-scroll-target="#mínimos-cuadrados-ordinarios"><span class="header-section-number">2.1</span> Mínimos Cuadrados Ordinarios</a></li>
  <li><a href="#ejemplo-en-python-mínimos-cuadrados-ordinarios" id="toc-ejemplo-en-python-mínimos-cuadrados-ordinarios" class="nav-link" data-scroll-target="#ejemplo-en-python-mínimos-cuadrados-ordinarios"><span class="header-section-number">2.2</span> Ejemplo en Python: Mínimos Cuadrados Ordinarios</a></li>
  <li><a href="#regresión-logística" id="toc-regresión-logística" class="nav-link" data-scroll-target="#regresión-logística"><span class="header-section-number">2.3</span> Regresión Logística</a>
  <ul class="collapse">
  <li><a href="#terminología" id="toc-terminología" class="nav-link" data-scroll-target="#terminología"><span class="header-section-number">2.3.1</span> Terminología</a></li>
  </ul></li>
  <li><a href="#ejemplo-en-python-regresión-logística" id="toc-ejemplo-en-python-regresión-logística" class="nav-link" data-scroll-target="#ejemplo-en-python-regresión-logística"><span class="header-section-number">2.4</span> Ejemplo en Python: Regresión Logística</a></li>
  <li><a href="#regresión-softmax" id="toc-regresión-softmax" class="nav-link" data-scroll-target="#regresión-softmax"><span class="header-section-number">2.5</span> Regresión Softmax</a>
  <ul class="collapse">
  <li><a href="#contexto" id="toc-contexto" class="nav-link" data-scroll-target="#contexto"><span class="header-section-number">2.5.1</span> Contexto</a></li>
  <li><a href="#la-multinomial-como-distribución-de-la-familia-exponencial" id="toc-la-multinomial-como-distribución-de-la-familia-exponencial" class="nav-link" data-scroll-target="#la-multinomial-como-distribución-de-la-familia-exponencial"><span class="header-section-number">2.5.2</span> La Multinomial como Distribución de la Familia Exponencial</a></li>
  <li><a href="#función-de-enlace-y-función-de-respuesta" id="toc-función-de-enlace-y-función-de-respuesta" class="nav-link" data-scroll-target="#función-de-enlace-y-función-de-respuesta"><span class="header-section-number">2.5.3</span> Función de Enlace y Función de Respuesta</a></li>
  </ul></li>
  <li><a href="#softmax-function-y-modelo-de-clasificación" id="toc-softmax-function-y-modelo-de-clasificación" class="nav-link" data-scroll-target="#softmax-function-y-modelo-de-clasificación"><span class="header-section-number">2.6</span> Softmax Function y Modelo de Clasificación</a>
  <ul class="collapse">
  <li><a href="#hipótesis-del-modelo" id="toc-hipótesis-del-modelo" class="nav-link" data-scroll-target="#hipótesis-del-modelo"><span class="header-section-number">2.6.1</span> Hipótesis del Modelo</a></li>
  </ul></li>
  <li><a href="#ajuste-de-parámetros" id="toc-ajuste-de-parámetros" class="nav-link" data-scroll-target="#ajuste-de-parámetros"><span class="header-section-number">2.7</span> Ajuste de Parámetros</a>
  <ul class="collapse">
  <li><a href="#ejemplo-práctico-en-python" id="toc-ejemplo-práctico-en-python" class="nav-link" data-scroll-target="#ejemplo-práctico-en-python"><span class="header-section-number">2.7.1</span> Ejemplo Práctico en Python</a></li>
  </ul></li>
  <li><a href="#ejemplo-poisson" id="toc-ejemplo-poisson" class="nav-link" data-scroll-target="#ejemplo-poisson"><span class="header-section-number">2.8</span> Ejemplo Poisson</a></li>
  </ul></li>
  <li><a href="#ajuste-de-parámetros-en-modelos-lineales-generalizados" id="toc-ajuste-de-parámetros-en-modelos-lineales-generalizados" class="nav-link" data-scroll-target="#ajuste-de-parámetros-en-modelos-lineales-generalizados"><span class="header-section-number">3</span> Ajuste de Parámetros en Modelos Lineales Generalizados</a>
  <ul class="collapse">
  <li><a href="#el-principio-de-máxima-verosimilitud" id="toc-el-principio-de-máxima-verosimilitud" class="nav-link" data-scroll-target="#el-principio-de-máxima-verosimilitud"><span class="header-section-number">3.1</span> El Principio de Máxima Verosimilitud</a></li>
  <li><a href="#el-método-de-newton-raphson" id="toc-el-método-de-newton-raphson" class="nav-link" data-scroll-target="#el-método-de-newton-raphson"><span class="header-section-number">3.2</span> El Método de Newton-Raphson</a></li>
  <li><a href="#casos-específicos-de-glms" id="toc-casos-específicos-de-glms" class="nav-link" data-scroll-target="#casos-específicos-de-glms"><span class="header-section-number">3.3</span> Casos Específicos de GLMs</a>
  <ul class="collapse">
  <li><a href="#regresión-lineal-distribución-gaussiana" id="toc-regresión-lineal-distribución-gaussiana" class="nav-link" data-scroll-target="#regresión-lineal-distribución-gaussiana"><span class="header-section-number">3.3.1</span> Regresión Lineal (Distribución Gaussiana)</a></li>
  <li><a href="#regresión-logística-distribución-bernoulli" id="toc-regresión-logística-distribución-bernoulli" class="nav-link" data-scroll-target="#regresión-logística-distribución-bernoulli"><span class="header-section-number">3.3.2</span> Regresión Logística (Distribución Bernoulli)</a></li>
  <li><a href="#regresión-softmax-distribución-multinomial" id="toc-regresión-softmax-distribución-multinomial" class="nav-link" data-scroll-target="#regresión-softmax-distribución-multinomial"><span class="header-section-number">3.3.3</span> Regresión Softmax (Distribución Multinomial)</a></li>
  <li><a href="#regresión-poisson" id="toc-regresión-poisson" class="nav-link" data-scroll-target="#regresión-poisson"><span class="header-section-number">3.3.4</span> Regresión Poisson</a></li>
  </ul></li>
  <li><a href="#conclusión" id="toc-conclusión" class="nav-link" data-scroll-target="#conclusión"><span class="header-section-number">3.4</span> Conclusión</a></li>
  </ul></li>
  </ul>
</nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Modelos lineales generalizados</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Código</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Mostrar todo el código</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Ocultar todo el código</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">Ver el código fuente</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="hidden">
<p><span class="math display">\[
\def\RR{\mathbb{R}}
\def\media{\mathbb{E}}
\def\xx{{\bf x}}
\def\XX{{\bf X}}
\def\TT{{\bf T}}
\def\bftheta{\boldsymbol{\theta}}
\def\bfeta{\boldsymbol{\eta}}
   \]</span></p>
</div>
<p>Tanto en el modelo de regresión lineal como en el de regresión logística, asumimos una cierta distribución condicional para <span class="math inline">\(Y|\XX\)</span> que depende de un conjunto de parámetros <span class="math inline">\(\bftheta\)</span>.</p>
<ul>
<li><p>En regresión lineal, suponemos <span class="math inline">\(Y|\XX\sim\mathcal{N}(\mu,\sigma^2)\)</span> con <span class="math inline">\(\mu=\bftheta^T\XX\)</span>.</p></li>
<li><p>En regresión logística, que recordemos está pensado para problemas de clasificación, suponemos <span class="math inline">\(Y|\XX\sim\text{Bernoulli}(\phi)\)</span> con <span class="math inline">\(\phi=g(\bftheta^T\XX)\)</span>, donde <span class="math inline">\(g(z)=1/(1+e^{-z})\)</span>.</p></li>
</ul>
<p>En esta sección mostraremos que ambos métodos son casos especiales de una familia más amplia de modelos, llamados <strong>Modelos Lineales Generalizados (GLMs)</strong>. También mostraremos cómo otros modelos en GLM pueden derivarse y aplicarse a otros problemas de regresión y clasificación.</p>
<section id="la-familia-exponencial" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> La familia exponencial</h1>
<div class="definicion">
<p><strong>Definición:</strong> Decimos que una clase de distribuciones pertenece a la familia exponencial si se puede escribir en la forma</p>
<p><span class="math display">\[p(y; \bfeta) = b(y) \exp\left(\bfeta^T \TT(y) - a(\bfeta)\right)\]</span></p>
</div>
<ul>
<li><span class="math inline">\(\bfeta\)</span> se llama <strong>parámetro natural</strong> de la distribución.</li>
<li><span class="math inline">\(\TT(y)\)</span> es el <strong>estadístico suficiente</strong>.</li>
<li><span class="math inline">\(a(\bfeta)\)</span> es la <strong>función de partición logarítmica</strong> que normaliza la expresión para que la distribución sea válida (es decir, integre a 1).</li>
</ul>
<p>Una elección fija de <span class="math inline">\(\TT\)</span>, <span class="math inline">\(a\)</span> y <span class="math inline">\(b\)</span> define un conjunto de distribuciones parametrizadas por <span class="math inline">\(\bfeta\)</span>, de manera que al variar <span class="math inline">\(\bfeta\)</span> obtenemos diferentes distribuciones dentro de esta familia.</p>
<section id="ejemplos" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="ejemplos"><span class="header-section-number">1.1</span> Ejemplos</h2>
<p>Mostraremos que las distribuciones Bernoulli y Normal son ejemplos de distribuciones de la familia exponencial.</p>
<section id="distribución-bernoulli" class="level3" data-number="1.1.1">
<h3 data-number="1.1.1" class="anchored" data-anchor-id="distribución-bernoulli"><span class="header-section-number">1.1.1</span> Distribución Bernoulli</h3>
<p>Sea <span class="math inline">\(Y\in\{0,1\}\)</span> tal que <span class="math inline">\(Y\sim\text{Bernoulli}(\phi)\)</span>. Entonces</p>
<p><span class="math display">\[
\begin{align*}
p(y; \phi) &amp;= \phi^y (1 - \phi)^{1-y} \\
&amp; = \exp\left( y \log\phi + (1-y)\log(1-\phi) \right) \\
&amp; = \exp\left(\log\left(\frac{\phi}{1-\phi}\right)y+\log(1-\phi)\right) \\
&amp; = \exp\left(\log\left(\frac{\phi}{1-\phi}\right)y+\log(1-\phi)\right)
\end{align*}
\]</span></p>
<p>Reconocemos el parámetro natural <span class="math display">\[\eta=\log\left(\frac{\phi}{1-\phi}\right)\]</span></p>
<p>mientras que el resto de los elementos son: <span class="math display">\[T(y)=y, \qquad a(\eta)=-\log(1-\phi)=\log(1+e^\eta), \qquad b(y)=1.\]</span></p>
</section>
<section id="distribución-gaussiana" class="level3" data-number="1.1.2">
<h3 data-number="1.1.2" class="anchored" data-anchor-id="distribución-gaussiana"><span class="header-section-number">1.1.2</span> Distribución Gaussiana</h3>
<p>Sea <span class="math inline">\(Y\in\RR\)</span> tal que <span class="math inline">\(Y\sim\mathcal{N}(\mu,\sigma^2)\)</span>. Entonces</p>
<p><span class="math display">\[
\begin{align*}
p(y; \mu, \sigma^2) &amp; = \frac{1}{\sqrt{2\pi}\sigma} \exp\left(-\frac{(y - \mu)^2}{2\sigma^2}\right) \\
&amp; = \frac{1}{\sqrt{2\pi}}\exp\left(-\frac{1}{2\sigma^2}y^2+\frac{\mu}{\sigma^2}y-\frac{\mu^2}{2\sigma^2}-\log\sigma\right) \\
&amp; = \frac{1}{\sqrt{2\pi}}\exp\left(\left(\frac{\mu}{\sigma^2},-\frac{1}{2\sigma^2}\right)^T(y,y^2)-\frac{\mu^2}{2\sigma^2}-\log\sigma\right)
\end{align*}
\]</span></p>
<p>El parámetro natural es el vector</p>
<p><span class="math display">\[\bfeta=\left(\frac{\mu}{\sigma^2},-\frac{1}{2\sigma^2}\right)\]</span></p>
<p>y el resto de los elementos son <span class="math display">\[\TT(y)=(y,y^2), \qquad a(\bfeta)=\frac{\mu^2}{2\sigma^2}+\log\sigma=-\frac{\eta_1^2}{4\eta_2}+\frac{1}{2}\log(-2\eta_2), \qquad b(y)=\frac{1}{\sqrt{2\pi}}.\]</span></p>
<hr>
<p>Existen muchas otras distribuciones que son miembros de la familia exponencial: la multinomial (que veremos más adelante), la Poisson (para modelar datos de conteo), la gamma y la exponencial (para modelar variables continuas no negativas, como intervalos de tiempo); la beta y la Dirichlet (para distribuciones sobre probabilidades); y muchas más.</p>
<p>En la próxima sección, describiremos una “receta” general para construir modelos en los cuales <span class="math inline">\(y\)</span> (dado <span class="math inline">\(\xx\)</span> y <span class="math inline">\(\theta\)</span>) proviene de cualquiera de estas distribuciones.</p>
</section>
</section>
</section>
<section id="construcción-de-glms" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Construcción de GLMs</h1>
<p>Supongamos que deseas construir un modelo para estimar el número <span class="math inline">\(y\)</span> de clientes que llegan a tu tienda (o el número de visitas a páginas web en tu sitio) en una hora determinada, basándote en ciertas características como promociones en la tienda, publicidad reciente, clima, día de la semana, etc. Sabemos que la distribución Poisson usualmente proporciona un buen modelo para el número de visitantes. Sabiendo esto, ¿cómo podemos proponer un modelo para nuestro problema? Afortunadamente, la Poisson es una distribución de la familia exponencial, por lo que podemos aplicar un Modelo Lineal Generalizado (GLM). En esta sección, describiremos un método para construir modelos GLM para problemas como este.</p>
<p>Más generalmente, consideremos un problema de regresión o clasificación o regresión donde queremos predecir el valor de alguna variable aleatoria <span class="math inline">\(y\)</span> como función de <span class="math inline">\(\xx\)</span>. Para derivar un GLM para este problema, haremos las siguientes tres suposiciones:</p>
<div class="highlight">
<p><strong>Suposición 1:</strong> <span class="math inline">\(y|\xx;\bftheta\sim\text{FamiliaExponencial}(\bfeta)\)</span>.</p>
<p><strong>Suposición 2:</strong> Dado <span class="math inline">\(\xx\)</span>, el objetivo es predecir <span class="math inline">\(\media[T(y)|\xx]\)</span>.</p>
<p><strong>Suposición 3:</strong> El parámetro natural <span class="math inline">\(\bfeta\)</span> y las predictoras <span class="math inline">\(\xx\)</span> están relacionadas linealmente; esto es, <span class="math inline">\(\bfeta=\bftheta^T\xx\)</span>.</p>
</div>
<p>Generalmente <span class="math inline">\(T(y)=y\)</span> y, en tal caso, la suposición 2 significa que queremos que la predicción sea <span class="math inline">\(h(\xx)=\media[\xx]\)</span>. Por otra parte, si el parámetro natural <span class="math inline">\(\bfeta\)</span> es un vector, observar que la suposición 3 implica que <span class="math inline">\(\bftheta\)</span> debe ser una matriz. Esta última suposición podría parecer la menos justificada de las anteriores, y podría considerarse más como una “elección de diseño” en nuestra receta para diseñar GLMs, en lugar de una suposición como tal.</p>
<p>Estas tres suposiciones o elecciones de diseño nos permitirán derivar una clase muy elegante de algoritmos de aprendizaje, a saber, los GLMs, que tienen muchas propiedades deseables, como la facilidad de aprendizaje. Además, los modelos resultantes son frecuentemente muy efectivos.</p>
<section id="mínimos-cuadrados-ordinarios" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="mínimos-cuadrados-ordinarios"><span class="header-section-number">2.1</span> Mínimos Cuadrados Ordinarios</h2>
<p>Para demostrar que los mínimos cuadrados ordinarios son un caso especial de la familia de modelos GLM, consideremos el escenario en el que la variable objetivo <span class="math inline">\(y\)</span> (también llamada <em>variable de respuesta</em> en la terminología GLM) es continua, y modelamos la distribución condicional de <span class="math inline">\(y\)</span> dado <span class="math inline">\(x\)</span> como una Gaussiana <span class="math inline">\(\mathcal{N}(\mu, \sigma^2)\)</span>. (Aquí, <span class="math inline">\(\mu\)</span> puede depender de <span class="math inline">\(x\)</span>.) Por lo tanto, tomamos la distribución <span class="math inline">\(\text{ExponentialFamily}(\eta)\)</span> sobre la distribución Gaussiana anterior. Como vimos previamente, en la formulación de la distribución Gaussiana como una distribución de la familia exponencial, tenemos <span class="math inline">\(\mu = \eta\)</span>. Así que tenemos:</p>
<p><span class="math display">\[
\begin{align*}
h_\theta(x) &amp;= \mathbb{E}[y | x; \theta] \\
&amp;= \mu \\ &amp;= \eta \\&amp;= \theta^T x.
\end{align*}
\]</span></p>
<p>La primera igualdad se deduce de la Suposición 2 mencionada anteriormente; la segunda igualdad se deduce del hecho de que <span class="math inline">\(y | x; \theta \sim \mathcal{N}(\mu, \sigma^2)\)</span>, y por lo tanto su valor esperado está dado por <span class="math inline">\(\mu\)</span>; la tercera igualdad se deduce de la Suposición 1 (y nuestra derivación anterior mostrando que <span class="math inline">\(\mu = \eta\)</span> en la formulación de la Gaussiana como una distribución de la familia exponencial); y la última igualdad se deduce de la Suposición 3.</p>
<p>PARA MI ESSTos ejempLOS VAN DEPSUES DE DECIR COMO SE ESTIMAN LOS PARAMETROS EN GENERAL EN LOS MODELOS GENERALIZADOS.</p>
</section>
<section id="ejemplo-en-python-mínimos-cuadrados-ordinarios" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="ejemplo-en-python-mínimos-cuadrados-ordinarios"><span class="header-section-number">2.2</span> Ejemplo en Python: Mínimos Cuadrados Ordinarios</h2>
<p>A continuación, implementamos un modelo de regresión lineal que ilustra cómo se ajustan los datos siguiendo un enfoque de mínimos cuadrados ordinarios.</p>
<div id="133f6918" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error, r2_score</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Generar datos simulados</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>n_samples <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> np.random.rand(n_samples, <span class="dv">1</span>)  <span class="co"># Características (una variable predictora)</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="dv">4</span> <span class="op">+</span> <span class="dv">3</span> <span class="op">*</span> X <span class="op">+</span> np.random.randn(n_samples, <span class="dv">1</span>)  <span class="co"># Respuesta con ruido</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Dividir en entrenamiento y prueba</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Ajustar un modelo de regresión lineal</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LinearRegression()</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>model.fit(X_train, y_train)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Predicciones</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model.predict(X_test)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Parámetros ajustados</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>theta_0 <span class="op">=</span> model.intercept_  <span class="co"># Intercepto</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>theta_1 <span class="op">=</span> model.coef_       <span class="co"># Pendiente</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualizar los resultados</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_test, y_test, color<span class="op">=</span><span class="st">"blue"</span>, label<span class="op">=</span><span class="st">"Datos reales"</span>)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>plt.plot(X_test, y_pred, color<span class="op">=</span><span class="st">"red"</span>, label<span class="op">=</span><span class="ss">f"Modelo ajustado: $y = </span><span class="sc">{</span>theta_0[<span class="dv">0</span>]<span class="sc">:.2f}</span><span class="ss"> + </span><span class="sc">{</span>theta_1[<span class="dv">0</span>][<span class="dv">0</span>]<span class="sc">:.2f}</span><span class="ss">x$"</span>)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Variable predictora (X)"</span>)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Variable respuesta (y)"</span>)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Mínimos Cuadrados Ordinarios"</span>)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluar el modelo</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>mse <span class="op">=</span> mean_squared_error(y_test, y_pred)</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>r2 <span class="op">=</span> r2_score(y_test, y_pred)</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Error cuadrático medio (MSE): </span><span class="sc">{</span>mse<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Coeficiente de determinación (R^2): </span><span class="sc">{</span>r2<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="02_GLM_files/figure-html/cell-2-output-1.png" width="659" height="523" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Error cuadrático medio (MSE): 0.6537
Coeficiente de determinación (R^2): 0.8072</code></pre>
</div>
</div>
</section>
<section id="regresión-logística" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="regresión-logística"><span class="header-section-number">2.3</span> Regresión Logística</h2>
<p>Ahora consideramos la regresión logística. Aquí estamos interesados en la clasificación binaria, por lo que <span class="math inline">\(y \in \{0, 1\}\)</span>. Dado que <span class="math inline">\(y\)</span> es un valor binario, parece natural elegir la familia Bernoulli de distribuciones para modelar la distribución condicional de <span class="math inline">\(y\)</span> dado <span class="math inline">\(x\)</span>. En nuestra formulación de la distribución Bernoulli como una distribución de la familia exponencial, teníamos <span class="math inline">\(\phi = 1/(1 + e^{-\eta})\)</span>. Además, nota que si <span class="math inline">\(y | x; \theta \sim \text{Bernoulli}(\phi)\)</span>, entonces <span class="math inline">\(\mathbb{E}[y | x; \theta] = \phi\)</span>. Por lo tanto, siguiendo una derivación similar a la de los mínimos cuadrados ordinarios, obtenemos:</p>
<p><span class="math display">\[
\begin{align*}
h_\theta(x) = \mathbb{E}[y | x; \theta] = \phi = \frac{1}{1 + e^{-\eta}} = \frac{1}{1 + e^{-\theta^T x}}.
\end{align*}
\]</span></p>
<p>Así, obtenemos funciones hipótesis de la forma:</p>
<p><span class="math display">\[
h_\theta(x) = \frac{1}{1 + e^{-\theta^T x}}.
\]</span></p>
<p>Si alguna vez te preguntaste cómo llegamos a la forma de la función logística <span class="math inline">\(1/(1 + e^{-z})\)</span>, esta es la respuesta: Una vez que asumimos que <span class="math inline">\(y\)</span> condicionado en <span class="math inline">\(x\)</span> es Bernoulli, surge como una consecuencia de la definición de los GLMs y las distribuciones de la familia exponencial.</p>
<section id="terminología" class="level3" data-number="2.3.1">
<h3 data-number="2.3.1" class="anchored" data-anchor-id="terminología"><span class="header-section-number">2.3.1</span> Terminología</h3>
<ul>
<li>La <strong>función de respuesta canónica</strong>, <span class="math inline">\(g\)</span>, da la media de la distribución como una función del parámetro natural: <span class="math inline">\(g(\eta) = \mathbb{E}[T(y); \eta]\)</span>.</li>
<li>La inversa de esta función, <span class="math inline">\(g^{-1}\)</span>, se llama la <strong>función de enlace canónica</strong>.</li>
</ul>
<p>Por ejemplo: - Para la familia Gaussiana, la función de respuesta canónica es la función identidad. - Para la familia Bernoulli, la función de respuesta canónica es la función logística.</p>
<p>ESTE TAMBIEN , tiene que haber una seccion de como estimar, luego compu</p>
</section>
</section>
<section id="ejemplo-en-python-regresión-logística" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="ejemplo-en-python-regresión-logística"><span class="header-section-number">2.4</span> Ejemplo en Python: Regresión Logística</h2>
<p>A continuación, implementamos un ejemplo práctico de regresión logística en un problema de clasificación binaria.</p>
<div id="31b2af28" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> classification_report, confusion_matrix</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Generar datos simulados para clasificación binaria</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_classification(</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    n_samples<span class="op">=</span><span class="dv">200</span>, n_features<span class="op">=</span><span class="dv">2</span>, n_informative<span class="op">=</span><span class="dv">2</span>, n_redundant<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span>, class_sep<span class="op">=</span><span class="fl">1.5</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualizar los datos</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>plt.scatter(X[y <span class="op">==</span> <span class="dv">0</span>, <span class="dv">0</span>], X[y <span class="op">==</span> <span class="dv">0</span>, <span class="dv">1</span>], color<span class="op">=</span><span class="st">"blue"</span>, label<span class="op">=</span><span class="st">"Clase 0"</span>)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>plt.scatter(X[y <span class="op">==</span> <span class="dv">1</span>, <span class="dv">0</span>], X[y <span class="op">==</span> <span class="dv">1</span>, <span class="dv">1</span>], color<span class="op">=</span><span class="st">"red"</span>, label<span class="op">=</span><span class="st">"Clase 1"</span>)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Datos de Clasificación Binaria"</span>)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Característica 1"</span>)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Característica 2"</span>)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Dividir datos en entrenamiento y prueba</span></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Ajustar un modelo de regresión logística</span></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression()</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>model.fit(X_train, y_train)</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Predicciones</span></span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model.predict(X_test)</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualización de la frontera de decisión</span></span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>x_min, x_max <span class="op">=</span> X[:, <span class="dv">0</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="dv">1</span>, X[:, <span class="dv">0</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>y_min, y_max <span class="op">=</span> X[:, <span class="dv">1</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="dv">1</span>, X[:, <span class="dv">1</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>xx, yy <span class="op">=</span> np.meshgrid(np.arange(x_min, x_max, <span class="fl">0.01</span>),</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>                     np.arange(y_min, y_max, <span class="fl">0.01</span>))</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> model.predict(np.c_[xx.ravel(), yy.ravel()])</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> Z.reshape(xx.shape)</span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a>plt.contourf(xx, yy, Z, alpha<span class="op">=</span><span class="fl">0.8</span>, cmap<span class="op">=</span><span class="st">"coolwarm"</span>)</span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a>plt.scatter(X[y <span class="op">==</span> <span class="dv">0</span>, <span class="dv">0</span>], X[y <span class="op">==</span> <span class="dv">0</span>, <span class="dv">1</span>], color<span class="op">=</span><span class="st">"blue"</span>, label<span class="op">=</span><span class="st">"Clase 0"</span>)</span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a>plt.scatter(X[y <span class="op">==</span> <span class="dv">1</span>, <span class="dv">0</span>], X[y <span class="op">==</span> <span class="dv">1</span>, <span class="dv">1</span>], color<span class="op">=</span><span class="st">"red"</span>, label<span class="op">=</span><span class="st">"Clase 1"</span>)</span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Frontera de Decisión de Regresión Logística"</span>)</span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Característica 1"</span>)</span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Característica 2"</span>)</span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluación del modelo</span></span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Reporte de Clasificación:"</span>)</span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_test, y_pred))</span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Matriz de Confusión:"</span>)</span>
<span id="cb3-58"><a href="#cb3-58" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(confusion_matrix(y_test, y_pred))</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="02_GLM_files/figure-html/cell-3-output-1.png" width="662" height="523" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="02_GLM_files/figure-html/cell-3-output-2.png" width="662" height="524" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Reporte de Clasificación:
              precision    recall  f1-score   support

           0       0.97      0.97      0.97        32
           1       0.96      0.96      0.96        28

    accuracy                           0.97        60
   macro avg       0.97      0.97      0.97        60
weighted avg       0.97      0.97      0.97        60

Matriz de Confusión:
[[31  1]
 [ 1 27]]</code></pre>
</div>
</div>
</section>
<section id="regresión-softmax" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="regresión-softmax"><span class="header-section-number">2.5</span> Regresión Softmax</h2>
<section id="contexto" class="level3" data-number="2.5.1">
<h3 data-number="2.5.1" class="anchored" data-anchor-id="contexto"><span class="header-section-number">2.5.1</span> Contexto</h3>
<p>Consideremos un problema de clasificación en el que la variable de respuesta $yConsideremos un problema de clasificación en el que la variable de respuesta $yConsideremos un problema de clasificación en el que la variable de respuesta $yConsideremos un problema de clasificación en el que la variable de respuesta <span class="math inline">\(y\)</span> puede tomar cualquiera de (k) valores, por lo que <span class="math inline">\(y \in \{1, 2, \ldots, k\}\)</span>. Por ejemplo, en lugar de clasificar correos electrónicos en dos clases (spam o no spam), podríamos clasificarlos en tres clases: spam, correo personal y correo relacionado con el trabajo. Para modelar esta situación, usaremos la <strong>distribución multinomial</strong> como una distribución de la familia exponencial.</p>
</section>
<section id="la-multinomial-como-distribución-de-la-familia-exponencial" class="level3" data-number="2.5.2">
<h3 data-number="2.5.2" class="anchored" data-anchor-id="la-multinomial-como-distribución-de-la-familia-exponencial"><span class="header-section-number">2.5.2</span> La Multinomial como Distribución de la Familia Exponencial</h3>
<p>Derivemos un GLM para datos multinomiales parametrizando la multinomial con <span class="math inline">\(k-1\)</span> parámetros <span class="math inline">\(\phi_1, \dots, \phi_{k-1}\)</span>, dado que <span class="math inline">\(\sum_{i=1}^k \phi_i = 1\)</span>. Para expresar la multinomial como una distribución de la familia exponencial, definimos:</p>
<p><span class="math display">\[
\begin{align*}
T(1)&amp; = \begin{bmatrix} 1 \\ 0 \\ \vdots \\ 0 \end{bmatrix}, \\
T(2) &amp;= \begin{bmatrix} 0 \\ 1 \\ \vdots \\ 0 \end{bmatrix}, \\ \dots, \\
T(k) &amp;= \begin{bmatrix} 0 \\ 0 \\ \vdots \\ 0 \end{bmatrix}.
\end{align*}
\]</span></p>
<p>La distribución multinomial pertenece a la familia exponencial. Tenemos:</p>
<p><span class="math display">\[
\begin{align*}
p(y; \phi)&amp; =
\phi_1^{1\{y=1\}} \phi_2^{1\{y=2\}} \cdots \phi_k^{1\{y=k\}}\\
&amp;= \phi_1^{(T(y))_1} \phi_2^{(T(y))_2} \cdots \phi_k^{1-\sum_{i=1}^{k-1}(T(y))_i}
\\
&amp;= \phi_1^{(T(y))_1} \phi_2^{(T(y))_2} \cdots \phi_{k-1}^{(T(y))_{k-1}} \left(1 - \sum_{i=1}^{k-1} \phi_i\right)^{1-\sum_{i=1}^{k-1}(T(y))_i}
\\
&amp;= \exp\left((T(y))_1 \log(\phi_1) + (T(y))_2 \log(\phi_2) + \dots + \left(1 - \sum_{i=1}^{k-1}(T(y))_i\right)\log\left(1 - \sum_{i=1}^{k-1} \phi_i\right)\right).
\end{align*}
\]</span></p>
<p>Finalmente, organizamos los términos en la forma estándar de la familia exponencial:</p>
<p><span class="math display">\[
p(y; \phi) = b(y) \exp\left(\eta^T T(y) - a(\eta)\right),
\]</span></p>
<p>donde:</p>
<p><span class="math display">\[
\begin{align*}
\eta &amp;= \begin{bmatrix}
\log\left(\frac{\phi_1}{\phi_k}\right) \\
\log\left(\frac{\phi_2}{\phi_k}\right) \\
\vdots \\
\log\left(\frac{\phi_{k-1}}{\phi_k}\right)
\end{bmatrix},
\\
a(\eta) &amp;= -\log(\phi_k),
\\
b(y)&amp; = 1.
\end{align*}
\]</span></p>
<p>Esto completa nuestra formulación de la multinomial como una distribución de la familia exponencial.</p>
<p>La función que mapea () a () se llama la <strong>función softmax</strong>:</p>
</section>
<section id="función-de-enlace-y-función-de-respuesta" class="level3" data-number="2.5.3">
<h3 data-number="2.5.3" class="anchored" data-anchor-id="función-de-enlace-y-función-de-respuesta"><span class="header-section-number">2.5.3</span> Función de Enlace y Función de Respuesta</h3>
<p>La función de enlace está dada (para <span class="math inline">\(i = 1, \dots, k\)</span>) por:</p>
<p><span class="math display">\[
\eta_i = \log\left(\frac{\phi_i}{\phi_k}\right).
\]</span></p>
<p>Para conveniencia, también definimos <span class="math inline">\(\eta_k = \log(\phi_k / \phi_k) = 0\)</span>. Para invertir la función de enlace y derivar la función de respuesta, tenemos que:</p>
<p><span class="math display">\[
e^{\eta_i} = \frac{\phi_i}{\phi_k},
\]</span></p>
<p>lo que implica:</p>
<p><span class="math display">\[
\phi_k \sum_{i=1}^k e^{\eta_i} = \sum_{i=1}^k \phi_i = 1.
\]</span></p>
<p>Por lo tanto:</p>
<p><span class="math display">\[
\phi_k = \frac{1}{\sum_{i=1}^k e^{\eta_i}},
\]</span></p>
<p>y al sustituir en la ecuación obtenemos la función de respuesta:</p>
<p><span class="math display">\[
\phi_i = \frac{e^{\eta_i}}{\sum_{j=1}^k e^{\eta_j}}.
\]</span></p>
</section>
</section>
<section id="softmax-function-y-modelo-de-clasificación" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="softmax-function-y-modelo-de-clasificación"><span class="header-section-number">2.6</span> Softmax Function y Modelo de Clasificación</h2>
<p>La función que mapea de <span class="math inline">\(\eta\)</span> a <span class="math inline">\(\phi\)</span> se llama la función .</p>
<p>Para completar nuestro modelo, usamos la Asunción 3, mencionada previamente, donde los <span class="math inline">\(\eta_i\)</span> están relacionados linealmente con los <span class="math inline">\(x\)</span>’s. Así, tenemos <span class="math inline">\(\eta_i = \theta_i^T x\)</span> (para <span class="math inline">\(i = 1, \dots, k - 1\)</span>), donde <span class="math inline">\(\theta_1, \dots, \theta_{k-1} \in \mathbb{R}^{d+1}\)</span> son los parámetros de nuestro modelo. Por notación, podemos definir <span class="math inline">\(\theta_k = 0\)</span>, de modo que <span class="math inline">\(\eta_k = \theta_k^T x = 0\)</span>, como se definió anteriormente. Por lo tanto, nuestro modelo asume que la distribución condicional de <span class="math inline">\(y\)</span> dado <span class="math inline">\(x\)</span> está dada por:</p>
<p><span class="math display">\[
\begin{align*}
p(y = i | x; \theta) &amp;= \phi_i\\
&amp;= \frac{e^{\eta_i}}{\sum_{j=1}^k e^{\eta_j}}\\
&amp;= \frac{e^{\theta_i^T x}}{\sum_{j=1}^k e^{\theta_j^T x}}, \quad (5)
\end{align*}
\]</span></p>
<p>donde este modelo, que aplica a problemas de clasificación donde <span class="math inline">\(y \in \{1, \dots, k\}\)</span>, se llama . Es una generalización de la regresión logística.</p>
<section id="hipótesis-del-modelo" class="level3" data-number="2.6.1">
<h3 data-number="2.6.1" class="anchored" data-anchor-id="hipótesis-del-modelo"><span class="header-section-number">2.6.1</span> Hipótesis del Modelo</h3>
<p>Nuestra hipótesis producirá:</p>
<p><span class="math display">\[
\begin{align*}
h_\theta(x) &amp;= \mathbb{E}[T(y) | x; \theta]
\\
&amp;= \mathbb{E}
\begin{bmatrix}
1\{y = 1\} \\
1\{y = 2\} \\
\vdots \\
1\{y = k - 1\}
\end{bmatrix}
\\&amp;=
\begin{bmatrix}
\phi_1 \\
\phi_2 \\
\vdots \\
\phi_{k-1}
\end{bmatrix}
\\&amp;=
\begin{bmatrix}
\frac{\exp(\theta_1^T x)}{\sum_{j=1}^k \exp(\theta_j^T x)} \\
\frac{\exp(\theta_2^T x)}{\sum_{j=1}^k \exp(\theta_j^T x)} \\
\vdots \\
\frac{\exp(\theta_{k-1}^T x)}{\sum_{j=1}^k \exp(\theta_j^T x)}
\end{bmatrix}.
\end{align*}
\]</span></p>
<p>En otras palabras, nuestra hipótesis producirá la probabilidad estimada de que <span class="math inline">\(p(y = i | x; \theta)\)</span> para cada valor <span class="math inline">\(i = 1, \dots, k\)</span>. (Aunque <span class="math inline">\(h_\theta(x)\)</span>, como se definió arriba, tiene solo <span class="math inline">\(k - 1\)</span> dimensiones, claramente <span class="math inline">\(p(y = k | x; \theta)\)</span> puede obtenerse como <span class="math inline">\(1 - \sum_{i=1}^{k-1} \phi_i\)</span>).</p>
</section>
</section>
<section id="ajuste-de-parámetros" class="level2" data-number="2.7">
<h2 data-number="2.7" class="anchored" data-anchor-id="ajuste-de-parámetros"><span class="header-section-number">2.7</span> Ajuste de Parámetros</h2>
<p>Por último, discutamos el ajuste de parámetros. Similar a nuestra derivación original de mínimos cuadrados ordinarios y regresión logística, si tenemos un conjunto de entrenamiento de <span class="math inline">\(n\)</span> ejemplos <span class="math inline">\(\{(x^{(i)}, y^{(i)}); i = 1, \dots, n\}\)</span> y deseamos aprender los parámetros <span class="math inline">\(\theta_i\)</span> de este modelo, comenzaríamos escribiendo la log-verosimilitud:</p>
<p><span class="math display">\[
\begin{align*}
\ell(\theta) &amp;= \sum_{i=1}^n \log p(y^{(i)} | x^{(i)}; \theta)
\\
&amp;= \sum_{i=1}^n \log \prod_{l=1}^k \left( \frac{e^{\theta_l^T x^{(i)}}}{\sum_{j=1}^k e^{\theta_j^T x^{(i)}}} \right)^{1\{y^{(i)} = l\}}.
\end{align*}
\]</span></p>
<p>Para obtener la segunda línea anterior, usamos la definición de <span class="math inline">\(p(y | x; \theta)\)</span> dada en la Ecuación (5). Ahora podemos obtener la estimación de máxima verosimilitud de los parámetros maximizando <span class="math inline">\(\ell(\theta)\)</span> en términos de <span class="math inline">\(\theta\)</span>, utilizando un método como ascenso de gradiente o el método de Newton.</p>
<p>ACA FALTA AJUSTE DE PARAMETROS PARA EL MODELO GLM EN GENERAL eso esta en el video de el viejo del 2008 , no esta en el nuevo. seria importante poner algo</p>
<section id="ejemplo-práctico-en-python" class="level3" data-number="2.7.1">
<h3 data-number="2.7.1" class="anchored" data-anchor-id="ejemplo-práctico-en-python"><span class="header-section-number">2.7.1</span> Ejemplo Práctico en Python</h3>
<p>Implementemos un ejemplo práctico para un problema de clasificación multiclasificación.</p>
<div id="2f063047" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> classification_report, confusion_matrix</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Generar datos simulados para clasificación multiclase</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_classification(</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    n_samples<span class="op">=</span><span class="dv">300</span>, n_features<span class="op">=</span><span class="dv">2</span>, n_informative<span class="op">=</span><span class="dv">2</span>, n_redundant<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    n_classes<span class="op">=</span><span class="dv">3</span>, n_clusters_per_class<span class="op">=</span><span class="dv">1</span>, random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualizar los datos</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>scatter <span class="op">=</span> plt.scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], c<span class="op">=</span>y, cmap<span class="op">=</span><span class="st">"viridis"</span>)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Datos de Clasificación Multiclase"</span>)</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Característica 1"</span>)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Característica 2"</span>)</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>plt.colorbar(scatter, label<span class="op">=</span><span class="st">"Clases"</span>)</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Dividir los datos en entrenamiento y prueba</span></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Ajustar el modelo de regresión softmax</span></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression(multi_class<span class="op">=</span><span class="st">"multinomial"</span>, solver<span class="op">=</span><span class="st">"lbfgs"</span>, max_iter<span class="op">=</span><span class="dv">200</span>)</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>model.fit(X_train, y_train)</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Predicciones</span></span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model.predict(X_test)</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Frontera de decisión</span></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>x_min, x_max <span class="op">=</span> X[:, <span class="dv">0</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="dv">1</span>, X[:, <span class="dv">0</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>y_min, y_max <span class="op">=</span> X[:, <span class="dv">1</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="dv">1</span>, X[:, <span class="dv">1</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>xx, yy <span class="op">=</span> np.meshgrid(np.arange(x_min, x_max, <span class="fl">0.01</span>),</span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>                     np.arange(y_min, y_max, <span class="fl">0.01</span>))</span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> model.predict(np.c_[xx.ravel(), yy.ravel()])</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> Z.reshape(xx.shape)</span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a>plt.contourf(xx, yy, Z, alpha<span class="op">=</span><span class="fl">0.8</span>, cmap<span class="op">=</span><span class="st">"viridis"</span>)</span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a>scatter <span class="op">=</span> plt.scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], c<span class="op">=</span>y, edgecolor<span class="op">=</span><span class="st">"k"</span>, cmap<span class="op">=</span><span class="st">"viridis"</span>)</span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Frontera de Decisión: Regresión Softmax"</span>)</span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Característica 1"</span>)</span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Característica 2"</span>)</span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a>plt.colorbar(scatter, label<span class="op">=</span><span class="st">"Clases"</span>)</span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluación del modelo</span></span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Reporte de Clasificación:"</span>)</span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_test, y_pred))</span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Matriz de Confusión:"</span>)</span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(confusion_matrix(y_test, y_pred))</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="02_GLM_files/figure-html/cell-4-output-1.png" width="652" height="523" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="02_GLM_files/figure-html/cell-4-output-2.png" width="652" height="524" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Reporte de Clasificación:
              precision    recall  f1-score   support

           0       0.75      0.89      0.81        27
           1       0.92      0.89      0.91        38
           2       1.00      0.84      0.91        25

    accuracy                           0.88        90
   macro avg       0.89      0.87      0.88        90
weighted avg       0.89      0.88      0.88        90

Matriz de Confusión:
[[24  3  0]
 [ 4 34  0]
 [ 4  0 21]]</code></pre>
</div>
</div>
</section>
</section>
<section id="ejemplo-poisson" class="level2" data-number="2.8">
<h2 data-number="2.8" class="anchored" data-anchor-id="ejemplo-poisson"><span class="header-section-number">2.8</span> Ejemplo Poisson</h2>
<div id="392ce34d" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> PoissonRegressor</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Simular datos</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>n_samples <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.rand(n_samples, <span class="dv">1</span>) <span class="op">*</span> <span class="dv">10</span>  <span class="co"># Promociones (característica)</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>theta <span class="op">=</span> <span class="fl">0.5</span>  <span class="co"># Relación lineal</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>y_mean <span class="op">=</span> np.exp(theta <span class="op">*</span> X).flatten()  <span class="co"># Media de Poisson</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.random.poisson(y_mean)  <span class="co"># Datos simulados de Poisson</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Dividir en entrenamiento y prueba</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Ajustar un modelo GLM con Poisson</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> PoissonRegressor(alpha<span class="op">=</span><span class="dv">0</span>, max_iter<span class="op">=</span><span class="dv">300</span>)</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>model.fit(X_train, y_train)</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Predicciones</span></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model.predict(X_test)</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualizar los resultados</span></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_test, y_test, color<span class="op">=</span><span class="st">"blue"</span>, label<span class="op">=</span><span class="st">"Datos reales"</span>)</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_test, y_pred, color<span class="op">=</span><span class="st">"red"</span>, label<span class="op">=</span><span class="st">"Predicciones"</span>)</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Promociones (característica)"</span>)</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Clientes (target)"</span>)</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Modelo Poisson GLM"</span>)</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluación del modelo</span></span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>score <span class="op">=</span> model.score(X_test, y_test)</span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Coeficiente de determinación (R^2): </span><span class="sc">{</span>score<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="02_GLM_files/figure-html/cell-5-output-1.png" width="667" height="523" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Coeficiente de determinación (R^2): 0.9775</code></pre>
</div>
</div>
</section>
</section>
<section id="ajuste-de-parámetros-en-modelos-lineales-generalizados" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Ajuste de Parámetros en Modelos Lineales Generalizados</h1>
<p>Los Modelos Lineales Generalizados (GLMs) son una extensión poderosa de los modelos lineales que permite trabajar con una amplia variedad de distribuciones en la familia exponencial. En esta sección, discutiremos en detalle cómo se ajustan los parámetros de un GLM usando el principio de máxima verosimilitud, seguido por un análisis de los casos específicos de las distribuciones Gaussianas, Bernoulli, Multinomial y Poisson.</p>
<section id="el-principio-de-máxima-verosimilitud" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="el-principio-de-máxima-verosimilitud"><span class="header-section-number">3.1</span> El Principio de Máxima Verosimilitud</h2>
<p>El ajuste de parámetros en los GLMs se basa en maximizar la log-verosimilitud de los datos observados. Dado un conjunto de entrenamiento <span class="math display">\[\{(x^{(i)}, y^{(i)})\}_{i=1}^n\]</span>, la función de verosimilitud está dada por:</p>
<p><span class="math display">\[
L(\theta) = \prod_{i=1}^n p(y^{(i)} | x^{(i)}; \theta),
\]</span></p>
<p>y la log-verosimilitud correspondiente es:</p>
<p><span class="math display">\[
\ell(\theta) = \log L(\theta) = \sum_{i=1}^n \log p(y^{(i)} | x^{(i)}; \theta).
\]</span></p>
<p>Para maximizar <span class="math display">\[\ell(\theta)\]</span>, derivamos con respecto a <span class="math display">\[\theta\]</span> para obtener el gradiente. En GLMs, las propiedades de la familia exponencial simplifican esta derivada, que resulta ser:</p>
<p><span class="math display">\[
\nabla_\theta \ell(\theta) = \sum_{i=1}^n (T(y^{(i)}) - \mathbb{E}[T(y) | x^{(i)}; \theta]) x^{(i)},
\]</span></p>
<p>donde <span class="math display">\[T(y)\]</span> es la estadística suficiente de la distribución y <span class="math display">\[\mathbb{E}[T(y) | x^{(i)}; \theta]\]</span> es el valor esperado de <span class="math display">\[T(y)\]</span> dado <span class="math display">\[x^{(i)}\]</span>.</p>
</section>
<section id="el-método-de-newton-raphson" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="el-método-de-newton-raphson"><span class="header-section-number">3.2</span> El Método de Newton-Raphson</h2>
<p>Para maximizar la log-verosimilitud, usamos un método iterativo como Newton-Raphson. Este método requiere calcular la Hessiana de <span class="math display">\[\ell(\theta)\]</span>, que es la matriz de segundas derivadas:</p>
<p><span class="math display">\[
H(\theta) = \sum_{i=1}^n \left(-\text{Var}[T(y) | x^{(i)}; \theta]\right) x^{(i)} (x^{(i)})^T,
\]</span></p>
<p>donde <span class="math display">\[\text{Var}[T(y) | x; \theta]\]</span> es la varianza condicional de <span class="math display">\[T(y)\]</span>, que depende de la distribución específica del modelo. El método de Newton-Raphson actualiza <span class="math display">\[\theta\]</span> como:</p>
<p><span class="math display">\[
\theta^{(t+1)} = \theta^{(t)} - H(\theta^{(t)})^{-1} \nabla_\theta \ell(\theta^{(t)}).
\]</span></p>
<p>Aunque este método converge rápidamente en muchas aplicaciones, calcular la Hessiana e invertirla puede ser computacionalmente costoso para modelos de alta dimensionalidad. En tales casos, se prefieren métodos más simples como el descenso por gradiente.</p>
</section>
<section id="casos-específicos-de-glms" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="casos-específicos-de-glms"><span class="header-section-number">3.3</span> Casos Específicos de GLMs</h2>
<section id="regresión-lineal-distribución-gaussiana" class="level3" data-number="3.3.1">
<h3 data-number="3.3.1" class="anchored" data-anchor-id="regresión-lineal-distribución-gaussiana"><span class="header-section-number">3.3.1</span> Regresión Lineal (Distribución Gaussiana)</h3>
<p>En la regresión lineal, asumimos que <span class="math display">\[y | x; \theta \sim \mathcal{N}(\mu, \sigma^2)\]</span>, con <span class="math display">\[\mu = \theta^T x\]</span>. Las propiedades de la distribución Gaussiana son:</p>
<p><span class="math display">\[
T(y) = y, \quad \mathbb{E}[T(y) | x; \theta] = \mu = \theta^T x, \quad \text{Var}[T(y) | x; \theta] = \sigma^2.
\]</span></p>
<p>El gradiente de la log-verosimilitud es:</p>
<p><span class="math display">\[
\nabla_\theta \ell(\theta) = \sum_{i=1}^n (y^{(i)} - \theta^T x^{(i)}) x^{(i)}.
\]</span></p>
<section id="ejemplo-en-python-regresión-lineal" class="level4" data-number="3.3.1.1">
<h4 data-number="3.3.1.1" class="anchored" data-anchor-id="ejemplo-en-python-regresión-lineal"><span class="header-section-number">3.3.1.1</span> Ejemplo en Python: Regresión Lineal</h4>
<div id="c66252c9" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Simular datos</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.rand(<span class="dv">100</span>, <span class="dv">1</span>) <span class="op">*</span> <span class="dv">10</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="dv">3</span> <span class="op">*</span> X.flatten() <span class="op">+</span> <span class="dv">5</span> <span class="op">+</span> np.random.randn(<span class="dv">100</span>) <span class="op">*</span> <span class="dv">2</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Ajustar modelo</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LinearRegression()</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>model.fit(X, y)</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Predicciones</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model.predict(X)</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Gráfico</span></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>plt.scatter(X, y, label<span class="op">=</span><span class="st">"Datos reales"</span>)</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>plt.plot(X, y_pred, color<span class="op">=</span><span class="st">"red"</span>, label<span class="op">=</span><span class="st">"Modelo ajustado"</span>)</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"X"</span>)</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"y"</span>)</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Regresión Lineal"</span>)</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="02_GLM_files/figure-html/cell-6-output-1.png" width="585" height="450" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<hr>
</section>
</section>
<section id="regresión-logística-distribución-bernoulli" class="level3" data-number="3.3.2">
<h3 data-number="3.3.2" class="anchored" data-anchor-id="regresión-logística-distribución-bernoulli"><span class="header-section-number">3.3.2</span> Regresión Logística (Distribución Bernoulli)</h3>
<p>En la regresión logística, <span class="math display">\[y | x; \theta \sim \text{Bernoulli}(\phi)\]</span>, con <span class="math display">\[\phi = \frac{1}{1 + e^{-\theta^T x}}\]</span>. Las propiedades relevantes son:</p>
<p><span class="math display">\[
T(y) = y, \quad \mathbb{E}[T(y) | x; \theta] = \phi, \quad \text{Var}[T(y) | x; \theta] = \phi (1 - \phi).
\]</span></p>
<section id="ejemplo-en-python-regresión-logística-1" class="level4" data-number="3.3.2.1">
<h4 data-number="3.3.2.1" class="anchored" data-anchor-id="ejemplo-en-python-regresión-logística-1"><span class="header-section-number">3.3.2.1</span> Ejemplo en Python: Regresión Logística</h4>
<div id="6ca93a46" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Simular datos binarios</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.rand(<span class="dv">100</span>, <span class="dv">2</span>) <span class="op">*</span> <span class="dv">10</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> (<span class="dv">3</span> <span class="op">*</span> X[:, <span class="dv">0</span>] <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> X[:, <span class="dv">1</span>] <span class="op">+</span> np.random.randn(<span class="dv">100</span>) <span class="op">&gt;</span> <span class="dv">15</span>).astype(<span class="bu">int</span>)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Ajustar modelo</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression()</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>model.fit(X, y)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Predicciones</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model.predict(X)</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Gráfico</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>plt.scatter(X[:, <span class="dv">0</span>], y, label<span class="op">=</span><span class="st">"Datos reales"</span>)</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>plt.scatter(X[:, <span class="dv">0</span>], y_pred, color<span class="op">=</span><span class="st">"red"</span>, label<span class="op">=</span><span class="st">"Predicciones"</span>)</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"X"</span>)</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"y"</span>)</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Regresión Logística"</span>)</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="02_GLM_files/figure-html/cell-7-output-1.png" width="589" height="450" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<hr>
</section>
</section>
<section id="regresión-softmax-distribución-multinomial" class="level3" data-number="3.3.3">
<h3 data-number="3.3.3" class="anchored" data-anchor-id="regresión-softmax-distribución-multinomial"><span class="header-section-number">3.3.3</span> Regresión Softmax (Distribución Multinomial)</h3>
<p>En la regresión softmax, <span class="math display">\[y \in \{1, \dots, k\}\]</span>:</p>
<p><span class="math display">\[
p(y = i | x; \theta) = \frac{\exp(\theta_i^T x)}{\sum_{j=1}^k \exp(\theta_j^T x)}.
\]</span></p>
<section id="ejemplo-en-python-regresión-softmax" class="level4" data-number="3.3.3.1">
<h4 data-number="3.3.3.1" class="anchored" data-anchor-id="ejemplo-en-python-regresión-softmax"><span class="header-section-number">3.3.3.1</span> Ejemplo en Python: Regresión Softmax</h4>
<div id="56ee332c" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Simular datos multinomiales</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.rand(<span class="dv">150</span>, <span class="dv">2</span>) <span class="op">*</span> <span class="dv">10</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.random.choice([<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>], size<span class="op">=</span><span class="dv">150</span>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Ajustar modelo</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression(multi_class<span class="op">=</span><span class="st">'multinomial'</span>, solver<span class="op">=</span><span class="st">'lbfgs'</span>)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>model.fit(X, y)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Predicciones</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model.predict(X)</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Gráfico</span></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>plt.scatter(X[:, <span class="dv">0</span>], y, label<span class="op">=</span><span class="st">"Datos reales"</span>)</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>plt.scatter(X[:, <span class="dv">0</span>], y_pred, color<span class="op">=</span><span class="st">"red"</span>, label<span class="op">=</span><span class="st">"Predicciones"</span>)</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"X"</span>)</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"y"</span>)</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Regresión Softmax"</span>)</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="02_GLM_files/figure-html/cell-8-output-1.png" width="597" height="450" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<hr>
</section>
</section>
<section id="regresión-poisson" class="level3" data-number="3.3.4">
<h3 data-number="3.3.4" class="anchored" data-anchor-id="regresión-poisson"><span class="header-section-number">3.3.4</span> Regresión Poisson</h3>
<p>En la regresión Poisson, <span class="math display">\[y | x; \theta \sim \text{Poisson}(\lambda)\]</span>, con <span class="math display">\[\lambda = \exp(\theta^T x)\]</span>. Las propiedades relevantes son:</p>
<p><span class="math display">\[
T(y) = y, \quad \mathbb{E}[T(y) | x; \theta] = \lambda = \exp(\theta^T x), \quad \text{Var}[T(y) | x; \theta] = \lambda.
\]</span></p>
<section id="ejemplo-en-python-regresión-poisson" class="level4" data-number="3.3.4.1">
<h4 data-number="3.3.4.1" class="anchored" data-anchor-id="ejemplo-en-python-regresión-poisson"><span class="header-section-number">3.3.4.1</span> Ejemplo en Python: Regresión Poisson</h4>
<div id="17bfb482" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> PoissonRegressor</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Simular datos de Poisson</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.rand(<span class="dv">100</span>, <span class="dv">1</span>) <span class="op">*</span> <span class="dv">10</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.random.poisson(lam<span class="op">=</span>np.exp(<span class="fl">0.5</span> <span class="op">*</span> X.flatten()))</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Ajustar modelo</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> PoissonRegressor(alpha<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>model.fit(X, y)</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Predicciones</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model.predict(X)</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Gráfico</span></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>plt.scatter(X, y, label<span class="op">=</span><span class="st">"Datos reales"</span>)</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>plt.plot(X, y_pred, color<span class="op">=</span><span class="st">"red"</span>, label<span class="op">=</span><span class="st">"Modelo ajustado"</span>)</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"X"</span>)</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"y"</span>)</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Regresión Poisson"</span>)</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="02_GLM_files/figure-html/cell-9-output-1.png" width="593" height="450" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<hr>
</section>
</section>
</section>
<section id="conclusión" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="conclusión"><span class="header-section-number">3.4</span> Conclusión</h2>
<p>El ajuste de parámetros en los GLMs se basa en maximizar la log-verosimilitud. Este marco unificado permite abordar una amplia variedad de problemas de clasificación y regresión usando distribuciones específicas de la familia exponencial.</p>
<!-- -->

</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copiado");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copiado");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Ejecutar el código</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb13" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Modelos lineales generalizados"</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="an">lang:</span><span class="co"> es</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="co">    css: style.css</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="co">    toc: true</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="co">    toc-depth: 3</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="co">    toc-location: left</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="co">    toc-floating: true</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: true</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools: true</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a><span class="co">    code-highlight: true</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a><span class="co">    code-theme: monokai</span></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a><span class="co">    theme: cosmo</span></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a><span class="co">    execute:</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a><span class="co">      echo: true</span></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a><span class="co">      warning: false</span></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a><span class="an">number-sections:</span><span class="co"> true</span></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>::: {.hidden} </span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>\def\RR{\mathbb{R}}</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>\def\media{\mathbb{E}}</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>\def\xx{{\bf x}}</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>\def\XX{{\bf X}}</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>\def\TT{{\bf T}}</span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>\def\bftheta{\boldsymbol{\theta}}</span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>\def\bfeta{\boldsymbol{\eta}}</span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a>   $$ </span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a>Tanto en el modelo de regresión lineal como en el de regresión logística, asumimos una cierta distribución condicional para $Y|\XX$ que depende de un conjunto de parámetros $\bftheta$. </span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>En regresión lineal, suponemos $Y|\XX\sim\mathcal{N}(\mu,\sigma^2)$ con $\mu=\bftheta^T\XX$.</span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>En regresión logística, que recordemos está pensado para problemas de clasificación, suponemos $Y|\XX\sim\text{Bernoulli}(\phi)$ con $\phi=g(\bftheta^T\XX)$, donde $g(z)=1/(1+e^{-z})$.</span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a>En esta sección mostraremos que ambos métodos son casos especiales de una familia más amplia de modelos, llamados **Modelos Lineales Generalizados (GLMs)**. También mostraremos cómo otros modelos en GLM pueden derivarse y aplicarse a otros problemas de regresión y clasificación. </span>
<span id="cb13-44"><a href="#cb13-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-45"><a href="#cb13-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-46"><a href="#cb13-46" aria-hidden="true" tabindex="-1"></a><span class="fu"># La familia exponencial</span></span>
<span id="cb13-47"><a href="#cb13-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-48"><a href="#cb13-48" aria-hidden="true" tabindex="-1"></a>::: {.definicion}</span>
<span id="cb13-49"><a href="#cb13-49" aria-hidden="true" tabindex="-1"></a>**Definición:**  Decimos que una clase de distribuciones pertenece a la familia exponencial si se puede escribir en la forma</span>
<span id="cb13-50"><a href="#cb13-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-51"><a href="#cb13-51" aria-hidden="true" tabindex="-1"></a>$$p(y; \bfeta) = b(y) \exp\left(\bfeta^T \TT(y) - a(\bfeta)\right)$$</span>
<span id="cb13-52"><a href="#cb13-52" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-53"><a href="#cb13-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-54"><a href="#cb13-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-55"><a href="#cb13-55" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\bfeta$ se llama **parámetro natural** de la distribución.</span>
<span id="cb13-56"><a href="#cb13-56" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\TT(y)$ es el **estadístico suficiente**.</span>
<span id="cb13-57"><a href="#cb13-57" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$a(\bfeta)$ es la **función de partición logarítmica** que normaliza la expresión para que la distribución sea válida (es decir, integre a 1).</span>
<span id="cb13-58"><a href="#cb13-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-59"><a href="#cb13-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-60"><a href="#cb13-60" aria-hidden="true" tabindex="-1"></a>Una elección fija de $\TT$, $a$ y $b$ define un conjunto de distribuciones parametrizadas por $\bfeta$, de manera que al variar $\bfeta$ obtenemos diferentes distribuciones dentro de esta familia.</span>
<span id="cb13-61"><a href="#cb13-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-62"><a href="#cb13-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-63"><a href="#cb13-63" aria-hidden="true" tabindex="-1"></a><span class="fu">## Ejemplos</span></span>
<span id="cb13-64"><a href="#cb13-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-65"><a href="#cb13-65" aria-hidden="true" tabindex="-1"></a>Mostraremos que las distribuciones Bernoulli y Normal son ejemplos de distribuciones de la familia exponencial.</span>
<span id="cb13-66"><a href="#cb13-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-67"><a href="#cb13-67" aria-hidden="true" tabindex="-1"></a><span class="fu">### Distribución Bernoulli</span></span>
<span id="cb13-68"><a href="#cb13-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-69"><a href="#cb13-69" aria-hidden="true" tabindex="-1"></a>Sea $Y\in<span class="sc">\{</span>0,1<span class="sc">\}</span>$ tal que $Y\sim\text{Bernoulli}(\phi)$. Entonces</span>
<span id="cb13-70"><a href="#cb13-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-71"><a href="#cb13-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-72"><a href="#cb13-72" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-73"><a href="#cb13-73" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb13-74"><a href="#cb13-74" aria-hidden="true" tabindex="-1"></a>p(y; \phi) &amp;= \phi^y (1 - \phi)^{1-y} <span class="sc">\\</span> </span>
<span id="cb13-75"><a href="#cb13-75" aria-hidden="true" tabindex="-1"></a>&amp; = \exp\left( y \log\phi + (1-y)\log(1-\phi) \right) <span class="sc">\\</span></span>
<span id="cb13-76"><a href="#cb13-76" aria-hidden="true" tabindex="-1"></a>&amp; = \exp\left(\log\left(\frac{\phi}{1-\phi}\right)y+\log(1-\phi)\right) <span class="sc">\\</span></span>
<span id="cb13-77"><a href="#cb13-77" aria-hidden="true" tabindex="-1"></a>&amp; = \exp\left(\log\left(\frac{\phi}{1-\phi}\right)y+\log(1-\phi)\right)</span>
<span id="cb13-78"><a href="#cb13-78" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb13-79"><a href="#cb13-79" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-80"><a href="#cb13-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-81"><a href="#cb13-81" aria-hidden="true" tabindex="-1"></a>Reconocemos el parámetro natural</span>
<span id="cb13-82"><a href="#cb13-82" aria-hidden="true" tabindex="-1"></a>$$\eta=\log\left(\frac{\phi}{1-\phi}\right)$$</span>
<span id="cb13-83"><a href="#cb13-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-84"><a href="#cb13-84" aria-hidden="true" tabindex="-1"></a>mientras que el resto de los elementos son:</span>
<span id="cb13-85"><a href="#cb13-85" aria-hidden="true" tabindex="-1"></a>$$T(y)=y, \qquad a(\eta)=-\log(1-\phi)=\log(1+e^\eta), \qquad b(y)=1.$$</span>
<span id="cb13-86"><a href="#cb13-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-87"><a href="#cb13-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-88"><a href="#cb13-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-89"><a href="#cb13-89" aria-hidden="true" tabindex="-1"></a><span class="fu">### Distribución Gaussiana</span></span>
<span id="cb13-90"><a href="#cb13-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-91"><a href="#cb13-91" aria-hidden="true" tabindex="-1"></a>Sea $Y\in\RR$ tal que $Y\sim\mathcal{N}(\mu,\sigma^2)$. Entonces</span>
<span id="cb13-92"><a href="#cb13-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-93"><a href="#cb13-93" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-94"><a href="#cb13-94" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb13-95"><a href="#cb13-95" aria-hidden="true" tabindex="-1"></a>p(y; \mu, \sigma^2) &amp; = \frac{1}{\sqrt{2\pi}\sigma} \exp\left(-\frac{(y - \mu)^2}{2\sigma^2}\right) <span class="sc">\\</span></span>
<span id="cb13-96"><a href="#cb13-96" aria-hidden="true" tabindex="-1"></a>&amp; = \frac{1}{\sqrt{2\pi}}\exp\left(-\frac{1}{2\sigma^2}y^2+\frac{\mu}{\sigma^2}y-\frac{\mu^2}{2\sigma^2}-\log\sigma\right) <span class="sc">\\</span></span>
<span id="cb13-97"><a href="#cb13-97" aria-hidden="true" tabindex="-1"></a>&amp; = \frac{1}{\sqrt{2\pi}}\exp\left(\left(\frac{\mu}{\sigma^2},-\frac{1}{2\sigma^2}\right)^T(y,y^2)-\frac{\mu^2}{2\sigma^2}-\log\sigma\right)</span>
<span id="cb13-98"><a href="#cb13-98" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb13-99"><a href="#cb13-99" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-100"><a href="#cb13-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-101"><a href="#cb13-101" aria-hidden="true" tabindex="-1"></a>El parámetro natural es el vector</span>
<span id="cb13-102"><a href="#cb13-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-103"><a href="#cb13-103" aria-hidden="true" tabindex="-1"></a>$$\bfeta=\left(\frac{\mu}{\sigma^2},-\frac{1}{2\sigma^2}\right)$$</span>
<span id="cb13-104"><a href="#cb13-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-105"><a href="#cb13-105" aria-hidden="true" tabindex="-1"></a>y el resto de los elementos son</span>
<span id="cb13-106"><a href="#cb13-106" aria-hidden="true" tabindex="-1"></a>$$\TT(y)=(y,y^2), \qquad a(\bfeta)=\frac{\mu^2}{2\sigma^2}+\log\sigma=-\frac{\eta_1^2}{4\eta_2}+\frac{1}{2}\log(-2\eta_2), \qquad b(y)=\frac{1}{\sqrt{2\pi}}.$$</span>
<span id="cb13-107"><a href="#cb13-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-108"><a href="#cb13-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-109"><a href="#cb13-109" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb13-110"><a href="#cb13-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-111"><a href="#cb13-111" aria-hidden="true" tabindex="-1"></a>Existen muchas otras distribuciones que son miembros de la familia exponencial: la multinomial (que veremos más adelante), la Poisson (para modelar datos de conteo), la gamma y la exponencial (para modelar variables continuas no negativas, como intervalos de tiempo); la beta y la Dirichlet (para distribuciones sobre probabilidades); y muchas más. </span>
<span id="cb13-112"><a href="#cb13-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-113"><a href="#cb13-113" aria-hidden="true" tabindex="-1"></a>En la próxima sección, describiremos una "receta" general para construir modelos en los cuales $y$ (dado $\xx$ y $\theta$) proviene de cualquiera de estas distribuciones.</span>
<span id="cb13-114"><a href="#cb13-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-115"><a href="#cb13-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-116"><a href="#cb13-116" aria-hidden="true" tabindex="-1"></a><span class="fu"># Construcción de GLMs</span></span>
<span id="cb13-117"><a href="#cb13-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-118"><a href="#cb13-118" aria-hidden="true" tabindex="-1"></a>Supongamos que deseas construir un modelo para estimar el número $y$ de clientes que llegan a tu tienda (o el número de visitas a páginas web en tu sitio) en una hora determinada, basándote en ciertas características como promociones en la tienda, publicidad reciente, clima, día de la semana, etc. Sabemos que la distribución Poisson usualmente proporciona un buen modelo para el número de visitantes. Sabiendo esto, ¿cómo podemos proponer un modelo para nuestro problema? Afortunadamente, la Poisson es una distribución de la familia exponencial, por lo que podemos aplicar un Modelo Lineal Generalizado (GLM). En esta sección, describiremos un método para construir modelos GLM para problemas como este.</span>
<span id="cb13-119"><a href="#cb13-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-120"><a href="#cb13-120" aria-hidden="true" tabindex="-1"></a>Más generalmente, consideremos un problema de regresión o clasificación o regresión donde queremos predecir el valor de alguna variable aleatoria $y$ como función de $\xx$. Para derivar un GLM para este problema, haremos las siguientes tres suposiciones:</span>
<span id="cb13-121"><a href="#cb13-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-122"><a href="#cb13-122" aria-hidden="true" tabindex="-1"></a>::: {.highlight}</span>
<span id="cb13-123"><a href="#cb13-123" aria-hidden="true" tabindex="-1"></a>**Suposición 1:**  $y|\xx;\bftheta\sim\text{FamiliaExponencial}(\bfeta)$.</span>
<span id="cb13-124"><a href="#cb13-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-125"><a href="#cb13-125" aria-hidden="true" tabindex="-1"></a>**Suposición 2:** Dado $\xx$, el objetivo es predecir $\media<span class="co">[</span><span class="ot">T(y)|\xx</span><span class="co">]</span>$.</span>
<span id="cb13-126"><a href="#cb13-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-127"><a href="#cb13-127" aria-hidden="true" tabindex="-1"></a>**Suposición 3:** El parámetro natural $\bfeta$ y las predictoras $\xx$ están relacionadas linealmente; esto es, $\bfeta=\bftheta^T\xx$.</span>
<span id="cb13-128"><a href="#cb13-128" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-129"><a href="#cb13-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-130"><a href="#cb13-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-131"><a href="#cb13-131" aria-hidden="true" tabindex="-1"></a>Generalmente $T(y)=y$ y, en tal caso, la suposición 2 significa que queremos que la predicción sea $h(\xx)=\media<span class="co">[</span><span class="ot">\xx</span><span class="co">]</span>$. Por otra parte, si el parámetro natural $\bfeta$ es un vector, observar que la suposición 3 implica que $\bftheta$ debe ser una matriz. Esta última suposición podría parecer la menos justificada de las anteriores, y podría considerarse más como una "elección de diseño" en nuestra receta para diseñar GLMs, en lugar de una suposición como tal. </span>
<span id="cb13-132"><a href="#cb13-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-133"><a href="#cb13-133" aria-hidden="true" tabindex="-1"></a>Estas tres suposiciones o elecciones de diseño nos permitirán derivar una clase muy elegante de algoritmos de aprendizaje, a saber, los GLMs, que tienen muchas propiedades deseables, como la facilidad de aprendizaje. Además, los modelos resultantes son frecuentemente muy efectivos.</span>
<span id="cb13-134"><a href="#cb13-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-135"><a href="#cb13-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-136"><a href="#cb13-136" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb13-137"><a href="#cb13-137" aria-hidden="true" tabindex="-1"></a><span class="fu">## Mínimos Cuadrados Ordinarios</span></span>
<span id="cb13-138"><a href="#cb13-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-139"><a href="#cb13-139" aria-hidden="true" tabindex="-1"></a>Para demostrar que los mínimos cuadrados ordinarios son un caso especial de la familia de modelos GLM, consideremos el escenario en el que la variable objetivo $y$ (también llamada *variable de respuesta* en la terminología GLM) es continua, y modelamos la distribución condicional de $y$ dado $x$ como una Gaussiana $\mathcal{N}(\mu, \sigma^2)$. (Aquí, $\mu$ puede depender de $x$.) Por lo tanto, tomamos la distribución $\text{ExponentialFamily}(\eta)$ sobre la distribución Gaussiana anterior. Como vimos previamente, en la formulación de la distribución Gaussiana como una distribución de la familia exponencial, tenemos $\mu = \eta$. Así que tenemos:</span>
<span id="cb13-140"><a href="#cb13-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-141"><a href="#cb13-141" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-142"><a href="#cb13-142" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb13-143"><a href="#cb13-143" aria-hidden="true" tabindex="-1"></a>h_\theta(x) &amp;= \mathbb{E}<span class="co">[</span><span class="ot">y | x; \theta</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb13-144"><a href="#cb13-144" aria-hidden="true" tabindex="-1"></a>&amp;= \mu <span class="sc">\\</span> &amp;= \eta <span class="sc">\\</span>&amp;= \theta^T x.</span>
<span id="cb13-145"><a href="#cb13-145" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb13-146"><a href="#cb13-146" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-147"><a href="#cb13-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-148"><a href="#cb13-148" aria-hidden="true" tabindex="-1"></a>La primera igualdad se deduce de la Suposición 2 mencionada anteriormente; la segunda igualdad se deduce del hecho de que $y | x; \theta \sim \mathcal{N}(\mu, \sigma^2)$, y por lo tanto su valor esperado está dado por $\mu$; la tercera igualdad se deduce de la Suposición 1 (y nuestra derivación anterior mostrando que $\mu = \eta$ en la formulación de la Gaussiana como una distribución de la familia exponencial); y la última igualdad se deduce de la Suposición 3.</span>
<span id="cb13-149"><a href="#cb13-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-150"><a href="#cb13-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-151"><a href="#cb13-151" aria-hidden="true" tabindex="-1"></a>PARA MI ESSTos ejempLOS VAN DEPSUES DE DECIR COMO SE ESTIMAN LOS PARAMETROS EN GENERAL EN LOS MODELOS GENERALIZADOS.</span>
<span id="cb13-152"><a href="#cb13-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-153"><a href="#cb13-153" aria-hidden="true" tabindex="-1"></a><span class="fu">## Ejemplo en Python: Mínimos Cuadrados Ordinarios</span></span>
<span id="cb13-154"><a href="#cb13-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-155"><a href="#cb13-155" aria-hidden="true" tabindex="-1"></a>A continuación, implementamos un modelo de regresión lineal que ilustra cómo se ajustan los datos siguiendo un enfoque de mínimos cuadrados ordinarios.</span>
<span id="cb13-156"><a href="#cb13-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-159"><a href="#cb13-159" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb13-160"><a href="#cb13-160" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb13-161"><a href="#cb13-161" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb13-162"><a href="#cb13-162" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb13-163"><a href="#cb13-163" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error, r2_score</span>
<span id="cb13-164"><a href="#cb13-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-165"><a href="#cb13-165" aria-hidden="true" tabindex="-1"></a><span class="co"># Generar datos simulados</span></span>
<span id="cb13-166"><a href="#cb13-166" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb13-167"><a href="#cb13-167" aria-hidden="true" tabindex="-1"></a>n_samples <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb13-168"><a href="#cb13-168" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> np.random.rand(n_samples, <span class="dv">1</span>)  <span class="co"># Características (una variable predictora)</span></span>
<span id="cb13-169"><a href="#cb13-169" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="dv">4</span> <span class="op">+</span> <span class="dv">3</span> <span class="op">*</span> X <span class="op">+</span> np.random.randn(n_samples, <span class="dv">1</span>)  <span class="co"># Respuesta con ruido</span></span>
<span id="cb13-170"><a href="#cb13-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-171"><a href="#cb13-171" aria-hidden="true" tabindex="-1"></a><span class="co"># Dividir en entrenamiento y prueba</span></span>
<span id="cb13-172"><a href="#cb13-172" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb13-173"><a href="#cb13-173" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb13-174"><a href="#cb13-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-175"><a href="#cb13-175" aria-hidden="true" tabindex="-1"></a><span class="co"># Ajustar un modelo de regresión lineal</span></span>
<span id="cb13-176"><a href="#cb13-176" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LinearRegression()</span>
<span id="cb13-177"><a href="#cb13-177" aria-hidden="true" tabindex="-1"></a>model.fit(X_train, y_train)</span>
<span id="cb13-178"><a href="#cb13-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-179"><a href="#cb13-179" aria-hidden="true" tabindex="-1"></a><span class="co"># Predicciones</span></span>
<span id="cb13-180"><a href="#cb13-180" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model.predict(X_test)</span>
<span id="cb13-181"><a href="#cb13-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-182"><a href="#cb13-182" aria-hidden="true" tabindex="-1"></a><span class="co"># Parámetros ajustados</span></span>
<span id="cb13-183"><a href="#cb13-183" aria-hidden="true" tabindex="-1"></a>theta_0 <span class="op">=</span> model.intercept_  <span class="co"># Intercepto</span></span>
<span id="cb13-184"><a href="#cb13-184" aria-hidden="true" tabindex="-1"></a>theta_1 <span class="op">=</span> model.coef_       <span class="co"># Pendiente</span></span>
<span id="cb13-185"><a href="#cb13-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-186"><a href="#cb13-186" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualizar los resultados</span></span>
<span id="cb13-187"><a href="#cb13-187" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb13-188"><a href="#cb13-188" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_test, y_test, color<span class="op">=</span><span class="st">"blue"</span>, label<span class="op">=</span><span class="st">"Datos reales"</span>)</span>
<span id="cb13-189"><a href="#cb13-189" aria-hidden="true" tabindex="-1"></a>plt.plot(X_test, y_pred, color<span class="op">=</span><span class="st">"red"</span>, label<span class="op">=</span><span class="ss">f"Modelo ajustado: $y = </span><span class="sc">{</span>theta_0[<span class="dv">0</span>]<span class="sc">:.2f}</span><span class="ss"> + </span><span class="sc">{</span>theta_1[<span class="dv">0</span>][<span class="dv">0</span>]<span class="sc">:.2f}</span><span class="ss">x$"</span>)</span>
<span id="cb13-190"><a href="#cb13-190" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Variable predictora (X)"</span>)</span>
<span id="cb13-191"><a href="#cb13-191" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Variable respuesta (y)"</span>)</span>
<span id="cb13-192"><a href="#cb13-192" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Mínimos Cuadrados Ordinarios"</span>)</span>
<span id="cb13-193"><a href="#cb13-193" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb13-194"><a href="#cb13-194" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb13-195"><a href="#cb13-195" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb13-196"><a href="#cb13-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-197"><a href="#cb13-197" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluar el modelo</span></span>
<span id="cb13-198"><a href="#cb13-198" aria-hidden="true" tabindex="-1"></a>mse <span class="op">=</span> mean_squared_error(y_test, y_pred)</span>
<span id="cb13-199"><a href="#cb13-199" aria-hidden="true" tabindex="-1"></a>r2 <span class="op">=</span> r2_score(y_test, y_pred)</span>
<span id="cb13-200"><a href="#cb13-200" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Error cuadrático medio (MSE): </span><span class="sc">{</span>mse<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb13-201"><a href="#cb13-201" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Coeficiente de determinación (R^2): </span><span class="sc">{</span>r2<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb13-202"><a href="#cb13-202" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-203"><a href="#cb13-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-204"><a href="#cb13-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-205"><a href="#cb13-205" aria-hidden="true" tabindex="-1"></a><span class="fu">## Regresión Logística</span></span>
<span id="cb13-206"><a href="#cb13-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-207"><a href="#cb13-207" aria-hidden="true" tabindex="-1"></a>Ahora consideramos la regresión logística. Aquí estamos interesados en la clasificación binaria, por lo que $y \in <span class="sc">\{</span>0, 1<span class="sc">\}</span>$. Dado que $y$ es un valor binario, parece natural elegir la familia Bernoulli de distribuciones para modelar la distribución condicional de $y$ dado $x$. En nuestra formulación de la distribución Bernoulli como una distribución de la familia exponencial, teníamos $\phi = 1/(1 + e^{-\eta})$. Además, nota que si $y | x; \theta \sim \text{Bernoulli}(\phi)$, entonces $\mathbb{E}<span class="co">[</span><span class="ot">y | x; \theta</span><span class="co">]</span> = \phi$. Por lo tanto, siguiendo una derivación similar a la de los mínimos cuadrados ordinarios, obtenemos:</span>
<span id="cb13-208"><a href="#cb13-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-209"><a href="#cb13-209" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-210"><a href="#cb13-210" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb13-211"><a href="#cb13-211" aria-hidden="true" tabindex="-1"></a>h_\theta(x) = \mathbb{E}<span class="co">[</span><span class="ot">y | x; \theta</span><span class="co">]</span> = \phi = \frac{1}{1 + e^{-\eta}} = \frac{1}{1 + e^{-\theta^T x}}.</span>
<span id="cb13-212"><a href="#cb13-212" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb13-213"><a href="#cb13-213" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-214"><a href="#cb13-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-215"><a href="#cb13-215" aria-hidden="true" tabindex="-1"></a>Así, obtenemos funciones hipótesis de la forma:</span>
<span id="cb13-216"><a href="#cb13-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-217"><a href="#cb13-217" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-218"><a href="#cb13-218" aria-hidden="true" tabindex="-1"></a>h_\theta(x) = \frac{1}{1 + e^{-\theta^T x}}.</span>
<span id="cb13-219"><a href="#cb13-219" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-220"><a href="#cb13-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-221"><a href="#cb13-221" aria-hidden="true" tabindex="-1"></a>Si alguna vez te preguntaste cómo llegamos a la forma de la función logística $1/(1 + e^{-z})$, esta es la respuesta: Una vez que asumimos que $y$ condicionado en $x$ es Bernoulli, surge como una consecuencia de la definición de los GLMs y las distribuciones de la familia exponencial.</span>
<span id="cb13-222"><a href="#cb13-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-223"><a href="#cb13-223" aria-hidden="true" tabindex="-1"></a><span class="fu">### Terminología</span></span>
<span id="cb13-224"><a href="#cb13-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-225"><a href="#cb13-225" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>La **función de respuesta canónica**, $g$, da la media de la distribución como una función del parámetro natural: $g(\eta) = \mathbb{E}<span class="co">[</span><span class="ot">T(y); \eta</span><span class="co">]</span>$.</span>
<span id="cb13-226"><a href="#cb13-226" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>La inversa de esta función, $g^{-1}$, se llama la **función de enlace canónica**.</span>
<span id="cb13-227"><a href="#cb13-227" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb13-228"><a href="#cb13-228" aria-hidden="true" tabindex="-1"></a>Por ejemplo:</span>
<span id="cb13-229"><a href="#cb13-229" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Para la familia Gaussiana, la función de respuesta canónica es la función identidad.</span>
<span id="cb13-230"><a href="#cb13-230" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Para la familia Bernoulli, la función de respuesta canónica es la función logística.</span>
<span id="cb13-231"><a href="#cb13-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-232"><a href="#cb13-232" aria-hidden="true" tabindex="-1"></a>ESTE TAMBIEN , tiene que haber una seccion de como estimar, luego compu</span>
<span id="cb13-233"><a href="#cb13-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-234"><a href="#cb13-234" aria-hidden="true" tabindex="-1"></a><span class="fu">## Ejemplo en Python: Regresión Logística</span></span>
<span id="cb13-235"><a href="#cb13-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-236"><a href="#cb13-236" aria-hidden="true" tabindex="-1"></a>A continuación, implementamos un ejemplo práctico de regresión logística en un problema de clasificación binaria.</span>
<span id="cb13-237"><a href="#cb13-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-240"><a href="#cb13-240" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb13-241"><a href="#cb13-241" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb13-242"><a href="#cb13-242" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb13-243"><a href="#cb13-243" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb13-244"><a href="#cb13-244" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb13-245"><a href="#cb13-245" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> classification_report, confusion_matrix</span>
<span id="cb13-246"><a href="#cb13-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-247"><a href="#cb13-247" aria-hidden="true" tabindex="-1"></a><span class="co"># Generar datos simulados para clasificación binaria</span></span>
<span id="cb13-248"><a href="#cb13-248" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_classification(</span>
<span id="cb13-249"><a href="#cb13-249" aria-hidden="true" tabindex="-1"></a>    n_samples<span class="op">=</span><span class="dv">200</span>, n_features<span class="op">=</span><span class="dv">2</span>, n_informative<span class="op">=</span><span class="dv">2</span>, n_redundant<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb13-250"><a href="#cb13-250" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span>, class_sep<span class="op">=</span><span class="fl">1.5</span></span>
<span id="cb13-251"><a href="#cb13-251" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb13-252"><a href="#cb13-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-253"><a href="#cb13-253" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualizar los datos</span></span>
<span id="cb13-254"><a href="#cb13-254" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb13-255"><a href="#cb13-255" aria-hidden="true" tabindex="-1"></a>plt.scatter(X[y <span class="op">==</span> <span class="dv">0</span>, <span class="dv">0</span>], X[y <span class="op">==</span> <span class="dv">0</span>, <span class="dv">1</span>], color<span class="op">=</span><span class="st">"blue"</span>, label<span class="op">=</span><span class="st">"Clase 0"</span>)</span>
<span id="cb13-256"><a href="#cb13-256" aria-hidden="true" tabindex="-1"></a>plt.scatter(X[y <span class="op">==</span> <span class="dv">1</span>, <span class="dv">0</span>], X[y <span class="op">==</span> <span class="dv">1</span>, <span class="dv">1</span>], color<span class="op">=</span><span class="st">"red"</span>, label<span class="op">=</span><span class="st">"Clase 1"</span>)</span>
<span id="cb13-257"><a href="#cb13-257" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Datos de Clasificación Binaria"</span>)</span>
<span id="cb13-258"><a href="#cb13-258" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Característica 1"</span>)</span>
<span id="cb13-259"><a href="#cb13-259" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Característica 2"</span>)</span>
<span id="cb13-260"><a href="#cb13-260" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb13-261"><a href="#cb13-261" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb13-262"><a href="#cb13-262" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb13-263"><a href="#cb13-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-264"><a href="#cb13-264" aria-hidden="true" tabindex="-1"></a><span class="co"># Dividir datos en entrenamiento y prueba</span></span>
<span id="cb13-265"><a href="#cb13-265" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb13-266"><a href="#cb13-266" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb13-267"><a href="#cb13-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-268"><a href="#cb13-268" aria-hidden="true" tabindex="-1"></a><span class="co"># Ajustar un modelo de regresión logística</span></span>
<span id="cb13-269"><a href="#cb13-269" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression()</span>
<span id="cb13-270"><a href="#cb13-270" aria-hidden="true" tabindex="-1"></a>model.fit(X_train, y_train)</span>
<span id="cb13-271"><a href="#cb13-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-272"><a href="#cb13-272" aria-hidden="true" tabindex="-1"></a><span class="co"># Predicciones</span></span>
<span id="cb13-273"><a href="#cb13-273" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model.predict(X_test)</span>
<span id="cb13-274"><a href="#cb13-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-275"><a href="#cb13-275" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualización de la frontera de decisión</span></span>
<span id="cb13-276"><a href="#cb13-276" aria-hidden="true" tabindex="-1"></a>x_min, x_max <span class="op">=</span> X[:, <span class="dv">0</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="dv">1</span>, X[:, <span class="dv">0</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb13-277"><a href="#cb13-277" aria-hidden="true" tabindex="-1"></a>y_min, y_max <span class="op">=</span> X[:, <span class="dv">1</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="dv">1</span>, X[:, <span class="dv">1</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb13-278"><a href="#cb13-278" aria-hidden="true" tabindex="-1"></a>xx, yy <span class="op">=</span> np.meshgrid(np.arange(x_min, x_max, <span class="fl">0.01</span>),</span>
<span id="cb13-279"><a href="#cb13-279" aria-hidden="true" tabindex="-1"></a>                     np.arange(y_min, y_max, <span class="fl">0.01</span>))</span>
<span id="cb13-280"><a href="#cb13-280" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> model.predict(np.c_[xx.ravel(), yy.ravel()])</span>
<span id="cb13-281"><a href="#cb13-281" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> Z.reshape(xx.shape)</span>
<span id="cb13-282"><a href="#cb13-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-283"><a href="#cb13-283" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb13-284"><a href="#cb13-284" aria-hidden="true" tabindex="-1"></a>plt.contourf(xx, yy, Z, alpha<span class="op">=</span><span class="fl">0.8</span>, cmap<span class="op">=</span><span class="st">"coolwarm"</span>)</span>
<span id="cb13-285"><a href="#cb13-285" aria-hidden="true" tabindex="-1"></a>plt.scatter(X[y <span class="op">==</span> <span class="dv">0</span>, <span class="dv">0</span>], X[y <span class="op">==</span> <span class="dv">0</span>, <span class="dv">1</span>], color<span class="op">=</span><span class="st">"blue"</span>, label<span class="op">=</span><span class="st">"Clase 0"</span>)</span>
<span id="cb13-286"><a href="#cb13-286" aria-hidden="true" tabindex="-1"></a>plt.scatter(X[y <span class="op">==</span> <span class="dv">1</span>, <span class="dv">0</span>], X[y <span class="op">==</span> <span class="dv">1</span>, <span class="dv">1</span>], color<span class="op">=</span><span class="st">"red"</span>, label<span class="op">=</span><span class="st">"Clase 1"</span>)</span>
<span id="cb13-287"><a href="#cb13-287" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Frontera de Decisión de Regresión Logística"</span>)</span>
<span id="cb13-288"><a href="#cb13-288" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Característica 1"</span>)</span>
<span id="cb13-289"><a href="#cb13-289" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Característica 2"</span>)</span>
<span id="cb13-290"><a href="#cb13-290" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb13-291"><a href="#cb13-291" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb13-292"><a href="#cb13-292" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb13-293"><a href="#cb13-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-294"><a href="#cb13-294" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluación del modelo</span></span>
<span id="cb13-295"><a href="#cb13-295" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Reporte de Clasificación:"</span>)</span>
<span id="cb13-296"><a href="#cb13-296" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_test, y_pred))</span>
<span id="cb13-297"><a href="#cb13-297" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Matriz de Confusión:"</span>)</span>
<span id="cb13-298"><a href="#cb13-298" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(confusion_matrix(y_test, y_pred))</span>
<span id="cb13-299"><a href="#cb13-299" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-300"><a href="#cb13-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-301"><a href="#cb13-301" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb13-302"><a href="#cb13-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-303"><a href="#cb13-303" aria-hidden="true" tabindex="-1"></a><span class="fu">## Regresión Softmax</span></span>
<span id="cb13-304"><a href="#cb13-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-305"><a href="#cb13-305" aria-hidden="true" tabindex="-1"></a><span class="fu">### Contexto</span></span>
<span id="cb13-306"><a href="#cb13-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-307"><a href="#cb13-307" aria-hidden="true" tabindex="-1"></a>Consideremos un problema de clasificación en el que la variable de respuesta $yConsideremos un problema de clasificación en el que la variable de respuesta $yConsideremos un problema de clasificación en el que la variable de respuesta $yConsideremos un problema de clasificación en el que la variable de respuesta $y$ puede tomar cualquiera de <span class="sc">\(</span>k<span class="sc">\)</span> valores, por lo que $y \in <span class="sc">\{</span>1, 2, \ldots, k<span class="sc">\}</span>$. Por ejemplo, en lugar de clasificar correos electrónicos en dos clases (spam o no spam), podríamos clasificarlos en tres clases: spam, correo personal y correo relacionado con el trabajo. Para modelar esta situación, usaremos la **distribución multinomial** como una distribución de la familia exponencial.</span>
<span id="cb13-308"><a href="#cb13-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-309"><a href="#cb13-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-310"><a href="#cb13-310" aria-hidden="true" tabindex="-1"></a><span class="fu">### La Multinomial como Distribución de la Familia Exponencial</span></span>
<span id="cb13-311"><a href="#cb13-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-312"><a href="#cb13-312" aria-hidden="true" tabindex="-1"></a>Derivemos un GLM para datos multinomiales parametrizando la multinomial con $k-1$ parámetros $\phi_1, \dots, \phi_{k-1}$, dado que $\sum_{i=1}^k \phi_i = 1$. Para expresar la multinomial como una distribución de la familia exponencial, definimos:</span>
<span id="cb13-313"><a href="#cb13-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-314"><a href="#cb13-314" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-315"><a href="#cb13-315" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb13-316"><a href="#cb13-316" aria-hidden="true" tabindex="-1"></a>T(1)&amp; = \begin{bmatrix} 1 <span class="sc">\\</span> 0 <span class="sc">\\</span> \vdots <span class="sc">\\</span> 0 \end{bmatrix}, <span class="sc">\\</span></span>
<span id="cb13-317"><a href="#cb13-317" aria-hidden="true" tabindex="-1"></a>T(2) &amp;= \begin{bmatrix} 0 <span class="sc">\\</span> 1 <span class="sc">\\</span> \vdots <span class="sc">\\</span> 0 \end{bmatrix}, <span class="sc">\\</span> \dots, <span class="sc">\\</span></span>
<span id="cb13-318"><a href="#cb13-318" aria-hidden="true" tabindex="-1"></a>T(k) &amp;= \begin{bmatrix} 0 <span class="sc">\\</span> 0 <span class="sc">\\</span> \vdots <span class="sc">\\</span> 0 \end{bmatrix}.</span>
<span id="cb13-319"><a href="#cb13-319" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb13-320"><a href="#cb13-320" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-321"><a href="#cb13-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-322"><a href="#cb13-322" aria-hidden="true" tabindex="-1"></a> La distribución multinomial pertenece a la familia exponencial. Tenemos:</span>
<span id="cb13-323"><a href="#cb13-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-324"><a href="#cb13-324" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-325"><a href="#cb13-325" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb13-326"><a href="#cb13-326" aria-hidden="true" tabindex="-1"></a>p(y; \phi)&amp; =</span>
<span id="cb13-327"><a href="#cb13-327" aria-hidden="true" tabindex="-1"></a>\phi_1^{1<span class="sc">\{</span>y=1<span class="sc">\}</span>} \phi_2^{1<span class="sc">\{</span>y=2<span class="sc">\}</span>} \cdots \phi_k^{1<span class="sc">\{</span>y=k<span class="sc">\}</span>}<span class="sc">\\</span></span>
<span id="cb13-328"><a href="#cb13-328" aria-hidden="true" tabindex="-1"></a>&amp;= \phi_1^{(T(y))_1} \phi_2^{(T(y))_2} \cdots \phi_k^{1-\sum_{i=1}^{k-1}(T(y))_i}</span>
<span id="cb13-329"><a href="#cb13-329" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span></span>
<span id="cb13-330"><a href="#cb13-330" aria-hidden="true" tabindex="-1"></a>&amp;= \phi_1^{(T(y))_1} \phi_2^{(T(y))_2} \cdots \phi_{k-1}^{(T(y))_{k-1}} \left(1 - \sum_{i=1}^{k-1} \phi_i\right)^{1-\sum_{i=1}^{k-1}(T(y))_i}</span>
<span id="cb13-331"><a href="#cb13-331" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span></span>
<span id="cb13-332"><a href="#cb13-332" aria-hidden="true" tabindex="-1"></a>&amp;= \exp\left((T(y))_1 \log(\phi_1) + (T(y))_2 \log(\phi_2) + \dots + \left(1 - \sum_{i=1}^{k-1}(T(y))_i\right)\log\left(1 - \sum_{i=1}^{k-1} \phi_i\right)\right).</span>
<span id="cb13-333"><a href="#cb13-333" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb13-334"><a href="#cb13-334" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-335"><a href="#cb13-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-336"><a href="#cb13-336" aria-hidden="true" tabindex="-1"></a>Finalmente, organizamos los términos en la forma estándar de la familia exponencial:</span>
<span id="cb13-337"><a href="#cb13-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-338"><a href="#cb13-338" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-339"><a href="#cb13-339" aria-hidden="true" tabindex="-1"></a>p(y; \phi) = b(y) \exp\left(\eta^T T(y) - a(\eta)\right),</span>
<span id="cb13-340"><a href="#cb13-340" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-341"><a href="#cb13-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-342"><a href="#cb13-342" aria-hidden="true" tabindex="-1"></a>donde:</span>
<span id="cb13-343"><a href="#cb13-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-344"><a href="#cb13-344" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-345"><a href="#cb13-345" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb13-346"><a href="#cb13-346" aria-hidden="true" tabindex="-1"></a>\eta &amp;= \begin{bmatrix}</span>
<span id="cb13-347"><a href="#cb13-347" aria-hidden="true" tabindex="-1"></a>\log\left(\frac{\phi_1}{\phi_k}\right) <span class="sc">\\</span></span>
<span id="cb13-348"><a href="#cb13-348" aria-hidden="true" tabindex="-1"></a>\log\left(\frac{\phi_2}{\phi_k}\right) <span class="sc">\\</span></span>
<span id="cb13-349"><a href="#cb13-349" aria-hidden="true" tabindex="-1"></a>\vdots <span class="sc">\\</span></span>
<span id="cb13-350"><a href="#cb13-350" aria-hidden="true" tabindex="-1"></a>\log\left(\frac{\phi_{k-1}}{\phi_k}\right)</span>
<span id="cb13-351"><a href="#cb13-351" aria-hidden="true" tabindex="-1"></a>\end{bmatrix},</span>
<span id="cb13-352"><a href="#cb13-352" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span></span>
<span id="cb13-353"><a href="#cb13-353" aria-hidden="true" tabindex="-1"></a>a(\eta) &amp;= -\log(\phi_k),</span>
<span id="cb13-354"><a href="#cb13-354" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span></span>
<span id="cb13-355"><a href="#cb13-355" aria-hidden="true" tabindex="-1"></a>b(y)&amp; = 1.</span>
<span id="cb13-356"><a href="#cb13-356" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb13-357"><a href="#cb13-357" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-358"><a href="#cb13-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-359"><a href="#cb13-359" aria-hidden="true" tabindex="-1"></a>Esto completa nuestra formulación de la multinomial como una distribución de la familia exponencial.</span>
<span id="cb13-360"><a href="#cb13-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-361"><a href="#cb13-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-362"><a href="#cb13-362" aria-hidden="true" tabindex="-1"></a>La función que mapea <span class="sc">\(</span>\eta<span class="sc">\)</span> a <span class="sc">\(</span>\phi<span class="sc">\)</span> se llama la **función softmax**:</span>
<span id="cb13-363"><a href="#cb13-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-364"><a href="#cb13-364" aria-hidden="true" tabindex="-1"></a><span class="fu">### Función de Enlace y Función de Respuesta</span></span>
<span id="cb13-365"><a href="#cb13-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-366"><a href="#cb13-366" aria-hidden="true" tabindex="-1"></a>La función de enlace está dada (para $i = 1, \dots, k$) por:</span>
<span id="cb13-367"><a href="#cb13-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-368"><a href="#cb13-368" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-369"><a href="#cb13-369" aria-hidden="true" tabindex="-1"></a>\eta_i = \log\left(\frac{\phi_i}{\phi_k}\right).</span>
<span id="cb13-370"><a href="#cb13-370" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-371"><a href="#cb13-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-372"><a href="#cb13-372" aria-hidden="true" tabindex="-1"></a>Para conveniencia, también definimos $\eta_k = \log(\phi_k / \phi_k) = 0$. Para invertir la función de enlace y derivar la función de respuesta, tenemos que:</span>
<span id="cb13-373"><a href="#cb13-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-374"><a href="#cb13-374" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-375"><a href="#cb13-375" aria-hidden="true" tabindex="-1"></a>e^{\eta_i} = \frac{\phi_i}{\phi_k},</span>
<span id="cb13-376"><a href="#cb13-376" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-377"><a href="#cb13-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-378"><a href="#cb13-378" aria-hidden="true" tabindex="-1"></a>lo que implica:</span>
<span id="cb13-379"><a href="#cb13-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-380"><a href="#cb13-380" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-381"><a href="#cb13-381" aria-hidden="true" tabindex="-1"></a>\phi_k \sum_{i=1}^k e^{\eta_i} = \sum_{i=1}^k \phi_i = 1.</span>
<span id="cb13-382"><a href="#cb13-382" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-383"><a href="#cb13-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-384"><a href="#cb13-384" aria-hidden="true" tabindex="-1"></a>Por lo tanto:</span>
<span id="cb13-385"><a href="#cb13-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-386"><a href="#cb13-386" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-387"><a href="#cb13-387" aria-hidden="true" tabindex="-1"></a>\phi_k = \frac{1}{\sum_{i=1}^k e^{\eta_i}},</span>
<span id="cb13-388"><a href="#cb13-388" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-389"><a href="#cb13-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-390"><a href="#cb13-390" aria-hidden="true" tabindex="-1"></a>y al sustituir en la ecuación obtenemos la función de respuesta:</span>
<span id="cb13-391"><a href="#cb13-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-392"><a href="#cb13-392" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-393"><a href="#cb13-393" aria-hidden="true" tabindex="-1"></a>\phi_i = \frac{e^{\eta_i}}{\sum_{j=1}^k e^{\eta_j}}.</span>
<span id="cb13-394"><a href="#cb13-394" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-395"><a href="#cb13-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-396"><a href="#cb13-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-397"><a href="#cb13-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-398"><a href="#cb13-398" aria-hidden="true" tabindex="-1"></a><span class="fu">## Softmax Function y Modelo de Clasificación</span></span>
<span id="cb13-399"><a href="#cb13-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-400"><a href="#cb13-400" aria-hidden="true" tabindex="-1"></a>La función que mapea de $\eta$ a $\phi$ se llama la función \textit{softmax}. </span>
<span id="cb13-401"><a href="#cb13-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-402"><a href="#cb13-402" aria-hidden="true" tabindex="-1"></a>Para completar nuestro modelo, usamos la Asunción 3, mencionada previamente, donde los $\eta_i$ están relacionados linealmente con los $x$'s. Así, tenemos $\eta_i = \theta_i^T x$ (para $i = 1, \dots, k - 1$), donde $\theta_1, \dots, \theta_{k-1} \in \mathbb{R}^{d+1}$ son los parámetros de nuestro modelo. Por notación, podemos definir $\theta_k = 0$, de modo que $\eta_k = \theta_k^T x = 0$, como se definió anteriormente. Por lo tanto, nuestro modelo asume que la distribución condicional de $y$ dado $x$ está dada por:</span>
<span id="cb13-403"><a href="#cb13-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-404"><a href="#cb13-404" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-405"><a href="#cb13-405" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb13-406"><a href="#cb13-406" aria-hidden="true" tabindex="-1"></a>p(y = i | x; \theta) &amp;= \phi_i<span class="sc">\\</span></span>
<span id="cb13-407"><a href="#cb13-407" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{e^{\eta_i}}{\sum_{j=1}^k e^{\eta_j}}<span class="sc">\\</span></span>
<span id="cb13-408"><a href="#cb13-408" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{e^{\theta_i^T x}}{\sum_{j=1}^k e^{\theta_j^T x}}, \quad (5)</span>
<span id="cb13-409"><a href="#cb13-409" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb13-410"><a href="#cb13-410" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-411"><a href="#cb13-411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-412"><a href="#cb13-412" aria-hidden="true" tabindex="-1"></a>donde este modelo, que aplica a problemas de clasificación donde $y \in <span class="sc">\{</span>1, \dots, k<span class="sc">\}</span>$, se llama \textit{regresión softmax}. Es una generalización de la regresión logística.</span>
<span id="cb13-413"><a href="#cb13-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-414"><a href="#cb13-414" aria-hidden="true" tabindex="-1"></a><span class="fu">### Hipótesis del Modelo</span></span>
<span id="cb13-415"><a href="#cb13-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-416"><a href="#cb13-416" aria-hidden="true" tabindex="-1"></a>Nuestra hipótesis producirá:</span>
<span id="cb13-417"><a href="#cb13-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-418"><a href="#cb13-418" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-419"><a href="#cb13-419" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb13-420"><a href="#cb13-420" aria-hidden="true" tabindex="-1"></a>h_\theta(x) &amp;= \mathbb{E}<span class="co">[</span><span class="ot">T(y) | x; \theta</span><span class="co">]</span></span>
<span id="cb13-421"><a href="#cb13-421" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span></span>
<span id="cb13-422"><a href="#cb13-422" aria-hidden="true" tabindex="-1"></a>&amp;= \mathbb{E}</span>
<span id="cb13-423"><a href="#cb13-423" aria-hidden="true" tabindex="-1"></a>\begin{bmatrix}</span>
<span id="cb13-424"><a href="#cb13-424" aria-hidden="true" tabindex="-1"></a>1<span class="sc">\{</span>y = 1<span class="sc">\}</span> <span class="sc">\\</span></span>
<span id="cb13-425"><a href="#cb13-425" aria-hidden="true" tabindex="-1"></a>1<span class="sc">\{</span>y = 2<span class="sc">\}</span> <span class="sc">\\</span></span>
<span id="cb13-426"><a href="#cb13-426" aria-hidden="true" tabindex="-1"></a>\vdots <span class="sc">\\</span></span>
<span id="cb13-427"><a href="#cb13-427" aria-hidden="true" tabindex="-1"></a>1<span class="sc">\{</span>y = k - 1<span class="sc">\}</span></span>
<span id="cb13-428"><a href="#cb13-428" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}</span>
<span id="cb13-429"><a href="#cb13-429" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span>&amp;=</span>
<span id="cb13-430"><a href="#cb13-430" aria-hidden="true" tabindex="-1"></a>\begin{bmatrix}</span>
<span id="cb13-431"><a href="#cb13-431" aria-hidden="true" tabindex="-1"></a>\phi_1 <span class="sc">\\</span></span>
<span id="cb13-432"><a href="#cb13-432" aria-hidden="true" tabindex="-1"></a>\phi_2 <span class="sc">\\</span></span>
<span id="cb13-433"><a href="#cb13-433" aria-hidden="true" tabindex="-1"></a>\vdots <span class="sc">\\</span></span>
<span id="cb13-434"><a href="#cb13-434" aria-hidden="true" tabindex="-1"></a>\phi_{k-1}</span>
<span id="cb13-435"><a href="#cb13-435" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}</span>
<span id="cb13-436"><a href="#cb13-436" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span>&amp;=</span>
<span id="cb13-437"><a href="#cb13-437" aria-hidden="true" tabindex="-1"></a>\begin{bmatrix}</span>
<span id="cb13-438"><a href="#cb13-438" aria-hidden="true" tabindex="-1"></a>\frac{\exp(\theta_1^T x)}{\sum_{j=1}^k \exp(\theta_j^T x)} <span class="sc">\\</span></span>
<span id="cb13-439"><a href="#cb13-439" aria-hidden="true" tabindex="-1"></a>\frac{\exp(\theta_2^T x)}{\sum_{j=1}^k \exp(\theta_j^T x)} <span class="sc">\\</span></span>
<span id="cb13-440"><a href="#cb13-440" aria-hidden="true" tabindex="-1"></a>\vdots <span class="sc">\\</span></span>
<span id="cb13-441"><a href="#cb13-441" aria-hidden="true" tabindex="-1"></a>\frac{\exp(\theta_{k-1}^T x)}{\sum_{j=1}^k \exp(\theta_j^T x)}</span>
<span id="cb13-442"><a href="#cb13-442" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}.</span>
<span id="cb13-443"><a href="#cb13-443" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb13-444"><a href="#cb13-444" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-445"><a href="#cb13-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-446"><a href="#cb13-446" aria-hidden="true" tabindex="-1"></a>En otras palabras, nuestra hipótesis producirá la probabilidad estimada de que $p(y = i | x; \theta)$ para cada valor $i = 1, \dots, k$. (Aunque $h_\theta(x)$, como se definió arriba, tiene solo $k - 1$ dimensiones, claramente $p(y = k | x; \theta)$ puede obtenerse como $1 - \sum_{i=1}^{k-1} \phi_i$).</span>
<span id="cb13-447"><a href="#cb13-447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-448"><a href="#cb13-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-449"><a href="#cb13-449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-450"><a href="#cb13-450" aria-hidden="true" tabindex="-1"></a><span class="fu">## Ajuste de Parámetros</span></span>
<span id="cb13-451"><a href="#cb13-451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-452"><a href="#cb13-452" aria-hidden="true" tabindex="-1"></a>Por último, discutamos el ajuste de parámetros. Similar a nuestra derivación original de mínimos cuadrados ordinarios y regresión logística, si tenemos un conjunto de entrenamiento de $n$ ejemplos $<span class="sc">\{</span>(x^{(i)}, y^{(i)}); i = 1, \dots, n<span class="sc">\}</span>$ y deseamos aprender los parámetros $\theta_i$ de este modelo, comenzaríamos escribiendo la log-verosimilitud:</span>
<span id="cb13-453"><a href="#cb13-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-454"><a href="#cb13-454" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-455"><a href="#cb13-455" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb13-456"><a href="#cb13-456" aria-hidden="true" tabindex="-1"></a>\ell(\theta) &amp;= \sum_{i=1}^n \log p(y^{(i)} | x^{(i)}; \theta)</span>
<span id="cb13-457"><a href="#cb13-457" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span></span>
<span id="cb13-458"><a href="#cb13-458" aria-hidden="true" tabindex="-1"></a>&amp;= \sum_{i=1}^n \log \prod_{l=1}^k \left( \frac{e^{\theta_l^T x^{(i)}}}{\sum_{j=1}^k e^{\theta_j^T x^{(i)}}} \right)^{1<span class="sc">\{</span>y^{(i)} = l<span class="sc">\}</span>}.</span>
<span id="cb13-459"><a href="#cb13-459" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb13-460"><a href="#cb13-460" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-461"><a href="#cb13-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-462"><a href="#cb13-462" aria-hidden="true" tabindex="-1"></a>Para obtener la segunda línea anterior, usamos la definición de $p(y | x; \theta)$ dada en la Ecuación (5). Ahora podemos obtener la estimación de máxima verosimilitud de los parámetros maximizando $\ell(\theta)$ en términos de $\theta$, utilizando un método como ascenso de gradiente o el método de Newton.</span>
<span id="cb13-463"><a href="#cb13-463" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-464"><a href="#cb13-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-465"><a href="#cb13-465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-466"><a href="#cb13-466" aria-hidden="true" tabindex="-1"></a>ACA FALTA AJUSTE DE PARAMETROS PARA EL MODELO GLM EN GENERAL</span>
<span id="cb13-467"><a href="#cb13-467" aria-hidden="true" tabindex="-1"></a>eso esta en el video de el viejo del 2008 , no esta en el nuevo. </span>
<span id="cb13-468"><a href="#cb13-468" aria-hidden="true" tabindex="-1"></a>seria importante poner algo</span>
<span id="cb13-469"><a href="#cb13-469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-470"><a href="#cb13-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-471"><a href="#cb13-471" aria-hidden="true" tabindex="-1"></a><span class="fu">### Ejemplo Práctico en Python</span></span>
<span id="cb13-472"><a href="#cb13-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-473"><a href="#cb13-473" aria-hidden="true" tabindex="-1"></a>Implementemos un ejemplo práctico para un problema de clasificación multiclasificación.</span>
<span id="cb13-474"><a href="#cb13-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-477"><a href="#cb13-477" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb13-478"><a href="#cb13-478" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb13-479"><a href="#cb13-479" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb13-480"><a href="#cb13-480" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb13-481"><a href="#cb13-481" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb13-482"><a href="#cb13-482" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> classification_report, confusion_matrix</span>
<span id="cb13-483"><a href="#cb13-483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-484"><a href="#cb13-484" aria-hidden="true" tabindex="-1"></a><span class="co"># Generar datos simulados para clasificación multiclase</span></span>
<span id="cb13-485"><a href="#cb13-485" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_classification(</span>
<span id="cb13-486"><a href="#cb13-486" aria-hidden="true" tabindex="-1"></a>    n_samples<span class="op">=</span><span class="dv">300</span>, n_features<span class="op">=</span><span class="dv">2</span>, n_informative<span class="op">=</span><span class="dv">2</span>, n_redundant<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb13-487"><a href="#cb13-487" aria-hidden="true" tabindex="-1"></a>    n_classes<span class="op">=</span><span class="dv">3</span>, n_clusters_per_class<span class="op">=</span><span class="dv">1</span>, random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb13-488"><a href="#cb13-488" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb13-489"><a href="#cb13-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-490"><a href="#cb13-490" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualizar los datos</span></span>
<span id="cb13-491"><a href="#cb13-491" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb13-492"><a href="#cb13-492" aria-hidden="true" tabindex="-1"></a>scatter <span class="op">=</span> plt.scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], c<span class="op">=</span>y, cmap<span class="op">=</span><span class="st">"viridis"</span>)</span>
<span id="cb13-493"><a href="#cb13-493" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Datos de Clasificación Multiclase"</span>)</span>
<span id="cb13-494"><a href="#cb13-494" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Característica 1"</span>)</span>
<span id="cb13-495"><a href="#cb13-495" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Característica 2"</span>)</span>
<span id="cb13-496"><a href="#cb13-496" aria-hidden="true" tabindex="-1"></a>plt.colorbar(scatter, label<span class="op">=</span><span class="st">"Clases"</span>)</span>
<span id="cb13-497"><a href="#cb13-497" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb13-498"><a href="#cb13-498" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb13-499"><a href="#cb13-499" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-500"><a href="#cb13-500" aria-hidden="true" tabindex="-1"></a><span class="co"># Dividir los datos en entrenamiento y prueba</span></span>
<span id="cb13-501"><a href="#cb13-501" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb13-502"><a href="#cb13-502" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb13-503"><a href="#cb13-503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-504"><a href="#cb13-504" aria-hidden="true" tabindex="-1"></a><span class="co"># Ajustar el modelo de regresión softmax</span></span>
<span id="cb13-505"><a href="#cb13-505" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression(multi_class<span class="op">=</span><span class="st">"multinomial"</span>, solver<span class="op">=</span><span class="st">"lbfgs"</span>, max_iter<span class="op">=</span><span class="dv">200</span>)</span>
<span id="cb13-506"><a href="#cb13-506" aria-hidden="true" tabindex="-1"></a>model.fit(X_train, y_train)</span>
<span id="cb13-507"><a href="#cb13-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-508"><a href="#cb13-508" aria-hidden="true" tabindex="-1"></a><span class="co"># Predicciones</span></span>
<span id="cb13-509"><a href="#cb13-509" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model.predict(X_test)</span>
<span id="cb13-510"><a href="#cb13-510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-511"><a href="#cb13-511" aria-hidden="true" tabindex="-1"></a><span class="co"># Frontera de decisión</span></span>
<span id="cb13-512"><a href="#cb13-512" aria-hidden="true" tabindex="-1"></a>x_min, x_max <span class="op">=</span> X[:, <span class="dv">0</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="dv">1</span>, X[:, <span class="dv">0</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb13-513"><a href="#cb13-513" aria-hidden="true" tabindex="-1"></a>y_min, y_max <span class="op">=</span> X[:, <span class="dv">1</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="dv">1</span>, X[:, <span class="dv">1</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb13-514"><a href="#cb13-514" aria-hidden="true" tabindex="-1"></a>xx, yy <span class="op">=</span> np.meshgrid(np.arange(x_min, x_max, <span class="fl">0.01</span>),</span>
<span id="cb13-515"><a href="#cb13-515" aria-hidden="true" tabindex="-1"></a>                     np.arange(y_min, y_max, <span class="fl">0.01</span>))</span>
<span id="cb13-516"><a href="#cb13-516" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> model.predict(np.c_[xx.ravel(), yy.ravel()])</span>
<span id="cb13-517"><a href="#cb13-517" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> Z.reshape(xx.shape)</span>
<span id="cb13-518"><a href="#cb13-518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-519"><a href="#cb13-519" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb13-520"><a href="#cb13-520" aria-hidden="true" tabindex="-1"></a>plt.contourf(xx, yy, Z, alpha<span class="op">=</span><span class="fl">0.8</span>, cmap<span class="op">=</span><span class="st">"viridis"</span>)</span>
<span id="cb13-521"><a href="#cb13-521" aria-hidden="true" tabindex="-1"></a>scatter <span class="op">=</span> plt.scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], c<span class="op">=</span>y, edgecolor<span class="op">=</span><span class="st">"k"</span>, cmap<span class="op">=</span><span class="st">"viridis"</span>)</span>
<span id="cb13-522"><a href="#cb13-522" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Frontera de Decisión: Regresión Softmax"</span>)</span>
<span id="cb13-523"><a href="#cb13-523" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Característica 1"</span>)</span>
<span id="cb13-524"><a href="#cb13-524" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Característica 2"</span>)</span>
<span id="cb13-525"><a href="#cb13-525" aria-hidden="true" tabindex="-1"></a>plt.colorbar(scatter, label<span class="op">=</span><span class="st">"Clases"</span>)</span>
<span id="cb13-526"><a href="#cb13-526" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb13-527"><a href="#cb13-527" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb13-528"><a href="#cb13-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-529"><a href="#cb13-529" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluación del modelo</span></span>
<span id="cb13-530"><a href="#cb13-530" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Reporte de Clasificación:"</span>)</span>
<span id="cb13-531"><a href="#cb13-531" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_test, y_pred))</span>
<span id="cb13-532"><a href="#cb13-532" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Matriz de Confusión:"</span>)</span>
<span id="cb13-533"><a href="#cb13-533" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(confusion_matrix(y_test, y_pred))</span>
<span id="cb13-534"><a href="#cb13-534" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-535"><a href="#cb13-535" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-536"><a href="#cb13-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-537"><a href="#cb13-537" aria-hidden="true" tabindex="-1"></a><span class="fu">## Ejemplo Poisson</span></span>
<span id="cb13-538"><a href="#cb13-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-541"><a href="#cb13-541" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb13-542"><a href="#cb13-542" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb13-543"><a href="#cb13-543" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb13-544"><a href="#cb13-544" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> PoissonRegressor</span>
<span id="cb13-545"><a href="#cb13-545" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb13-546"><a href="#cb13-546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-547"><a href="#cb13-547" aria-hidden="true" tabindex="-1"></a><span class="co"># Simular datos</span></span>
<span id="cb13-548"><a href="#cb13-548" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb13-549"><a href="#cb13-549" aria-hidden="true" tabindex="-1"></a>n_samples <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb13-550"><a href="#cb13-550" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.rand(n_samples, <span class="dv">1</span>) <span class="op">*</span> <span class="dv">10</span>  <span class="co"># Promociones (característica)</span></span>
<span id="cb13-551"><a href="#cb13-551" aria-hidden="true" tabindex="-1"></a>theta <span class="op">=</span> <span class="fl">0.5</span>  <span class="co"># Relación lineal</span></span>
<span id="cb13-552"><a href="#cb13-552" aria-hidden="true" tabindex="-1"></a>y_mean <span class="op">=</span> np.exp(theta <span class="op">*</span> X).flatten()  <span class="co"># Media de Poisson</span></span>
<span id="cb13-553"><a href="#cb13-553" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.random.poisson(y_mean)  <span class="co"># Datos simulados de Poisson</span></span>
<span id="cb13-554"><a href="#cb13-554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-555"><a href="#cb13-555" aria-hidden="true" tabindex="-1"></a><span class="co"># Dividir en entrenamiento y prueba</span></span>
<span id="cb13-556"><a href="#cb13-556" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb13-557"><a href="#cb13-557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-558"><a href="#cb13-558" aria-hidden="true" tabindex="-1"></a><span class="co"># Ajustar un modelo GLM con Poisson</span></span>
<span id="cb13-559"><a href="#cb13-559" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> PoissonRegressor(alpha<span class="op">=</span><span class="dv">0</span>, max_iter<span class="op">=</span><span class="dv">300</span>)</span>
<span id="cb13-560"><a href="#cb13-560" aria-hidden="true" tabindex="-1"></a>model.fit(X_train, y_train)</span>
<span id="cb13-561"><a href="#cb13-561" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-562"><a href="#cb13-562" aria-hidden="true" tabindex="-1"></a><span class="co"># Predicciones</span></span>
<span id="cb13-563"><a href="#cb13-563" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model.predict(X_test)</span>
<span id="cb13-564"><a href="#cb13-564" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-565"><a href="#cb13-565" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualizar los resultados</span></span>
<span id="cb13-566"><a href="#cb13-566" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb13-567"><a href="#cb13-567" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_test, y_test, color<span class="op">=</span><span class="st">"blue"</span>, label<span class="op">=</span><span class="st">"Datos reales"</span>)</span>
<span id="cb13-568"><a href="#cb13-568" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_test, y_pred, color<span class="op">=</span><span class="st">"red"</span>, label<span class="op">=</span><span class="st">"Predicciones"</span>)</span>
<span id="cb13-569"><a href="#cb13-569" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Promociones (característica)"</span>)</span>
<span id="cb13-570"><a href="#cb13-570" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Clientes (target)"</span>)</span>
<span id="cb13-571"><a href="#cb13-571" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Modelo Poisson GLM"</span>)</span>
<span id="cb13-572"><a href="#cb13-572" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb13-573"><a href="#cb13-573" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb13-574"><a href="#cb13-574" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb13-575"><a href="#cb13-575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-576"><a href="#cb13-576" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluación del modelo</span></span>
<span id="cb13-577"><a href="#cb13-577" aria-hidden="true" tabindex="-1"></a>score <span class="op">=</span> model.score(X_test, y_test)</span>
<span id="cb13-578"><a href="#cb13-578" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Coeficiente de determinación (R^2): </span><span class="sc">{</span>score<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb13-579"><a href="#cb13-579" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-580"><a href="#cb13-580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-581"><a href="#cb13-581" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-582"><a href="#cb13-582" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-583"><a href="#cb13-583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-584"><a href="#cb13-584" aria-hidden="true" tabindex="-1"></a><span class="fu"># Ajuste de Parámetros en Modelos Lineales Generalizados</span></span>
<span id="cb13-585"><a href="#cb13-585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-586"><a href="#cb13-586" aria-hidden="true" tabindex="-1"></a>Los Modelos Lineales Generalizados (GLMs) son una extensión poderosa de los modelos lineales que permite trabajar con una amplia variedad de distribuciones en la familia exponencial. En esta sección, discutiremos en detalle cómo se ajustan los parámetros de un GLM usando el principio de máxima verosimilitud, seguido por un análisis de los casos específicos de las distribuciones Gaussianas, Bernoulli, Multinomial y Poisson.</span>
<span id="cb13-587"><a href="#cb13-587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-588"><a href="#cb13-588" aria-hidden="true" tabindex="-1"></a><span class="fu">## El Principio de Máxima Verosimilitud</span></span>
<span id="cb13-589"><a href="#cb13-589" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-590"><a href="#cb13-590" aria-hidden="true" tabindex="-1"></a>El ajuste de parámetros en los GLMs se basa en maximizar la log-verosimilitud de los datos observados. Dado un conjunto de entrenamiento $$<span class="sc">\{</span>(x^{(i)}, y^{(i)})<span class="sc">\}</span>_{i=1}^n$$, la función de verosimilitud está dada por:</span>
<span id="cb13-591"><a href="#cb13-591" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-592"><a href="#cb13-592" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-593"><a href="#cb13-593" aria-hidden="true" tabindex="-1"></a>L(\theta) = \prod_{i=1}^n p(y^{(i)} | x^{(i)}; \theta),</span>
<span id="cb13-594"><a href="#cb13-594" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-595"><a href="#cb13-595" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-596"><a href="#cb13-596" aria-hidden="true" tabindex="-1"></a>y la log-verosimilitud correspondiente es:</span>
<span id="cb13-597"><a href="#cb13-597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-598"><a href="#cb13-598" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-599"><a href="#cb13-599" aria-hidden="true" tabindex="-1"></a>\ell(\theta) = \log L(\theta) = \sum_{i=1}^n \log p(y^{(i)} | x^{(i)}; \theta).</span>
<span id="cb13-600"><a href="#cb13-600" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-601"><a href="#cb13-601" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-602"><a href="#cb13-602" aria-hidden="true" tabindex="-1"></a>Para maximizar $$\ell(\theta)$$, derivamos con respecto a $$\theta$$ para obtener el gradiente. En GLMs, las propiedades de la familia exponencial simplifican esta derivada, que resulta ser:</span>
<span id="cb13-603"><a href="#cb13-603" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-604"><a href="#cb13-604" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-605"><a href="#cb13-605" aria-hidden="true" tabindex="-1"></a>\nabla_\theta \ell(\theta) = \sum_{i=1}^n (T(y^{(i)}) - \mathbb{E}<span class="co">[</span><span class="ot">T(y) | x^{(i)}; \theta</span><span class="co">]</span>) x^{(i)},</span>
<span id="cb13-606"><a href="#cb13-606" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-607"><a href="#cb13-607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-608"><a href="#cb13-608" aria-hidden="true" tabindex="-1"></a>donde $$T(y)$$ es la estadística suficiente de la distribución y $$\mathbb{E}<span class="co">[</span><span class="ot">T(y) | x^{(i)}; \theta</span><span class="co">]</span>$$ es el valor esperado de $$T(y)$$ dado $$x^{(i)}$$.</span>
<span id="cb13-609"><a href="#cb13-609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-610"><a href="#cb13-610" aria-hidden="true" tabindex="-1"></a><span class="fu">## El Método de Newton-Raphson</span></span>
<span id="cb13-611"><a href="#cb13-611" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-612"><a href="#cb13-612" aria-hidden="true" tabindex="-1"></a>Para maximizar la log-verosimilitud, usamos un método iterativo como Newton-Raphson. Este método requiere calcular la Hessiana de $$\ell(\theta)$$, que es la matriz de segundas derivadas:</span>
<span id="cb13-613"><a href="#cb13-613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-614"><a href="#cb13-614" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-615"><a href="#cb13-615" aria-hidden="true" tabindex="-1"></a>H(\theta) = \sum_{i=1}^n \left(-\text{Var}<span class="co">[</span><span class="ot">T(y) | x^{(i)}; \theta</span><span class="co">]</span>\right) x^{(i)} (x^{(i)})^T,</span>
<span id="cb13-616"><a href="#cb13-616" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-617"><a href="#cb13-617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-618"><a href="#cb13-618" aria-hidden="true" tabindex="-1"></a>donde $$\text{Var}<span class="co">[</span><span class="ot">T(y) | x; \theta</span><span class="co">]</span>$$ es la varianza condicional de $$T(y)$$, que depende de la distribución específica del modelo. El método de Newton-Raphson actualiza $$\theta$$ como:</span>
<span id="cb13-619"><a href="#cb13-619" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-620"><a href="#cb13-620" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-621"><a href="#cb13-621" aria-hidden="true" tabindex="-1"></a>\theta^{(t+1)} = \theta^{(t)} - H(\theta^{(t)})^{-1} \nabla_\theta \ell(\theta^{(t)}).</span>
<span id="cb13-622"><a href="#cb13-622" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-623"><a href="#cb13-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-624"><a href="#cb13-624" aria-hidden="true" tabindex="-1"></a>Aunque este método converge rápidamente en muchas aplicaciones, calcular la Hessiana e invertirla puede ser computacionalmente costoso para modelos de alta dimensionalidad. En tales casos, se prefieren métodos más simples como el descenso por gradiente.</span>
<span id="cb13-625"><a href="#cb13-625" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-626"><a href="#cb13-626" aria-hidden="true" tabindex="-1"></a><span class="fu">## Casos Específicos de GLMs</span></span>
<span id="cb13-627"><a href="#cb13-627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-628"><a href="#cb13-628" aria-hidden="true" tabindex="-1"></a><span class="fu">### Regresión Lineal (Distribución Gaussiana)</span></span>
<span id="cb13-629"><a href="#cb13-629" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-630"><a href="#cb13-630" aria-hidden="true" tabindex="-1"></a>En la regresión lineal, asumimos que $$y | x; \theta \sim \mathcal{N}(\mu, \sigma^2)$$, con $$\mu = \theta^T x$$. Las propiedades de la distribución Gaussiana son:</span>
<span id="cb13-631"><a href="#cb13-631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-632"><a href="#cb13-632" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-633"><a href="#cb13-633" aria-hidden="true" tabindex="-1"></a>T(y) = y, \quad \mathbb{E}<span class="co">[</span><span class="ot">T(y) | x; \theta</span><span class="co">]</span> = \mu = \theta^T x, \quad \text{Var}<span class="co">[</span><span class="ot">T(y) | x; \theta</span><span class="co">]</span> = \sigma^2.</span>
<span id="cb13-634"><a href="#cb13-634" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-635"><a href="#cb13-635" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-636"><a href="#cb13-636" aria-hidden="true" tabindex="-1"></a>El gradiente de la log-verosimilitud es:</span>
<span id="cb13-637"><a href="#cb13-637" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-638"><a href="#cb13-638" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-639"><a href="#cb13-639" aria-hidden="true" tabindex="-1"></a>\nabla_\theta \ell(\theta) = \sum_{i=1}^n (y^{(i)} - \theta^T x^{(i)}) x^{(i)}.</span>
<span id="cb13-640"><a href="#cb13-640" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-641"><a href="#cb13-641" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-642"><a href="#cb13-642" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Ejemplo en Python: Regresión Lineal</span></span>
<span id="cb13-643"><a href="#cb13-643" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-646"><a href="#cb13-646" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb13-647"><a href="#cb13-647" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb13-648"><a href="#cb13-648" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb13-649"><a href="#cb13-649" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb13-650"><a href="#cb13-650" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-651"><a href="#cb13-651" aria-hidden="true" tabindex="-1"></a><span class="co"># Simular datos</span></span>
<span id="cb13-652"><a href="#cb13-652" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb13-653"><a href="#cb13-653" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.rand(<span class="dv">100</span>, <span class="dv">1</span>) <span class="op">*</span> <span class="dv">10</span></span>
<span id="cb13-654"><a href="#cb13-654" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="dv">3</span> <span class="op">*</span> X.flatten() <span class="op">+</span> <span class="dv">5</span> <span class="op">+</span> np.random.randn(<span class="dv">100</span>) <span class="op">*</span> <span class="dv">2</span></span>
<span id="cb13-655"><a href="#cb13-655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-656"><a href="#cb13-656" aria-hidden="true" tabindex="-1"></a><span class="co"># Ajustar modelo</span></span>
<span id="cb13-657"><a href="#cb13-657" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LinearRegression()</span>
<span id="cb13-658"><a href="#cb13-658" aria-hidden="true" tabindex="-1"></a>model.fit(X, y)</span>
<span id="cb13-659"><a href="#cb13-659" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-660"><a href="#cb13-660" aria-hidden="true" tabindex="-1"></a><span class="co"># Predicciones</span></span>
<span id="cb13-661"><a href="#cb13-661" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model.predict(X)</span>
<span id="cb13-662"><a href="#cb13-662" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-663"><a href="#cb13-663" aria-hidden="true" tabindex="-1"></a><span class="co"># Gráfico</span></span>
<span id="cb13-664"><a href="#cb13-664" aria-hidden="true" tabindex="-1"></a>plt.scatter(X, y, label<span class="op">=</span><span class="st">"Datos reales"</span>)</span>
<span id="cb13-665"><a href="#cb13-665" aria-hidden="true" tabindex="-1"></a>plt.plot(X, y_pred, color<span class="op">=</span><span class="st">"red"</span>, label<span class="op">=</span><span class="st">"Modelo ajustado"</span>)</span>
<span id="cb13-666"><a href="#cb13-666" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"X"</span>)</span>
<span id="cb13-667"><a href="#cb13-667" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"y"</span>)</span>
<span id="cb13-668"><a href="#cb13-668" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Regresión Lineal"</span>)</span>
<span id="cb13-669"><a href="#cb13-669" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb13-670"><a href="#cb13-670" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb13-671"><a href="#cb13-671" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-672"><a href="#cb13-672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-673"><a href="#cb13-673" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb13-674"><a href="#cb13-674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-675"><a href="#cb13-675" aria-hidden="true" tabindex="-1"></a><span class="fu">### Regresión Logística (Distribución Bernoulli)</span></span>
<span id="cb13-676"><a href="#cb13-676" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-677"><a href="#cb13-677" aria-hidden="true" tabindex="-1"></a>En la regresión logística, $$y | x; \theta \sim \text{Bernoulli}(\phi)$$, con $$\phi = \frac{1}{1 + e^{-\theta^T x}}$$. Las propiedades relevantes son:</span>
<span id="cb13-678"><a href="#cb13-678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-679"><a href="#cb13-679" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-680"><a href="#cb13-680" aria-hidden="true" tabindex="-1"></a>T(y) = y, \quad \mathbb{E}<span class="co">[</span><span class="ot">T(y) | x; \theta</span><span class="co">]</span> = \phi, \quad \text{Var}<span class="co">[</span><span class="ot">T(y) | x; \theta</span><span class="co">]</span> = \phi (1 - \phi).</span>
<span id="cb13-681"><a href="#cb13-681" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-682"><a href="#cb13-682" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-683"><a href="#cb13-683" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Ejemplo en Python: Regresión Logística</span></span>
<span id="cb13-684"><a href="#cb13-684" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-687"><a href="#cb13-687" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb13-688"><a href="#cb13-688" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb13-689"><a href="#cb13-689" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-690"><a href="#cb13-690" aria-hidden="true" tabindex="-1"></a><span class="co"># Simular datos binarios</span></span>
<span id="cb13-691"><a href="#cb13-691" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.rand(<span class="dv">100</span>, <span class="dv">2</span>) <span class="op">*</span> <span class="dv">10</span></span>
<span id="cb13-692"><a href="#cb13-692" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> (<span class="dv">3</span> <span class="op">*</span> X[:, <span class="dv">0</span>] <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> X[:, <span class="dv">1</span>] <span class="op">+</span> np.random.randn(<span class="dv">100</span>) <span class="op">&gt;</span> <span class="dv">15</span>).astype(<span class="bu">int</span>)</span>
<span id="cb13-693"><a href="#cb13-693" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-694"><a href="#cb13-694" aria-hidden="true" tabindex="-1"></a><span class="co"># Ajustar modelo</span></span>
<span id="cb13-695"><a href="#cb13-695" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression()</span>
<span id="cb13-696"><a href="#cb13-696" aria-hidden="true" tabindex="-1"></a>model.fit(X, y)</span>
<span id="cb13-697"><a href="#cb13-697" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-698"><a href="#cb13-698" aria-hidden="true" tabindex="-1"></a><span class="co"># Predicciones</span></span>
<span id="cb13-699"><a href="#cb13-699" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model.predict(X)</span>
<span id="cb13-700"><a href="#cb13-700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-701"><a href="#cb13-701" aria-hidden="true" tabindex="-1"></a><span class="co"># Gráfico</span></span>
<span id="cb13-702"><a href="#cb13-702" aria-hidden="true" tabindex="-1"></a>plt.scatter(X[:, <span class="dv">0</span>], y, label<span class="op">=</span><span class="st">"Datos reales"</span>)</span>
<span id="cb13-703"><a href="#cb13-703" aria-hidden="true" tabindex="-1"></a>plt.scatter(X[:, <span class="dv">0</span>], y_pred, color<span class="op">=</span><span class="st">"red"</span>, label<span class="op">=</span><span class="st">"Predicciones"</span>)</span>
<span id="cb13-704"><a href="#cb13-704" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"X"</span>)</span>
<span id="cb13-705"><a href="#cb13-705" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"y"</span>)</span>
<span id="cb13-706"><a href="#cb13-706" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Regresión Logística"</span>)</span>
<span id="cb13-707"><a href="#cb13-707" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb13-708"><a href="#cb13-708" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb13-709"><a href="#cb13-709" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-710"><a href="#cb13-710" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-711"><a href="#cb13-711" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb13-712"><a href="#cb13-712" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-713"><a href="#cb13-713" aria-hidden="true" tabindex="-1"></a><span class="fu">### Regresión Softmax (Distribución Multinomial)</span></span>
<span id="cb13-714"><a href="#cb13-714" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-715"><a href="#cb13-715" aria-hidden="true" tabindex="-1"></a>En la regresión softmax, $$y \in <span class="sc">\{</span>1, \dots, k<span class="sc">\}</span>$$:</span>
<span id="cb13-716"><a href="#cb13-716" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-717"><a href="#cb13-717" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-718"><a href="#cb13-718" aria-hidden="true" tabindex="-1"></a>p(y = i | x; \theta) = \frac{\exp(\theta_i^T x)}{\sum_{j=1}^k \exp(\theta_j^T x)}.</span>
<span id="cb13-719"><a href="#cb13-719" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-720"><a href="#cb13-720" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-721"><a href="#cb13-721" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Ejemplo en Python: Regresión Softmax</span></span>
<span id="cb13-722"><a href="#cb13-722" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-725"><a href="#cb13-725" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb13-726"><a href="#cb13-726" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb13-727"><a href="#cb13-727" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-728"><a href="#cb13-728" aria-hidden="true" tabindex="-1"></a><span class="co"># Simular datos multinomiales</span></span>
<span id="cb13-729"><a href="#cb13-729" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.rand(<span class="dv">150</span>, <span class="dv">2</span>) <span class="op">*</span> <span class="dv">10</span></span>
<span id="cb13-730"><a href="#cb13-730" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.random.choice([<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>], size<span class="op">=</span><span class="dv">150</span>)</span>
<span id="cb13-731"><a href="#cb13-731" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-732"><a href="#cb13-732" aria-hidden="true" tabindex="-1"></a><span class="co"># Ajustar modelo</span></span>
<span id="cb13-733"><a href="#cb13-733" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression(multi_class<span class="op">=</span><span class="st">'multinomial'</span>, solver<span class="op">=</span><span class="st">'lbfgs'</span>)</span>
<span id="cb13-734"><a href="#cb13-734" aria-hidden="true" tabindex="-1"></a>model.fit(X, y)</span>
<span id="cb13-735"><a href="#cb13-735" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-736"><a href="#cb13-736" aria-hidden="true" tabindex="-1"></a><span class="co"># Predicciones</span></span>
<span id="cb13-737"><a href="#cb13-737" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model.predict(X)</span>
<span id="cb13-738"><a href="#cb13-738" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-739"><a href="#cb13-739" aria-hidden="true" tabindex="-1"></a><span class="co"># Gráfico</span></span>
<span id="cb13-740"><a href="#cb13-740" aria-hidden="true" tabindex="-1"></a>plt.scatter(X[:, <span class="dv">0</span>], y, label<span class="op">=</span><span class="st">"Datos reales"</span>)</span>
<span id="cb13-741"><a href="#cb13-741" aria-hidden="true" tabindex="-1"></a>plt.scatter(X[:, <span class="dv">0</span>], y_pred, color<span class="op">=</span><span class="st">"red"</span>, label<span class="op">=</span><span class="st">"Predicciones"</span>)</span>
<span id="cb13-742"><a href="#cb13-742" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"X"</span>)</span>
<span id="cb13-743"><a href="#cb13-743" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"y"</span>)</span>
<span id="cb13-744"><a href="#cb13-744" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Regresión Softmax"</span>)</span>
<span id="cb13-745"><a href="#cb13-745" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb13-746"><a href="#cb13-746" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb13-747"><a href="#cb13-747" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-748"><a href="#cb13-748" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-749"><a href="#cb13-749" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb13-750"><a href="#cb13-750" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-751"><a href="#cb13-751" aria-hidden="true" tabindex="-1"></a><span class="fu">### Regresión Poisson</span></span>
<span id="cb13-752"><a href="#cb13-752" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-753"><a href="#cb13-753" aria-hidden="true" tabindex="-1"></a>En la regresión Poisson, $$y | x; \theta \sim \text{Poisson}(\lambda)$$, con $$\lambda = \exp(\theta^T x)$$. Las propiedades relevantes son:</span>
<span id="cb13-754"><a href="#cb13-754" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-755"><a href="#cb13-755" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-756"><a href="#cb13-756" aria-hidden="true" tabindex="-1"></a>T(y) = y, \quad \mathbb{E}<span class="co">[</span><span class="ot">T(y) | x; \theta</span><span class="co">]</span> = \lambda = \exp(\theta^T x), \quad \text{Var}<span class="co">[</span><span class="ot">T(y) | x; \theta</span><span class="co">]</span> = \lambda.</span>
<span id="cb13-757"><a href="#cb13-757" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-758"><a href="#cb13-758" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-759"><a href="#cb13-759" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Ejemplo en Python: Regresión Poisson</span></span>
<span id="cb13-760"><a href="#cb13-760" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-763"><a href="#cb13-763" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb13-764"><a href="#cb13-764" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> PoissonRegressor</span>
<span id="cb13-765"><a href="#cb13-765" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-766"><a href="#cb13-766" aria-hidden="true" tabindex="-1"></a><span class="co"># Simular datos de Poisson</span></span>
<span id="cb13-767"><a href="#cb13-767" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.rand(<span class="dv">100</span>, <span class="dv">1</span>) <span class="op">*</span> <span class="dv">10</span></span>
<span id="cb13-768"><a href="#cb13-768" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.random.poisson(lam<span class="op">=</span>np.exp(<span class="fl">0.5</span> <span class="op">*</span> X.flatten()))</span>
<span id="cb13-769"><a href="#cb13-769" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-770"><a href="#cb13-770" aria-hidden="true" tabindex="-1"></a><span class="co"># Ajustar modelo</span></span>
<span id="cb13-771"><a href="#cb13-771" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> PoissonRegressor(alpha<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb13-772"><a href="#cb13-772" aria-hidden="true" tabindex="-1"></a>model.fit(X, y)</span>
<span id="cb13-773"><a href="#cb13-773" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-774"><a href="#cb13-774" aria-hidden="true" tabindex="-1"></a><span class="co"># Predicciones</span></span>
<span id="cb13-775"><a href="#cb13-775" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model.predict(X)</span>
<span id="cb13-776"><a href="#cb13-776" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-777"><a href="#cb13-777" aria-hidden="true" tabindex="-1"></a><span class="co"># Gráfico</span></span>
<span id="cb13-778"><a href="#cb13-778" aria-hidden="true" tabindex="-1"></a>plt.scatter(X, y, label<span class="op">=</span><span class="st">"Datos reales"</span>)</span>
<span id="cb13-779"><a href="#cb13-779" aria-hidden="true" tabindex="-1"></a>plt.plot(X, y_pred, color<span class="op">=</span><span class="st">"red"</span>, label<span class="op">=</span><span class="st">"Modelo ajustado"</span>)</span>
<span id="cb13-780"><a href="#cb13-780" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"X"</span>)</span>
<span id="cb13-781"><a href="#cb13-781" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"y"</span>)</span>
<span id="cb13-782"><a href="#cb13-782" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Regresión Poisson"</span>)</span>
<span id="cb13-783"><a href="#cb13-783" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb13-784"><a href="#cb13-784" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb13-785"><a href="#cb13-785" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-786"><a href="#cb13-786" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-787"><a href="#cb13-787" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb13-788"><a href="#cb13-788" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-789"><a href="#cb13-789" aria-hidden="true" tabindex="-1"></a><span class="fu">## Conclusión</span></span>
<span id="cb13-790"><a href="#cb13-790" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-791"><a href="#cb13-791" aria-hidden="true" tabindex="-1"></a>El ajuste de parámetros en los GLMs se basa en maximizar la log-verosimilitud. Este marco unificado permite abordar una amplia variedad de problemas de clasificación y regresión usando distribuciones específicas de la familia exponencial.</span>
</code><button title="Copiar al portapapeles" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>