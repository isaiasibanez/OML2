[
  {
    "objectID": "macros.html",
    "href": "macros.html",
    "title": "Mi Sitio en Quarto",
    "section": "",
    "text": "\\[\n\\def\\RR{\\mathbb{R}}\n\\def\\media{\\mathbb{E}}\n\\def\\xx{{\\bf x}}\n\\def\\XX{{\\bf X}}\n\\def\\TT{{\\bf T}}\n\\def\\bfa{\\boldsymbol{a}}\n\\def\\bfb{\\boldsymbol{b}}\n\\def\\bftheta{\\boldsymbol{\\theta}}\n\\def\\bflambda{\\boldsymbol{\\lambda}}\n\\def\\bfeta{\\boldsymbol{\\eta}}\n\\def\\bfmu{\\boldsymbol{\\mu}}\n\\def\\bfnu{\\boldsymbol{\\nu}}\n\\def\\bfSigma{\\boldsymbol{\\Sigma}}\n\\def\\bfone{\\mathbf{1}}\n\\def\\argmin{\\mathop{\\mathrm{arg\\,min\\,}}}\n\\def\\argmax{\\mathop{\\mathrm{arg\\,max\\,}}}\n\\]"
  },
  {
    "objectID": "CAPITULO_2/06_EM.html",
    "href": "CAPITULO_2/06_EM.html",
    "title": "Algoritmo EM",
    "section": "",
    "text": "\\[\n\\def\\RR{\\mathbb{R}}\n\\def\\media{\\mathbb{E}}\n\\def\\xx{{\\bf x}}\n\\def\\XX{{\\bf X}}\n\\def\\TT{{\\bf T}}\n\\def\\bftheta{\\boldsymbol{\\theta}}\n\\def\\bfeta{\\boldsymbol{\\eta}}\n\\def\\bfone{\\mathbf{1}}\n\\]\n\n\n\n1 First"
  },
  {
    "objectID": "CAPITULO_2/04_modelos_generativos.html",
    "href": "CAPITULO_2/04_modelos_generativos.html",
    "title": "Modelos generativos",
    "section": "",
    "text": "\\[\n\\def\\RR{\\mathbb{R}}\n\\def\\media{\\mathbb{E}}\n\\def\\xx{{\\bf x}}\n\\def\\XX{{\\bf X}}\n\\def\\TT{{\\bf T}}\n\\def\\bftheta{\\boldsymbol{\\theta}}\n\\def\\bfeta{\\boldsymbol{\\eta}}\n\\def\\bfmu{\\boldsymbol{\\mu}}\n\\def\\bfSigma{\\boldsymbol{\\Sigma}}\n\\def\\bfone{\\mathbf{1}}\n\\def\\argmin{\\mathop{\\mathrm{arg\\,min\\,}}}\n\\def\\argmax{\\mathop{\\mathrm{arg\\,max\\,}}}\n\\]\nHasta ahora, hemos hablado principalmente sobre algoritmos de aprendizaje que modelan \\(p(y|\\xx; \\bftheta)\\); esto es, la distribución condicional de \\(y\\) dado \\(\\xx\\) parametrizada por \\(\\bftheta\\). El objetivo es aprender un clasificador que asocie a una entrada \\(\\xx\\) una etiqueta. Nos referiremos a estos algoritmos como aprendizaje discriminativo.\nEn esta sección, hablaremos sobre algoritmos de aprendizaje generativo, en el cual se pone foco en \\(p(\\xx|y)\\). Para entender la idea, consideremos un problema de clasificación en el que queremos aprender a distinguir entre elefantes (\\(y=1\\)) y perros (\\(y=0\\)), basado en algunas características del animal. Una posibilidad es construir un modelo de cómo son los elefantes, \\(p(\\xx|y=1)\\), y un modelo separado para los perros, \\(p(\\xx|y=0)\\), de tal manera que al final del día clasificar un nuevo animal dependa de si se parece más a los elefantes o más a los perros del conjunto de entrenamiento.\nDespués de modelar \\(p(\\xx|y)\\) y también la distribución a priori \\(p(y)\\), la regla de Bayes permite obtener la distribución a posteriori:\n\\[\np(y|\\xx) = \\frac{p(\\xx|y)p(y)}{p(\\xx)}.\n\\]\nObservar que el denominador se obtiene mediante la regla \\[p(\\xx) = p(\\xx|y=1)p(y=1) + p(\\xx|y=0)p(y=0).\\]\nSin embargo, al momento de hacer una predicción el denominador no es necesario ya que \\[\n\\argmax_y p(y|\\xx) = \\argmax_y \\frac{p(\\xx|y)p(y)}{p(\\xx)} = \\argmax_y p(\\xx|y)p(y).\n\\]"
  },
  {
    "objectID": "CAPITULO_2/04_modelos_generativos.html#análisis-discriminante-lineal-lda",
    "href": "CAPITULO_2/04_modelos_generativos.html#análisis-discriminante-lineal-lda",
    "title": "Modelos generativos",
    "section": "1.1 Análisis discriminante lineal (LDA)",
    "text": "1.1 Análisis discriminante lineal (LDA)\nEl modelo es:\n\\[\n\\begin{align*}\ny &\\sim \\text{Bernoulli}(\\phi) \\\\\n\\xx|y=0 &\\sim \\mathcal{N}(\\bfmu_0, \\bfSigma) \\\\\nx|y=1 &\\sim \\mathcal{N}(\\bfmu_1, \\bfSigma)\n\\end{align*}\n\\]\nLos parámetros del modelo son \\(\\phi\\), \\(\\bfmu_0\\), \\(\\bfmu_1\\) y \\(\\bfSigma\\).\n\n\n\n\n\n\n\n\n\nEn la figura se muestran datos de entrenamiento correspondientes a dos clases y las curvas de nivel de las gaussianas que generan ambas clases. Se puede observar que ambas gaussianas tienen la misma forma y orientación, ya que comparten una matriz de covarianza \\(\\bfSigma\\) en común. También se muestra en la figura la línea recta que representa la frontera de decisión al momento de clasificar un nuevo dato. Dicha frontera corresponde a la ecuación \\(p(y=1|\\xx) = 0.5\\).\n\n1.1.1 Función de verosimilitud\nPara un conjunto de datos \\(\\{\\xx^{(i)}, y^{(i)}\\}_{i=1}^n\\) i.i.d., la función de log-verosimilitud es\n\\[\n\\begin{align*}\n\\ell(\\phi, \\bfmu_0, \\bfmu_1, \\bfSigma) &= \\log \\prod_{i=1}^n p(\\xx^{(i)}, y^{(i)}; \\phi, \\bfmu_0, \\bfmu_1, \\bfSigma)\\\\\n&= \\log \\prod_{i=1}^n p(\\xx^{(i)}| y^{(i)}; \\phi, \\bfmu_0, \\bfmu_1, \\bfSigma) p(y^{(i)}; \\phi)\\\\\n\\end{align*}\n\\]\nMaximizando \\(\\ell\\) con respecto a los parámetros, encontramos las estimaciones de máxima verosimilitud:\n\\[\n\\begin{align*}\n\\hat{\\phi} &= \\frac{1}{n} \\sum_{i=1}^n \\bfone\\{y^{(i)} = 1\\} \\\\\n\\hat{\\bfmu}_0 &= \\frac{\\sum_{i=1}^n \\bfone\\{y^{(i)} = 0\\} \\xx^{(i)}}{\\sum_{i=1}^n \\bfone\\{y^{(i)} = 0\\}} \\\\\n\\hat{\\bfmu}_1 &= \\frac{\\sum_{i=1}^n \\bfone\\{y^{(i)} = 1\\} \\xx^{(i)}}{\\sum_{i=1}^n \\bfone\\{y^{(i)} = 1\\}} \\\\\n\\hat{\\bfSigma} &= \\frac{1}{n} \\sum_{i=1}^n (\\xx^{(i)} - \\bfmu_{y^{(i)}})(\\xx^{(i)} - \\bfmu_{y^{(i)}})^T\n\\end{align*}\n\\]\n\n\n1.1.2 Clasificador final\nUna observación \\(\\xx\\) se etiquetará con \\(y=0\\) si\n\\[\n\\begin{align*}\n\\log\\left(\\frac{p(y=0|\\xx)}{p(y=1|\\xx)}\\right)&&gt;0\\\\\n\\log\\left(\\frac{p(\\xx|y=0)p(y=0)}{p(\\xx|y=1)p(y=1)}\\right)&&gt;0\\\\\n\\log\\left(\\frac{(1-\\hat{\\phi})\\,\\mathcal{N}(\\xx|\\hat{\\bfmu}_0,\\hat{\\bfSigma})}{\\hat{\\phi}\\,\\mathcal{N}(\\xx|\\hat{\\bfmu}_1,\\hat{\\bfSigma})}\\right)&&gt;0\n\\end{align*}.\n\\]\nSi se desarrolla el argumento del logaritmo en la expresión anterior, es decir el cociente de verosimilitudes, se puede deducir que la frontera de decisión es efectivamente una función lineal. ¿Ejercicio?"
  },
  {
    "objectID": "CAPITULO_2/04_modelos_generativos.html#análisis-discriminante-cuadrático-qda",
    "href": "CAPITULO_2/04_modelos_generativos.html#análisis-discriminante-cuadrático-qda",
    "title": "Modelos generativos",
    "section": "1.2 Análisis discriminante cuadrático (QDA)",
    "text": "1.2 Análisis discriminante cuadrático (QDA)\nEl modelo es:\n\\[\n\\begin{align*}\ny &\\sim \\text{Bernoulli}(\\phi) \\\\\n\\xx|y=0 &\\sim \\mathcal{N}(\\bfmu_0, \\bfSigma_0) \\\\\nx|y=1 &\\sim \\mathcal{N}(\\bfmu_1, \\bfSigma_1)\n\\end{align*}\n\\]\nLos parámetros del modelo son \\(\\phi\\), \\(\\bfmu_0\\), \\(\\bfmu_1\\), \\(\\bfSigma_0\\) y \\(\\bfSigma_1\\).\n\n\n\n\n\n\n\n\n\nEn la figura se observa como en este caso, a diferencia de LDA, las gaussianas tienen diferente forma y orientación debido a que se asume que las matrices de covarianza no son iguales. Además, la frontera de decisión no es lineal.\n\n1.2.1 Función de verosimilitud\nPara un conjunto de datos \\(\\{\\xx^{(i)}, y^{(i)}\\}_{i=1}^n\\) i.i.d., la función de log-verosimilitud es\n\\[\n\\ell(\\phi, \\bfmu_0, \\bfmu_1, \\bfSigma_0,\\bfSigma_1) = \\log \\prod_{i=1}^n p(\\xx^{(i)}| y^{(i)}; \\phi, \\bfmu_0, \\bfmu_1, \\bfSigma_1,\\bfSigma_2) p(y^{(i)}; \\phi).\n\\]\nLos estimadores de máxima verosimilitud de \\(\\phi\\), \\(\\bfmu_0\\) y \\(\\bfmu_1\\) son idénticos al que se obtiene en LDA. Para las matrices de covarianza, resulta\n\\[\n\\hat{\\bfSigma}_0 = \\frac{\\sum_{i=1}^n \\bfone\\{y^{(i)}=0\\}(\\xx^{(i)} - \\hat{\\bfmu}_0)(\\xx^{(i)} - \\hat{\\bfmu}_0)^T}{\\sum_{i=1}^n\\bfone\\{y^{(i)}=0\\}}\n\\]\n\\[\n\\hat{\\bfSigma}_1 = \\frac{\\sum_{i=1}^n \\bfone\\{y^{(i)}=1\\}(\\xx^{(i)} - \\hat{\\bfmu}_1)(\\xx^{(i)} - \\hat{\\bfmu}_1)^T}{\\sum_{i=1}^n\\bfone\\{y^{(i)}=1\\}}\n\\]\n\n\n1.2.2 Clasificador final\nUna observación \\(\\xx\\) se etiquetará con \\(y=0\\) si\n\\[\\log\\left(\\frac{(1-\\hat{\\phi})\\,\\mathcal{N}(\\xx|\\hat{\\bfmu}_0,\\hat{\\bfSigma}_0)}{\\hat{\\phi}\\,\\mathcal{N}(\\xx|\\hat{\\bfmu}_1,\\hat{\\bfSigma}_1)}\\right)&gt;0.\\]\nEn este caso, el desarrollo del cociente de verosimilitudes permite deducir que la frontera de decisión es una función cuadrática. ¿Ejercicio?"
  },
  {
    "objectID": "CAPITULO_2/04_modelos_generativos.html#tipos-de-clasificadores-naive-bayes",
    "href": "CAPITULO_2/04_modelos_generativos.html#tipos-de-clasificadores-naive-bayes",
    "title": "Modelos generativos",
    "section": "2.1 Tipos de clasificadores Naive Bayes",
    "text": "2.1 Tipos de clasificadores Naive Bayes\nNo existe solo un tipo de clasificador Naïve Bayes. Los tipos más comunes se diferencian según las distribuciones de los valores de las características. Algunos de estos son:\n\nNaive Bayes Gaussiano: Este es un tipo de clasificador Naïve Bayes que se utiliza con distribuciones gaussianas, es decir, distribuciones normales y variables continuas. Este modelo se ajusta encontrando la media y la desviación estándar de cada clase.\nNaive Bayes Multinomial: Este tipo de clasificador Naïve Bayes asume que las características provienen de distribuciones multinomiales. Esta variante es útil cuando se trabaja con datos discretos, como conteos de frecuencia, y se aplica típicamente en casos de uso de procesamiento de lenguaje natural, como la clasificación de spam.\nNaïve Bayes Bernoulli: Esta es otra variante del clasificador Naïve Bayes, que se utiliza con variables booleanas, es decir, variables con dos valores, como Verdadero y Falso o 1 y 0.\n\n\n2.1.1 Naive Bayes Bernoulli\nLas suposiciones son:\n\\[y\\sim\\text{Bernoulli}(\\phi),\\] \\[x_j|y=0\\sim\\text{Bernoulli}(\\phi_{j|y=0}),\\] \\[x_j|y=1\\sim\\text{Bernoulli}(\\phi_{j|y=1}),\\]\nLos parámetros del modelo son \\[\\phi=p(y=1),\\quad \\phi_{j|y=0}=p(x_j=1|y=0),\\quad \\phi_{j|y=1}=p(x_j=1|y=1).\\]\n\n2.1.1.1 Función de verosimilitud\nPara un conjunto de datos \\(\\{\\xx^{(i)}, y^{(i)}\\}_{i=1}^n\\) i.i.d., la función de verosimilitud es \\[\n\\mathcal{L}(\\phi, \\phi_{j|y=0}, \\phi_{j|y=1}) = \\prod_{i=1}^n p(\\xx^{(i)}, y^{(i)})\n\\]\nLos estimadores de máxima verosimilitud resultan:\n\\[\\phi = \\frac{\\sum_{i=1}^n \\bfone\\{y^{(i)} = 1\\}}{n},\\] \\[\\phi_{j|y=1} = \\frac{\\sum_{i=1}^n \\bfone\\{\\xx_j^{(i)} = 1 \\land y^{(i)} = 1\\}}{\\sum_{i=1}^n \\bfone\\{y^{(i)} = 1\\}},\\] \\[\\phi_{j|y=0} = \\frac{\\sum_{i=1}^n \\bfone\\{\\xx_j^{(i)} = 1 \\land y^{(i)} = 0\\}}{\\sum_{i=1}^n \\bfone\\{y^{(i)} = 0\\}}.\\]\nA pesar de que pueda parecer engorroso a simple vista, estas estimaciones tienen una interpretación muy natural. Por ejemplo, \\(\\phi_{j|y=1}\\) es simplemente la fracción de correos etiquetados como spam (\\(y=1\\)) en los que aparece la palabra \\(j\\).\n\n\nEjercicios"
  },
  {
    "objectID": "CAPITULO_2/02_GLM.html",
    "href": "CAPITULO_2/02_GLM.html",
    "title": "Modelos lineales generalizados",
    "section": "",
    "text": "\\[\n\\def\\RR{\\mathbb{R}}\n\\def\\media{\\mathbb{E}}\n\\def\\xx{{\\bf x}}\n\\def\\XX{{\\bf X}}\n\\def\\TT{{\\bf T}}\n\\def\\bftheta{\\boldsymbol{\\theta}}\n\\def\\bfeta{\\boldsymbol{\\eta}}\n\\def\\bfone{\\mathbf{1}}\n\\]\nTanto en el modelo de regresión lineal como en el de regresión logística, asumimos una cierta distribución condicional para \\(Y|\\XX\\) que depende de un conjunto de parámetros \\(\\bftheta\\).\nEn esta sección mostraremos que ambos métodos son casos especiales de una familia más amplia de modelos, llamados Modelos Lineales Generalizados (GLMs). También mostraremos cómo otros modelos en GLM pueden derivarse y aplicarse a otros problemas de regresión y clasificación."
  },
  {
    "objectID": "CAPITULO_2/02_GLM.html#ejemplos",
    "href": "CAPITULO_2/02_GLM.html#ejemplos",
    "title": "Modelos lineales generalizados",
    "section": "1.1 Ejemplos",
    "text": "1.1 Ejemplos\nMostraremos que las distribuciones Bernoulli y Normal son ejemplos de distribuciones de la familia exponencial.\n\n1.1.1 Distribución Bernoulli\nSea \\(Y\\in\\{0,1\\}\\) tal que \\(Y\\sim\\text{Bernoulli}(\\phi)\\). Entonces\n\\[\n\\begin{align*}\np(y; \\phi) &= \\phi^y (1 - \\phi)^{1-y} \\\\\n& = \\exp\\left( y \\log\\phi + (1-y)\\log(1-\\phi) \\right) \\\\\n& = \\exp\\left(\\log\\left(\\frac{\\phi}{1-\\phi}\\right)y+\\log(1-\\phi)\\right) \\\\\n& = \\exp\\left(\\log\\left(\\frac{\\phi}{1-\\phi}\\right)y+\\log(1-\\phi)\\right)\n\\end{align*}\n\\]\nReconocemos el parámetro natural \\[\\eta=\\log\\left(\\frac{\\phi}{1-\\phi}\\right)\\]\nmientras que el resto de los elementos son: \\[T(y)=y, \\qquad a(\\eta)=-\\log(1-\\phi)=\\log(1+e^\\eta), \\qquad b(y)=1.\\]\n\n\n1.1.2 Distribución Gaussiana\nSea \\(Y\\in\\RR\\) tal que \\(Y\\sim\\mathcal{N}(\\mu,\\sigma^2)\\). Entonces\n\\[\n\\begin{align*}\np(y; \\mu, \\sigma^2) & = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(y - \\mu)^2}{2\\sigma^2}\\right) \\\\\n& = \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2\\sigma^2}y^2+\\frac{\\mu}{\\sigma^2}y-\\frac{\\mu^2}{2\\sigma^2}-\\log\\sigma\\right) \\\\\n& = \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(\\left(\\frac{\\mu}{\\sigma^2},-\\frac{1}{2\\sigma^2}\\right)^T(y,y^2)-\\frac{\\mu^2}{2\\sigma^2}-\\log\\sigma\\right)\n\\end{align*}\n\\]\nEl parámetro natural es el vector\n\\[\\bfeta=\\left(\\frac{\\mu}{\\sigma^2},-\\frac{1}{2\\sigma^2}\\right)\\]\ny el resto de los elementos son \\[\\TT(y)=(y,y^2), \\qquad a(\\bfeta)=\\frac{\\mu^2}{2\\sigma^2}+\\log\\sigma=-\\frac{\\eta_1^2}{4\\eta_2}+\\frac{1}{2}\\log(-2\\eta_2), \\qquad b(y)=\\frac{1}{\\sqrt{2\\pi}}.\\]\n\nExisten muchas otras distribuciones que son miembros de la familia exponencial: la multinomial (que veremos más adelante), la Poisson (para modelar datos de conteo), la gamma y la exponencial (para modelar variables continuas no negativas, como intervalos de tiempo); la beta y la Dirichlet (para distribuciones sobre probabilidades); y muchas más.\nEn la próxima sección, describiremos una “receta” general para construir modelos en los cuales \\(y\\) (dado \\(\\xx\\) y \\(\\theta\\)) proviene de cualquiera de estas distribuciones."
  },
  {
    "objectID": "CAPITULO_2/02_GLM.html#terminología",
    "href": "CAPITULO_2/02_GLM.html#terminología",
    "title": "Modelos lineales generalizados",
    "section": "2.1 Terminología",
    "text": "2.1 Terminología\n\nLa función \\(g(\\bfeta) = \\mathbb{E}[T(y); \\bfeta]\\) se denomina función de respuesta canónica.\nLa inversa de esta función, \\(g^{-1}\\), se llama la función de enlace canónica."
  },
  {
    "objectID": "CAPITULO_2/02_GLM.html#el-principio-de-máxima-verosimilitud",
    "href": "CAPITULO_2/02_GLM.html#el-principio-de-máxima-verosimilitud",
    "title": "Modelos lineales generalizados",
    "section": "3.1 El principio de máxima verosimilitud",
    "text": "3.1 El principio de máxima verosimilitud\nEl ajuste de parámetros en los GLMs se basa en maximizar la log-verosimilitud de los datos observados. Dado un conjunto de entrenamiento \\(\\{(\\xx^{(i)}, y^{(i)})\\}_{i=1}^n\\), la función de log-verosimilitud está dada por:\n\\[\n\\ell(\\theta) = \\log L(\\theta) = \\sum_{i=1}^n \\log p(y^{(i)} | \\xx^{(i)}; \\theta).\n\\]\nPara maximizar \\(\\ell(\\theta)\\), derivamos con respecto a \\(\\theta\\) para obtener el gradiente. En GLMs, las propiedades de la familia exponencial simplifican esta derivada, que resulta ser:\n\\[\n\\nabla_\\theta \\ell(\\theta) = \\sum_{i=1}^n (T(y^{(i)}) - \\mathbb{E}[T(y) | \\xx^{(i)}; \\theta]) \\xx^{(i)}.\n\\]\nA continuación describiremos dos métodos iterativos para realizar la tarea de maximizar la log-verosimilitud.\n\n3.1.1 El método de gradiente descendente\nEste método optimiza \\(\\ell(\\bftheta)\\) iterativamente ajustando los parámetros en la dirección opuesta al gradiente de la función objetivo, con un paso de tamaño controlado por una tasa de aprendizaje \\(\\alpha\\):\n\\[\n\\bftheta^{(t+1)} = \\bftheta^{(t)} - \\alpha \\nabla_\\theta \\ell(\\bftheta^{(t)}).\n\\]\nEl gradiente descendente es computacionalmente barato, pero su convergencia puede ser lenta, especialmente si no se ajusta adecuadamente la tasa de aprendizaje.\n\n\n3.1.2 El método de Newton-Raphson\nEste método requiere calcular la Hessiana de \\(\\ell(\\theta)\\), que es la matriz de segundas derivadas\n\\[\nH(\\bftheta) = \\sum_{i=1}^n \\left(-\\text{Var}[T(y) | \\xx^{(i)}; \\bftheta]\\right) \\xx^{(i)} (\\xx^{(i)})^T,\n\\]\nEl método de Newton-Raphson actualiza \\(\\bftheta\\) como:\n\\[\n\\bftheta^{(t+1)} = \\bftheta^{(t)} - H(\\bftheta^{(t)})^{-1} \\nabla_\\theta \\ell(\\bftheta^{(t)}).\n\\]\nAunque este método converge rápidamente en muchas aplicaciones, calcular la Hessiana e invertirla puede ser computacionalmente costoso para modelos de alta dimensionalidad."
  },
  {
    "objectID": "CAPITULO_2/02_GLM.html#la-distribución-multinomial-como-familia-exponencial",
    "href": "CAPITULO_2/02_GLM.html#la-distribución-multinomial-como-familia-exponencial",
    "title": "Modelos lineales generalizados",
    "section": "4.1 La distribución multinomial como familia exponencial",
    "text": "4.1 La distribución multinomial como familia exponencial\nSupongamos que \\(Y\\in\\{1, 2, \\cdots, k\\}\\) se distribuye mediante una distribución multinomial con \\(k-1\\) parámetros \\(\\phi_1, \\dots, \\phi_{k-1}\\), y sea \\(\\phi_k=1-\\sum_{i=1}^{k-1}\\phi_i\\), tal que \\(\\sum_{i=1}^k \\phi_i = 1\\). Para expresar la multinomial como una distribución de la familia exponencial, definimos\n\\[\nT(1) = \\begin{pmatrix} 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix},\n\\quad\nT(2) = \\begin{pmatrix} 0 \\\\ 1 \\\\ \\vdots \\\\ 0 \\end{pmatrix},\n\\quad \\dots, \\quad\nT(k-1) = \\begin{pmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 1 \\end{pmatrix},\n\\quad\nT(k) = \\begin{pmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix}.\n\\]\nEn lo que sigue, \\(\\bfone\\{\\cdot\\}\\) es la función indicatriz, que vale 1 si su argumento es cierto y 0 en caso contrario. Resulta\n\\[\n\\begin{align*}\np(y; \\mathbf{\\phi})& =\n\\phi_1^{\\bfone\\{y=1\\}} \\phi_2^{\\bfone\\{y=2\\}} \\cdots \\phi_k^{\\bfone\\{y=k\\}}\\\\\n&= \\phi_1^{(T(y))_1} \\phi_2^{(T(y))_2} \\cdots \\phi_{k-1}^{(T(y))_{k-1}} \\left(1 - \\sum_{i=1}^{k-1} \\phi_i\\right)^{1-\\sum_{i=1}^{k-1}(T(y))_i}\n\\\\\n&= \\exp\\left((T(y))_1 \\log(\\phi_1) + (T(y))_2 \\log(\\phi_2) + \\dots + \\left(1 - \\sum_{i=1}^{k-1}(T(y))_i\\right)\\log\\left(1 - \\sum_{i=1}^{k-1} \\phi_i\\right)\\right).\n\\end{align*}\n\\]\nAsí, \\(Y\\) se distribuye en la familia exponencial\n\\[\np(y; \\phi) = b(y) \\exp\\left(\\eta^T T(y) - a(\\eta)\\right),\n\\]\ndonde:\n\\[\n\\begin{align*}\n\\eta &= \\begin{bmatrix}\n\\log\\left(\\frac{\\phi_1}{\\phi_k}\\right) \\\\\n\\log\\left(\\frac{\\phi_2}{\\phi_k}\\right) \\\\\n\\vdots \\\\\n\\log\\left(\\frac{\\phi_{k-1}}{\\phi_k}\\right)\n\\end{bmatrix},\n\\\\\na(\\eta) &= -\\log(\\phi_k),\n\\\\\nb(y)& = 1.\n\\end{align*}\n\\]\nEsto completa nuestra formulación de la multinomial como una distribución de la familia exponencial.\nLa función que mapea () a () se llama la función softmax:"
  },
  {
    "objectID": "CAPITULO_2/02_GLM.html#función-de-enlace-y-función-de-respuesta",
    "href": "CAPITULO_2/02_GLM.html#función-de-enlace-y-función-de-respuesta",
    "title": "Modelos lineales generalizados",
    "section": "4.2 Función de Enlace y Función de Respuesta",
    "text": "4.2 Función de Enlace y Función de Respuesta\nLa función de enlace está dada (para \\(i = 1, \\dots, k\\)) por:\n\\[\n\\eta_i = \\log\\left(\\frac{\\phi_i}{\\phi_k}\\right).\n\\]\nPara conveniencia, también definimos \\(\\eta_k = \\log(\\phi_k / \\phi_k) = 0\\). Para invertir la función de enlace y derivar la función de respuesta, tenemos que:\n\\[\ne^{\\eta_i} = \\frac{\\phi_i}{\\phi_k},\n\\]\nlo que implica:\n\\[\n\\phi_k \\sum_{i=1}^k e^{\\eta_i} = \\sum_{i=1}^k \\phi_i = 1.\n\\]\nPor lo tanto:\n\\[\n\\phi_k = \\frac{1}{\\sum_{i=1}^k e^{\\eta_i}},\n\\]\ny al sustituir en la ecuación obtenemos la función de respuesta:\n\\[\n\\phi_i = \\frac{e^{\\eta_i}}{\\sum_{j=1}^k e^{\\eta_j}}.\n\\]"
  },
  {
    "objectID": "CAPITULO_2/02_GLM.html#softmax-function-y-modelo-de-clasificación",
    "href": "CAPITULO_2/02_GLM.html#softmax-function-y-modelo-de-clasificación",
    "title": "Modelos lineales generalizados",
    "section": "4.3 Softmax Function y Modelo de Clasificación",
    "text": "4.3 Softmax Function y Modelo de Clasificación\nLa función que mapea de \\(\\eta\\) a \\(\\phi\\) se llama la función .\nPara completar nuestro modelo, usamos la Asunción 3, mencionada previamente, donde los \\(\\eta_i\\) están relacionados linealmente con los \\(x\\)’s. Así, tenemos \\(\\eta_i = \\theta_i^T x\\) (para \\(i = 1, \\dots, k - 1\\)), donde \\(\\theta_1, \\dots, \\theta_{k-1} \\in \\mathbb{R}^{d+1}\\) son los parámetros de nuestro modelo. Por notación, podemos definir \\(\\theta_k = 0\\), de modo que \\(\\eta_k = \\theta_k^T x = 0\\), como se definió anteriormente. Por lo tanto, nuestro modelo asume que la distribución condicional de \\(y\\) dado \\(x\\) está dada por:\n\\[\n\\begin{align*}\np(y = i | x; \\theta) &= \\phi_i\\\\\n&= \\frac{e^{\\eta_i}}{\\sum_{j=1}^k e^{\\eta_j}}\\\\\n&= \\frac{e^{\\theta_i^T x}}{\\sum_{j=1}^k e^{\\theta_j^T x}}, \\quad (5)\n\\end{align*}\n\\]\ndonde este modelo, que aplica a problemas de clasificación donde \\(y \\in \\{1, \\dots, k\\}\\), se llama . Es una generalización de la regresión logística.\n\n4.3.1 Hipótesis del Modelo\nNuestra hipótesis producirá:\n\\[\n\\begin{align*}\nh_\\theta(x) &= \\mathbb{E}[T(y) | x; \\theta]\n\\\\\n&= \\mathbb{E}\n\\begin{bmatrix}\n1\\{y = 1\\} \\\\\n1\\{y = 2\\} \\\\\n\\vdots \\\\\n1\\{y = k - 1\\}\n\\end{bmatrix}\n\\\\&=\n\\begin{bmatrix}\n\\phi_1 \\\\\n\\phi_2 \\\\\n\\vdots \\\\\n\\phi_{k-1}\n\\end{bmatrix}\n\\\\&=\n\\begin{bmatrix}\n\\frac{\\exp(\\theta_1^T x)}{\\sum_{j=1}^k \\exp(\\theta_j^T x)} \\\\\n\\frac{\\exp(\\theta_2^T x)}{\\sum_{j=1}^k \\exp(\\theta_j^T x)} \\\\\n\\vdots \\\\\n\\frac{\\exp(\\theta_{k-1}^T x)}{\\sum_{j=1}^k \\exp(\\theta_j^T x)}\n\\end{bmatrix}.\n\\end{align*}\n\\]\nEn otras palabras, nuestra hipótesis producirá la probabilidad estimada de que \\(p(y = i | x; \\theta)\\) para cada valor \\(i = 1, \\dots, k\\). (Aunque \\(h_\\theta(x)\\), como se definió arriba, tiene solo \\(k - 1\\) dimensiones, claramente \\(p(y = k | x; \\theta)\\) puede obtenerse como \\(1 - \\sum_{i=1}^{k-1} \\phi_i\\))."
  },
  {
    "objectID": "CAPITULO_2/02_GLM.html#ajuste-de-parámetros-1",
    "href": "CAPITULO_2/02_GLM.html#ajuste-de-parámetros-1",
    "title": "Modelos lineales generalizados",
    "section": "4.4 Ajuste de Parámetros",
    "text": "4.4 Ajuste de Parámetros\nPor último, discutamos el ajuste de parámetros. Similar a nuestra derivación original de mínimos cuadrados ordinarios y regresión logística, si tenemos un conjunto de entrenamiento de \\(n\\) ejemplos \\(\\{(x^{(i)}, y^{(i)}); i = 1, \\dots, n\\}\\) y deseamos aprender los parámetros \\(\\theta_i\\) de este modelo, comenzaríamos escribiendo la log-verosimilitud:\n\\[\n\\begin{align*}\n\\ell(\\theta) &= \\sum_{i=1}^n \\log p(y^{(i)} | x^{(i)}; \\theta)\n\\\\\n&= \\sum_{i=1}^n \\log \\prod_{l=1}^k \\left( \\frac{e^{\\theta_l^T x^{(i)}}}{\\sum_{j=1}^k e^{\\theta_j^T x^{(i)}}} \\right)^{1\\{y^{(i)} = l\\}}.\n\\end{align*}\n\\]\nPara obtener la segunda línea anterior, usamos la definición de \\(p(y | x; \\theta)\\) dada en la Ecuación (5). Ahora podemos obtener la estimación de máxima verosimilitud de los parámetros maximizando \\(\\ell(\\theta)\\) en términos de \\(\\theta\\), utilizando un método como ascenso de gradiente o el método de Newton.\nACA FALTA AJUSTE DE PARAMETROS PARA EL MODELO GLM EN GENERAL eso esta en el video de el viejo del 2008 , no esta en el nuevo. seria importante poner algo\n\n4.4.1 Ejemplo Práctico en Python\nImplementemos un ejemplo práctico para un problema de clasificación multiclasificación.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# Generar datos simulados para clasificación multiclase\nX, y = make_classification(\n    n_samples=300, n_features=2, n_informative=2, n_redundant=0,\n    n_classes=3, n_clusters_per_class=1, random_state=42\n)\n\n# Visualizar los datos\nplt.figure(figsize=(8, 6))\nscatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap=\"viridis\")\nplt.title(\"Datos de Clasificación Multiclase\")\nplt.xlabel(\"Característica 1\")\nplt.ylabel(\"Característica 2\")\nplt.colorbar(scatter, label=\"Clases\")\nplt.grid(True)\nplt.show()\n\n# Dividir los datos en entrenamiento y prueba\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Ajustar el modelo de regresión softmax\nmodel = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", max_iter=200)\nmodel.fit(X_train, y_train)\n\n# Predicciones\ny_pred = model.predict(X_test)\n\n# Frontera de decisión\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n                     np.arange(y_min, y_max, 0.01))\nZ = model.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nplt.figure(figsize=(8, 6))\nplt.contourf(xx, yy, Z, alpha=0.8, cmap=\"viridis\")\nscatter = plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor=\"k\", cmap=\"viridis\")\nplt.title(\"Frontera de Decisión: Regresión Softmax\")\nplt.xlabel(\"Característica 1\")\nplt.ylabel(\"Característica 2\")\nplt.colorbar(scatter, label=\"Clases\")\nplt.grid(True)\nplt.show()\n\n# Evaluación del modelo\nprint(\"Reporte de Clasificación:\")\nprint(classification_report(y_test, y_pred))\nprint(\"Matriz de Confusión:\")\nprint(confusion_matrix(y_test, y_pred))\n\n\n\n\n\n\n\n\nC:\\Users\\isaia\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\nReporte de Clasificación:\n              precision    recall  f1-score   support\n\n           0       0.75      0.89      0.81        27\n           1       0.92      0.89      0.91        38\n           2       1.00      0.84      0.91        25\n\n    accuracy                           0.88        90\n   macro avg       0.89      0.87      0.88        90\nweighted avg       0.89      0.88      0.88        90\n\nMatriz de Confusión:\n[[24  3  0]\n [ 4 34  0]\n [ 4  0 21]]"
  },
  {
    "objectID": "CAPITULO_1/A2_optimalidad.html",
    "href": "CAPITULO_1/A2_optimalidad.html",
    "title": "Condiciones de optimalidad en Optimización",
    "section": "",
    "text": "En un problema de optimización general \\[\n\\min_{x} f(x)\n\\] \\[\n\\text{s.t.}\\; x\\in\\Omega=\\left\\{x\\in\\RR^n\\left|\\begin{array}{rl}\ng_i(x)\\leq 0,& i=1,\\cdots,r.\\\\\nh_j(x)=0, & j=1,\\cdots,m.\n\\end{array}\\right.\\right\\},\n\\]\nlas condiciones de optimalidad definen los requisitos que deben cumplir los puntos óptimos. En lo que sigue, asumiremos que trabajamos con funciones diferenciables."
  },
  {
    "objectID": "CAPITULO_1/A2_optimalidad.html#optimización-sin-restricciones",
    "href": "CAPITULO_1/A2_optimalidad.html#optimización-sin-restricciones",
    "title": "Condiciones de optimalidad en Optimización",
    "section": "1.1 Optimización sin restricciones",
    "text": "1.1 Optimización sin restricciones\nDe cursos anteriores recordemos que, cuando se pretende optimizar una función \\(f\\) respecto a \\(x\\in\\RR^n\\), una condición necesaria para que un punto sea óptimo es que verifique \\[\n\\nabla f(x)=\\mathbf{0}.\n\\]\nPero cuidado: es solo una condición necesaria que todos los puntos óptimos deben cumplir, pero no implica que cualquier punto que la satisfaga sea automáticamente óptimo. En otras palabras, las soluciones de \\(\\nabla f(x) = 0\\) forman una lista de puntos candidatos para minimizar, llamados puntos críticos.\nDe inmediato surgen dos preguntas claves:\n\n¿Cuál es la generalización correcta de la condición necesaria \\(\\nabla f(x) = 0\\) cuando enfrentamos un problema de optimización con restricciones?\n¿Bajo qué circunstancias \\(\\nabla f(x) = 0\\) también se convierte en una condición suficiente para la optimalidad?\n\nAntes, veamos cómo surge la condición \\(\\nabla f(x)=\\mathbf{0}\\) para problemas de optimización sin restricciones.\n\n1.1.1 Prueba de condición necesaria \\(\\nabla f(x)=\\mathbf{0}\\)\nSea \\(x\\in\\RR^n\\) es un minimizador de la función \\(f: \\RR^n \\to \\RR\\) y sea \\(d\\in\\RR^n\\) una dirección arbitraria. Entonces se cumple\n\\[\nf(x + t \\cdot d) \\geq f(x) \\quad \\text{para todo } t \\geq 0.\n\\]\nEn consecuencia, la derivada direccional de \\(f\\) en \\(x\\) a lo largo de \\(d\\) es\n\\[\n\\frac{\\partial f}{\\partial d}(x)= \\lim_{t \\to 0} \\frac{f(x + t \\cdot d) - f(x)}{t} \\geq 0.\n\\]\nAhora bien, como \\(f\\) es diferenciable, se verifica la propiedad \\(\\frac{\\partial f}{\\partial d}(x)=\\langle \\nabla f(x),d\\rangle\\), por lo que la desigualdad anterior se puede reescribir como:\n\\[\n\\langle \\nabla f(x), d \\rangle \\geq 0 \\quad \\forall d \\in {R}^n.\n\\]\nEn particular, eligiendo \\(d = -\\nabla f(x)\\), resulta \\(-\\|\\nabla f(x)\\|^2\\geq 0\\), lo cual es cierto si y sólo si \\(\\nabla f(x)=\\mathbf{0}\\)."
  },
  {
    "objectID": "CAPITULO_1/A2_optimalidad.html#optimización-con-restricciones",
    "href": "CAPITULO_1/A2_optimalidad.html#optimización-con-restricciones",
    "title": "Condiciones de optimalidad en Optimización",
    "section": "1.2 Optimización con restricciones",
    "text": "1.2 Optimización con restricciones\nLa clave para generalizar la condición \\(\\nabla f(x)=\\mathbf{0}\\) al caso de optimización con restricciones surge de la prueba anterior, más precisamente de la desigualdad \\(\\langle\\nabla f(x),d\\rangle\\geq 0\\).\nLa diferencia principal con el caso sin restricciones es que, en un conjunto restringido, podríamos estar limitados en la elección de las direcciones \\(d\\) a lo largo de las cuales podemos aproximarnos a \\(x\\) sin salirnos del conjunto.\nNo obstante, para cualquier dirección \\(d\\) tal que \\(x + t \\cdot d \\in \\Omega\\) para todo \\(t \\geq 0\\) suficientemente pequeño, el mismo argumento aplicado anteriormente sigue siendo válido. Por lo tanto, podemos concluir que necesariamente:\n\\[\n\\langle \\nabla f(x), d \\rangle \\geq 0 \\quad \\text{para todo } d \\in  {R}^n \\text{ que permanezca en } \\Omega \\text{ desde } x.\n\\]\nPara aplicar esta condición, se requieren dos pasos:\n\nDeterminar el conjunto de direcciones \\(d\\) que permanecen en \\(\\Omega\\) desde \\(x\\)*.\nA partir de esas direcciones, ver de qué manera imponen restricciones sobre \\(\\nabla f(x)\\).\n\nDe estos dos pasos, el primero suele ser el más sencillo. En todos los casos de interés, podemos determinar el conjunto de direcciones permitidas simplemente considerando cualquier otro punto \\(y \\in \\Omega\\) y observando la dirección de \\(x\\) a \\(y\\). Esta propiedad se cumple trivialmente si todos los segmentos de línea entre \\(x\\) y cualquier punto en \\(\\Omega\\) están contenidos en \\(\\Omega\\), lo cual es siempre cierto para conjuntos convexos.\n\nTeorema 1. (Condición necesaria de optimalidad de primer orden para un conjunto factible convexo) Sea \\(\\Omega \\subseteq  {R}^n\\) convexo y sea \\(f:  {R}^n \\to  {R}\\) una función diferenciable. Si \\(x \\in \\Omega\\) es un minimizador de \\(f\\) sobre \\(\\Omega\\), entonces \\[\n\\langle \\nabla f(x), y - x \\rangle \\geq 0 \\quad \\forall y \\in \\Omega.\n\\]\n\n\n\nInterpretación geométrica\n\n\nLa condición del Teorema 1 se verifica si el vector gradiente de \\(f\\) en una solución \\(x\\in\\Omega\\) forma un ángulo agudo con todas las direcciones \\(y-x\\), donde \\(y\\in\\Omega\\).\nAhora bien, a la hora de buscar un minimizador, la dirección que realmente nos importa es \\(-\\nabla f(x)\\). Así, el Teorema 1 nos dice que \\(-\\nabla f(x)\\) debe formar un ángulo obtuso con las direcciones \\(y-x\\). En otras palabras, la condición dada es equivalente a escribir\n\\[\n-\\nabla f(x)\\in N_{\\Omega}(x),\n\\]\ndonde \\(N_{\\Omega}(x)\\) es el cono normal a \\(\\Omega\\) en \\(x\\).\nEs importante observar que si \\(x\\) es un punto interior de \\(\\Omega\\), sabemos que \\(N_{\\Omega}(x)=\\{\\mathbf{0}\\}\\) y, en consecuencia, debe ser \\(\\nabla f(x)=\\mathbf{0}\\). Esto es consistente con la condición de optimalidad de los problemas sin restricciones.\nNo obstante, la importancia del Teorema 1 radica en cómo tratar los puntos en la frontera de \\(\\Omega\\). A partir de los ejemplos previamente estudiados sobre el cono normal, podemos ver que la interpretación del Teorema 1 es bastante intuitiva. Por ejemplo, si \\(\\Omega\\) es un polígono y \\(x\\in\\Omega\\) es una solución, el vector \\(-\\nabla f(x)\\) es perpendicular a \\(\\Omega\\) y apunta hacia afuera, lo cual significa que no hay posibilidad de “moverse” en dirección opuesta al gradiente sin salirse de \\(\\Omega\\).\n\n\n¡Cuidado! La condición provista por el Teorema 1 es necesaria pero no suficiente. Podría verificarse la condición en otros puntos críticos de \\(\\Omega\\) que no sean un mínimo global de \\(f\\) en dicho conjunto. Sin embargo, para el notable caso de las funciones convexas dicha condición sí es suficiente.\n\nTeorema 2. (Condición necesaria de optimalidad de primer orden para un conjunto factible convexo y una función objetivo convexa) Sea \\(\\Omega \\subseteq  {R}^n\\) convexo y sea \\(f:  {R}^n \\to  {R}\\) una función diferenciable convexa. Entonces \\[\n-\\nabla f(x)\\in N_{\\Omega}(x)\\Leftrightarrow x \\text{ es un minimizador de }f\\text{ en }\\Omega.\n\\]"
  },
  {
    "objectID": "CAPITULO_1/A2_optimalidad.html#el-caso-particular-de-restricciones-lineales",
    "href": "CAPITULO_1/A2_optimalidad.html#el-caso-particular-de-restricciones-lineales",
    "title": "Condiciones de optimalidad en Optimización",
    "section": "2.1 El caso particular de restricciones lineales",
    "text": "2.1 El caso particular de restricciones lineales\nVamos a considerar el caso en que \\(\\Omega\\) un polítopo convexo, esto es, el conjunto convexo definido por la intersección de un número finito de medios espacios (desigualdades lineales):\n\\[\n\\Omega=\\{\\xx\\in\\RR^n|A\\xx\\leq \\bfb\\},\n\\]\ndonde \\(A\\in\\RR^{m\\times n}\\) es una matriz, cuyas filas denotaremos con \\(\\bfa_j\\), \\(j=1,\\cdots,m\\), y \\(\\bfb\\in\\RR^m\\).\nEn el ejemplo … (Agregar ejemplo 3.6 de Lecture 2 a ‘Intro’) hemos visto que el cono normal en un punto en la intersección de dos medios espacios es la envolvente cónica de las direcciones ortogonales a dichos subespacios.\n\n\n\nEnvolvente cónica de la intersección de dos subespacios \\(\\bfa_1^T\\xx\\leq b_1\\) y \\(\\bfa_2^T\\xx\\leq b_2\\).\n\n\nPor otra parte, los otros casos también ya han sido vistos:\n\nSi \\(\\xx\\) pertenece al interior de \\(\\Omega\\), entonces \\(N_{\\Omega}(\\xx)=\\{\\mathbf{0}\\}\\).\nSi \\(\\xx\\) pertenece a la frontera de un único medio espacio, a saber \\(\\bfa_k^T\\xx=b_k\\), entonces \\(N_{\\Omega}(\\xx)=\\{\\lambda \\bfa_k:\\lambda\\geq 0\\}\\).\n\nLa generalización al caso de \\(m\\) medios espacios se presenta en el siguiente teorema.\n\nTeorema 3. Sea \\(\\Omega=\\{x\\in\\RR^n\\}\\) la intersección de \\(m\\) medios espacios \\(\\bfa_j^T\\xx\\leq b_j\\). Dado un punto \\(x\\in\\Omega\\), se define el conjunto de índices de las restricciones activas mediante\n\\[\nI(\\xx):=\\left\\{j\\in\\{1,\\cdots,m\\}:\\bfa_j^T\\xx=b_j\\right\\}.\n\\] Entonces, el cono normal en \\(\\xx\\) está dado por\n\\[\nN_{\\Omega}(\\xx)=\\left\\{\\sum_{j\\in I(\\xx)}\\lambda_j\\bfa_j:\\lambda_j\\geq 0\\right\\}.\n\\]\n\n\n\nInterpretación\n\n\nSi \\(\\xx\\) pertenece al interior de \\(\\Omega\\), entonces no hay restricciones activas. Esto se corresponde con el hecho de que el cono normal es \\(N_{\\Omega}(\\xx)=\\{\\mathbf{0}\\}\\).\nPor su parte, si \\(\\xx\\) pertenece a la frontera de \\(\\Omega\\), entonces el cono normal es la envolvente cónica de las direcciones ortogonales a las restricciones activas.\nObservar que la condición de optimalidad \\(-\\nabla f(\\xx)\\in N_{\\Omega}(\\xx)\\) se traduce como\n\\[\n-\\nabla f(\\xx)=\\sum_{j\\in I(\\xx)}\\lambda_j\\bfa_j,\n\\]\ny, en consecuencia, se puede escribir\n\\[\n\\nabla f(\\xx)-\\sum_{j\\in I(\\xx)}\\lambda_j \\nabla g_j(\\xx)=0,\n\\]\ncon \\(g_j(\\xx)=\\bfa_j^T\\xx-b\\). Los coeficientes \\(\\lambda_j\\) se denominan tipicamente multiplicadores de Lagrange.\n\n\nLa suma en la expresión del cono normal puede ser reescrita sin restringir \\(j\\in I(\\xx)\\), simplemente imponiendo \\(\\lambda_j=0\\) para todo \\(j\\notin I(\\xx)\\). Esta imposición queda ímplicita de forma inmediata si se escribe\n\\[\n\\sum_{j=1}^m\\lambda_j\\left(\\bfa_j^T\\xx-b_j\\right)=0.\n\\]\nEn forma vectorial, esto es \\(\\bflambda^T\\left(A\\xx-\\bfb\\right)=0\\), donde \\(\\bflambda=(\\lambda_1,\\cdots,\\lambda_m)^T\\). Con esta notación, el resultado del Teorema 3 puede reescribirse como\n\\[\nN_{\\Omega}(\\xx)=\\left\\{A^T\\bflambda:\\bflambda^T(A\\xx-\\bfb)=0,\\bflambda\\in\\RR^m_{\\geq 0}\\right\\}.\n\\]\nPara concluir este análisis, y teniendo en cuenta lo expuesto hasta aquí, podemos hacer énfasis en tres condiciones necesarias para que una solución sea óptima en este caso particular de restricciones lineales.\n\nPara que \\(\\xx\\in\\Omega\\) sea un minimizador de \\(f\\) sobre \\(\\Omega\\), debe cumplir:\n\nEstacionariedad: El gradiente debe ser una combinación lineal de los gradientes de las restricciones activas. \\[\n\\nabla f(\\xx)-\\sum_{j=1}^{m}\\lambda_j(\\bfa_j^T\\xx-b_j).\n\\]\nFactibilidad dual: Los multiplicadores de Lagrange asociados a las restricciones de desigualdad deben ser no negativos. \\[\n\\lambda_j\\geq 0 \\quad \\forall j=1,\\cdots,m.\n\\]\nHolgura complementaria: Los multiplicadores de Lagrange solo pueden ser positivos si la restricción está activa. \\[\n\\lambda_j\\left(\\bfa_j^T\\xx-b_j\\right)=0 \\quad \\forall j=1,\\cdots,m.\n\\]\n\n\n Las condiciones mencionadas se conocen como condiciones de Karush-Kuhn-Tucker y son una consecuencia de la caracterización del cono normal para conjuntos definidos como interseccion de restricciones lineales, provista por el Teorema 3. Ya estamos en condiciones de abordar el problema de optimización general."
  },
  {
    "objectID": "CAPITULO_1/A2_optimalidad.html#generalización",
    "href": "CAPITULO_1/A2_optimalidad.html#generalización",
    "title": "Condiciones de optimalidad en Optimización",
    "section": "2.2 Generalización",
    "text": "2.2 Generalización\nConsideremos el problema de optimización general \\[\n\\min_{x} f(x)\n\\] \\[\n\\text{s.t.}\\; x\\in\\Omega=\\left\\{x\\in\\RR^n\\left|\\begin{array}{rl}\nh_i(x)= 0,& i=1,\\cdots,r.\\\\\ng_j(x)\\leq 0, & j=1,\\cdots,m.\n\\end{array}\\right.\\right\\},\n\\]\ndonde \\(\\Omega\\) está definido como una intersección de restricciones funcionales diferenciables.\nSupongamos que \\(\\xx^*\\) es un punto óptimo en la frontera del conjunto factible \\(\\Omega\\) correspondiente a tres condiciones de desigualdad \\(g_i(x) \\le 0\\) para \\(i=1,2,3\\) (ver figura abajo). La idea principal es la siguiente:\n\nEl conjunto de direcciones que forma un ángulo obtuso con todas las direcciones desde \\(\\xx^*\\) que permanecen en \\(\\Omega\\) coincide con el cono normal de la linearización de las restricciones activas en \\(\\xx^*\\).\n\n\n\n\nEnvolvente cónica de la intersección de restricciones \\(g_i(\\xx)\\leq 0\\), para \\(i=1,2,3\\).\n\n\nSi aplicamos la idea anterior a \\(-\\nabla f(\\xx)\\) y consideramos el Teorema 3, obtenemos la generalización de las condiciones de optimalidad de Karush-Kuhn-Tucker (KKT). Denominemos \\(\\tilde{\\Omega}\\) la linearización de \\(\\Omega\\), dado por la linearización de las restricciones en un punto óptimo \\(\\xx^*\\):\n\\[\nh_i(\\xx^*)+\\nabla h_i(\\xx^*)\\cdot (\\xx-\\xx^*)=0,\n\\]\n\\[\ng_i(\\xx^*)+\\nabla g_i(\\xx^*)\\cdot (\\xx-\\xx^*)\\leq 0.\n\\]\nEntonces, por Teorema 3, resulta\n\\[\nN_{\\tilde{\\Omega}}(\\xx^*)=\\left\\{\\sum_{i=1}^m\\lambda_i\\nabla h_i(\\xx^*)+\\sum_{j\\in I(\\xx^*)}\\mu_j\\nabla g_j(\\xx^*): \\lambda_i\\in\\RR, \\mu_j\\in\\RR_{\\geq 0}\\right\\},\n\\]\ndonde \\(I(\\xx^*):=\\left\\{j\\in\\{1,\\cdots,m\\}: g_j(\\xx^*)=0\\right\\}\\). Por supuesto, puede reformularse la condición $jI(^*) utilizando holgura complementaria, tal como antes.\nSi bien no estamos aún en condiciones de extender el Teorema 3, puesto que el desarrollo previo fue más bien intuitivo, podemos establecer las condiciones KKT mediante una definición formal de la siguiente manera.\n\nDefinición 1. (Condiciones de Karush-Kuhn-Tucker (KKT) Sea un problema de optimización no lineal con función objetivo diferenciable y restricciones funcionales, de la forma \\[\n\\text{s.t.}\\; x\\in\\Omega=\\left\\{x\\in\\RR^n\\left|\\begin{array}{rl}\nh_i(x)= 0,& i=1,\\cdots,r.\\\\\ng_j(x)\\leq 0, & j=1,\\cdots,m.\n\\end{array}\\right.\\right\\},\n\\]\ny sea \\(\\xx\\in\\Omega\\) (factibilidad primal). Las condiciones KKT en \\(\\xx\\) están dadas por:\n\nEstacionariedad: \\[\n-\\nabla f(\\xx)=\\sum_{i=1}^r\\lambda_i\\nabla h_i(\\xx)+\\sum_{j=1}^m\\mu_j\\nabla g_j(\\xx).\n\\]\nFactibilidad dual: \\[\n\\lambda_i\\in\\RR, \\mu_j\\in\\RR_{\\geq 0}\\quad\\forall i=1,\\cdots,r, \\forall j=1,\\cdots, m.\n\\]\nHolgura complementaria: \\[\n\\mu_j g_j(\\xx)=0\\quad \\forall j=1,\\cdots,m.\n\\]\n\n\nEs importante remarcar que:\n\nLas condiciones KKT indican que -\\(\\nabla f(\\xx)\\) debe estar en el cono normal a la linearización del conjunto de restricciones.\nLa condición de holgura compelementaria se resume en “si la restricción \\(g_j(\\xx)\\leq 0\\) no está activa, entonces \\(\\mu_j=0\\)”.\nLas condiciones KKT a menudo son necesarias para la optimalidad, pero no siempre.\n\nA continuación, veremos un ejemplo donde las condiciones KKT fallan. Luego, en la siguiente sección veremos en qué escenarios las condiciones KKT son efectivamente una condición necesaria de optimalidad."
  },
  {
    "objectID": "CAPITULO_1/A2_optimalidad.html#cualificación-de-restricciones",
    "href": "CAPITULO_1/A2_optimalidad.html#cualificación-de-restricciones",
    "title": "Condiciones de optimalidad en Optimización",
    "section": "2.3 Cualificación de restricciones",
    "text": "2.3 Cualificación de restricciones"
  },
  {
    "objectID": "CAPITULO_1/A2_optimalidad.html#función-dual-de-lagrange",
    "href": "CAPITULO_1/A2_optimalidad.html#función-dual-de-lagrange",
    "title": "Condiciones de optimalidad en Optimización",
    "section": "3.1 Función dual de Lagrange",
    "text": "3.1 Función dual de Lagrange\n\nDefinición 2. (Lagrangiano) Sea el problema de optimización general \\[\n\\min f(\\xx)\n\\] \\[\n\\text{sujeto a}\\; \\xx\\in\\Omega=\\left\\{\\xx\\in\\RR^p\\left|\\begin{array}{rl}\ng_i(\\xx)\\leq 0,& i=1,\\cdots,q\\\\\nh_j(\\xx)= 0, & j=1,\\cdots,r\n\\end{array}\\right.\\right\\}.\n\\]\nSe denomina Lagrangiano a la función \\(\\calL:\\RR^p\\times\\RR^q\\times\\RR^r\\to\\RR\\) definida por \\[\n\\calL(\\xx,\\bflambda,\\bfnu)=f(\\xx)+\\sum_{i=1}^q\\lambda_i g_i(\\xx)+\\sum_{j=1}^r\\nu_j h_j(\\xx).\n\\]\n\nLos vectores \\(\\bflambda\\) y \\(\\bfnu\\), cuyas componentes son multiplicadores de Lagrange, se denominan variables duales del problema de optimización y son el argumento de la función definida a continuación. A su vez, al problema de optimización original se lo denomina problema primal.\n\nDefinición 3. (Función dual) Sea \\(\\calL\\) el Lagrangiano de la Definición 2. Se denomina función dual de Lagrange a \\(\\calG:\\RR^q\\times\\RR^r\\to\\RR\\) definida por \\[\n\\calG(\\bflambda,\\bfmu)=\\inf_{\\xx}\\calL(\\xx,\\bflambda,\\bfnu).\n\\]\nCuando el Lagrangiano no está acotado inferiormente en \\(\\xx\\), se asume el valor \\(-\\infty\\).\n\nDado que la función dual es el ínfimo puntual de una familia de funciones afínes de \\((\\bflambda,\\bfnu)\\), es cóncava, aún cuando el problema primal no sea convexo [ver Ejercicio …].\nLa propiedad más importante de la función dual es que es una cota inferior del valor óptimo \\(p^*\\) del problema primal. Es decir, para todo \\(\\bflambda\\geq 0\\) y cualquier \\(\\bfnu\\), resulta \\[\n\\calG(\\bflambda,\\bfnu)\\leq p^*.\n\\]\n\n\nVerificación\n\n\nSea \\(\\tilde{\\xx}\\) un punto factible del problema primal. Entonces \\(g_i(\\tilde{\\xx})\\leq 0\\) y \\(h_j(\\tilde{\\xx})=0\\) para todo \\(i=1,\\cdots,q\\) y \\(j=1,\\cdots,r\\), respectivamente. Por lo tanto, para \\(\\bflambda\\geq 0\\) se tiene que \\[\n\\sum_{i=1}^q\\lambda_ig_i(\\tilde{\\xx})+\\sum_{j=1}^r\\nu_j h_j(\\tilde{\\xx})\\geq 0.\n\\]\nSumando \\(f(\\tilde{\\xx})\\) a ambos miembros, se obtiene \\(\\calL(\\xx,\\bflambda,\\bfnu)\\leq f(\\tilde{\\xx})\\). Luego, por propiedad del ínfimo, resulta \\[\n\\calG(\\bflambda,\\bfnu)\\leq f(\\tilde{\\xx}).\n\\]\nFinalmente, dado que \\(\\tilde{\\xx}\\) es cualquier punto factible, en particular la desigualdad anterior es cierta para un punto óptimo \\(\\xx^*\\) tal que \\(f(\\xx^*)=p^*\\)."
  },
  {
    "objectID": "CAPITULO_1/A2_optimalidad.html#el-problema-dual-de-lagrange",
    "href": "CAPITULO_1/A2_optimalidad.html#el-problema-dual-de-lagrange",
    "title": "Condiciones de optimalidad en Optimización",
    "section": "3.2 El problema dual de Lagrange",
    "text": "3.2 El problema dual de Lagrange\nInmediatamente nos podemos preguntar: ¿cual es el valor máximo \\(d^*\\) de \\(\\calG(\\bflambda,\\bfnu)\\) si asumimos \\(\\bflambda\\geq 0\\)? Esto da lugar al problema que definiremos a continuación. Observar que la desigualdad \\(\\calG(\\bflambda,\\bfnu)\\leq p^*\\) implica \\[\nd^*\\leq p^*.\n\\]\n\nDefinición 3. (Problema dual) Sea \\(\\calG\\) la función dual de Lagrange asociado al problema primal de la Definición 2. El problema dual de Lagrange es \\[\n\\max \\calG(\\bflambda,\\bfnu)\n\\] \\[\n\\text{sujeto a }\\; \\bflambda\\geq 0.\n\\]\n\nEn línea con la notación anterior, denominaremos multiplicadores óptimos a un par \\((\\bflambda^*,\\bfnu^*)\\) que es solución del problema dual. Es decir, verifican \\(\\calG(\\bflambda^*,\\bfnu^*)=d^*\\).\nEl problema dual de Lagrange es un problema de optimización convexo, independientemente de que el problema primal sea convexo o no. Esto se debe a que la función a maximizar es cóncava y la restricción es un conjunto convexo."
  },
  {
    "objectID": "CAPITULO_1/A2_optimalidad.html#suboptimalidad-y-criterio-de-parada",
    "href": "CAPITULO_1/A2_optimalidad.html#suboptimalidad-y-criterio-de-parada",
    "title": "Condiciones de optimalidad en Optimización",
    "section": "3.3 Suboptimalidad y criterio de parada",
    "text": "3.3 Suboptimalidad y criterio de parada\nLos puntos factibles duales nos permiten acotar cuán subóptimo es un punto factible dado, sin conocer el valor exacto de \\(p^{*}\\). De hecho, si \\(\\xx\\) es factible primal y \\((\\bflambda, \\bfnu)\\) es factible dual, entonces se cumple la siguiente desigualdad: \\[\nf(\\xx) - p^{*} \\leq f(\\xx)-\\calG(\\bflambda, \\bfnu).\n\\]\nEn particular, esto establece que \\(\\xx\\) es \\(\\epsilon\\)-subóptimo, con \\[\\epsilon = f(\\xx) - g(\\bflambda, \\bfnu).\n\\]\nLa brecha \\(\\epsilon\\) se conoce como brecha de dualidad para los puntos factibles \\(\\xx\\) y \\((\\bflambda,\\bfnu)\\). Para dichos puntos, los valores óptimos de los problemas primal y dual verifican \\[\np^*,d^*\\in\\left[g(\\bflambda,\\bfnu), f(\\xx)\\right],\n\\] donde el ancho del intervalo es justamente la brecha de dualidad. Observar que si \\(\\epsilon=0\\), entonces \\(\\xx\\) es óptimo primal y \\((\\bflambda,\\bfnu)\\) es óptimo dual.\nEl concepto de brecha de dualidad puede usarse en algoritmos de optimización para proporcionar criterios de parada no heurísticos. Supongamos que un algoritmo produce una secuencia de puntos factibles primales \\(\\xx^{(k)}\\) y puntos factibles duales \\((\\bflambda^{(k)}, \\bfnu^{(k)})\\), para \\(k = 1, 2, \\ldots\\), y sea \\(\\epsilon_{\\text{abs}} &gt; 0\\) una precisión absoluta requerida. Entonces, el criterio de parada \\[\nf(\\xx^{(k)}) - g(\\bflambda^{(k)}, \\bfnu^{(k)}) \\leq \\epsilon_{\\text{abs}}\n\\]\ngarantiza que, cuando el algoritmo termina, \\(\\xx^{(k)}\\) es \\(\\epsilon_{\\text{abs}}\\)-subóptimo. Por supuesto, la dualidad fuerte debe cumplirse si se pretende que este método funcione para tolerancias \\(\\epsilon_{\\text{abs}}\\) arbitrariamente pequeñas."
  },
  {
    "objectID": "CAPITULO_1/A2_optimalidad.html#holgura-complementaria",
    "href": "CAPITULO_1/A2_optimalidad.html#holgura-complementaria",
    "title": "Condiciones de optimalidad en Optimización",
    "section": "3.4 Holgura complementaria",
    "text": "3.4 Holgura complementaria\nSupongamos que los valores óptimos primal y dual se alcanzan y son iguales. Esto es, se cumple la desigualdad fuerte \\(p^*=d^*\\). Esto significa que para puntos óptimos \\(\\xx^*\\) y \\((\\bflambda^*,\\bfnu^*)\\) se verifica \\[\n\\begin{align*}\nf(\\xx^{*}) &= \\calG(\\bflambda^{*}, \\bfnu^{*}) \\\\\n&= \\inf_{x} \\left( f(\\xx) + \\sum_{i=1}^{q} \\lambda_{i}^{*} g_{i}(x) + \\sum_{j=1}^{r} \\nu_{j}^{*} h_{j}(\\xx) \\right)\\\\\n& \\leq f(\\xx^{*}) + \\sum_{i=1}^{q} \\lambda_{i}^{*} g_{i}(\\xx^{*}) + \\sum_{j=1}^{r} \\nu_{j}^{*} h_{j}(\\xx^{*}) \\\\&\\leq f(\\xx^{*}).\n\\end{align*}\n\\]\nLa justificación de cada uno los pasos es sencilla [Ejercicio …]. Concluimos que las dos desigualdades en esta cadena se cumplen con igualdad. En particular, la última igualdad\n\\[\nf(\\xx^*)+ \\sum_{i=1}^{q} \\lambda_{i}^{*} g_{i}(\\xx^{*}) + \\sum_{j=1}^{r} \\nu_{j}^{*} h_{j}(\\xx^{*})=f(\\xx^*)\n\\]\nimplica que\n\\[\n\\sum_{i=1}^{q} \\lambda_{i}^{*} g_{i}(\\xx^{*}) = 0.\n\\]\nPero cada término en esta suma es no positivo, por lo tanto debe verificarse\n\\[\n\\boxed{\\lambda_{i}^{*} g_{i}(\\xx^{*}) = 0, \\quad \\forall i = 1, \\ldots, q.}\n\\]\nEsta es la condición de holgura complementaria que hemos visto en la sección anterior. Se cumple para cualquier punto óptimo primal \\(\\xx^{*}\\) y cualquier punto óptimo dual \\((\\bflambda^{*}, \\bfnu^{*})\\) bajo dualidad fuerte. Recordemos que, en términos generales, esta condición significa que el \\(i\\)-ésimo multiplicador de Lagrange óptimo es cero salvo que la \\(i\\)-ésima restricción esté activa."
  },
  {
    "objectID": "CAPITULO_1/A2_optimalidad.html#relación-entre-dualidad-y-kkt",
    "href": "CAPITULO_1/A2_optimalidad.html#relación-entre-dualidad-y-kkt",
    "title": "Condiciones de optimalidad en Optimización",
    "section": "3.5 Relación entre dualidad y KKT",
    "text": "3.5 Relación entre dualidad y KKT\nLos conceptos de Lagrangiano y su función dual han permitido caracterizar la relación entre los valores óptimos \\(p^*\\) y \\(d^*\\). En particular, las condiciones de factibilidad dual y de holgura complementaria de la Definición 1 (Condiciones de Karush-Kuhn-Tucker) han permitido primero escribir la desigualdad \\(\\calG(\\bflambda,\\bfmu)\\leq p^*\\) y luego analizar qué sucede cuando \\(p^*=q^*\\).\nVeamos ahora cómo surge la estacionariedad en este contexto. Podemos afirmar que\n\\[\nf(\\xx^*)\\leq f(\\xx)+\\sum_{i=1}^{q} \\lambda_{i}^{*} g_{i}(\\xx) + \\sum_{j=1}^{r} \\nu_{j}^{*} h_{j}(\\xx)\n\\]\npara todo punto factible \\(\\xx\\), con igualdad si \\(\\xx=\\xx^*\\). El lado derecho es el Lagrangiano \\(L(\\xx,\\bflambda^*,\\bfnu^*)\\), del cual podemos afirmar que \\(\\xx\\) es un minimizador. En consecuencia, debe verificarse\n\\[\n\\begin{align*}\n  \\nabla L(\\xx^*,\\bflambda^*,\\bfnu^*)&=0\\\\\n  \\nabla f(\\xx^*)+\\sum_{i=1}^q\\lambda_i^*\\nabla g_i(\\xx^*)+\\sum_{j=1}^r\\nu_j^*\\nabla h_j(\\xx^*)&=0.\n\\end{align*}\n\\]\n\nTeorema 4. (Dualidad y KKT) Sea \\(\\xx^*\\) un punto óptimo primal y \\((\\bflambda^*,\\bfnu^*)\\) un punto óptimo dual. Si \\(p^*=q^*\\), entonces \\((\\xx^*,\\bflambda^*,\\bfnu^*)\\) satisfacen las condiciones de Karush-Kuhn-Tucker.\n\n\n\n\nEjercicios\n\n\nConsidere el problema \\[\n\\min x^2\\quad\\text{sujeto a}\\quad 2-x\\leq 0.\n\\] Obtenga la función dual de Lagrange y verifique graficamente su convexidad. ¿Cuál es el valor de \\(d^*\\)?\nProbar que la función dual de Lagrange \\(\\calG(\\bflambda,\\bfmu)\\) es cóncava.\nMostrar que, si \\(\\xx\\) es factible primal y \\((\\bflambda,\\bfnu)\\) es factible dual, entonces \\((\\bflambda,\\bfnu)\\) es \\(\\epsilon\\)-subóptimo para el problema dual."
  },
  {
    "objectID": "CAPITULO_1/A1_intro_optimizacion.html",
    "href": "CAPITULO_1/A1_intro_optimizacion.html",
    "title": "Introducción a la Optimización",
    "section": "",
    "text": "\\[\n\\def\\RR{\\mathbb{R}}\n\\def\\media{\\mathbb{E}}\n\\def\\xx{{\\bf x}}\n\\def\\XX{{\\bf X}}\n\\def\\TT{{\\bf T}}\n\\def\\bfa{\\boldsymbol{a}}\n\\def\\bfb{\\boldsymbol{b}}\n\\def\\bftheta{\\boldsymbol{\\theta}}\n\\def\\bflambda{\\boldsymbol{\\lambda}}\n\\def\\bfeta{\\boldsymbol{\\eta}}\n\\def\\bfmu{\\boldsymbol{\\mu}}\n\\def\\bfnu{\\boldsymbol{\\nu}}\n\\def\\bfSigma{\\boldsymbol{\\Sigma}}\n\\def\\bfone{\\mathbf{1}}\n\\def\\argmin{\\mathop{\\mathrm{arg\\,min\\,}}}\n\\def\\argmax{\\mathop{\\mathrm{arg\\,max\\,}}}\n\\]\n\nLala"
  },
  {
    "objectID": "CAPITULO_2/01_reg_lineal_logistica.html",
    "href": "CAPITULO_2/01_reg_lineal_logistica.html",
    "title": "Regresión lineal y logística",
    "section": "",
    "text": "\\[\n\\def\\RR{\\mathbb{R}}\n\\def\\media{\\mathbb{E}}\n\\def\\xx{{\\bf x}}\n\\def\\XX{{\\bf X}}\n\\def\\TT{{\\bf T}}\n\\def\\bftheta{\\boldsymbol{\\theta}}\n\\def\\bfeta{\\boldsymbol{\\eta}}\n\\def\\bfone{\\mathbf{1}}\n\\]"
  },
  {
    "objectID": "CAPITULO_2/01_reg_lineal_logistica.html#interpretación-probabilística",
    "href": "CAPITULO_2/01_reg_lineal_logistica.html#interpretación-probabilística",
    "title": "Regresión lineal y logística",
    "section": "1.1 Interpretación probabilística",
    "text": "1.1 Interpretación probabilística\nEn el modelo de regresión lineal, las variables predictoras \\(\\XX\\in\\RR^p\\) se relacionan con la variable respuesta \\(Y\\in\\RR\\) mediante la ecuación\n\\[\nY = \\theta_0+\\theta_1 X_1+\\theta_2 X_2+\\cdots+\\theta_p X_p + \\varepsilon,\n\\]\ndonde \\(\\bftheta=(\\theta_0,\\theta_1,\\cdots,\\theta_p)\\in\\RR^{p+1}\\) es el conjunto de parámetros del modelo y \\(\\varepsilon\\) es un término de error que captura efectos no modelados (como características omitidas o ruido aleatorio). Podemos escribir más abreviado el modelo bajo la convención \\(x_0=1\\), como:\n\\[\nY = \\bftheta^T \\XX + \\varepsilon,\n\\]\nAdemás, se asume que el error aleatorio \\(\\varepsilon\\) se distribuye según una distribución Normal con media cero y varianza \\(\\sigma^2\\), esto es\n\\[\n\\varepsilon^{(i)} \\sim \\mathcal{N}(0, \\sigma^2).\n\\]\nDe esta manera, la densidad de \\(\\varepsilon\\) es\n\\[\np(\\varepsilon) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{\\varepsilon^2}{2 \\sigma^2}\\right),\n\\]\nEsto implica que \\(Y|\\XX\\sim \\mathcal{N}(\\bftheta^T\\XX,\\sigma^2)\\) o, lo que es lo mismo, que la función de densidad condicional está dada por\n\\[\np(y |\\xx; \\bftheta) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{(y-\\bftheta^T \\xx)^2}{2 \\sigma^2}\\right).\n\\]\n\n\n1.1.1 Función de verosimilitud\nPara un conjunto de datos \\(\\{\\xx_i, y_i\\}_{i=1}^n\\) i.i.d., la función de verosimilitud es\n\\[\nL(\\bftheta) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{(y_i - \\bftheta^T\\xx_i)^2}{2 \\sigma^2}\\right).\n\\]\nAhora debemos aplicar el principio de máxima verosimilitud, que consiste en elegir \\(\\bftheta\\) de forma que los datos sean lo más probables posible (es decir, maximizar \\(L(\\bftheta)\\)). Sin embargo, es más conveniente maximizar el logaritmo de la verosimilitud, lo cual se denota con \\(\\ell(\\bftheta)\\). Resulta en este caso\n\\[\n\\begin{align*}\n\\ell(\\bftheta) &= \\log L(\\bftheta) \\\\\n&= \\log \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2 \\pi}\\sigma} \\exp\\left(-\\frac{(y_i - \\bftheta^T \\xx_i)^2}{2 \\sigma^2}\\right) \\\\\n&= \\sum_{i=1}^{n} \\left[\\log\\frac{1}{\\sqrt{2 \\pi}\\sigma} - \\frac{1}{2 \\sigma^2}  (y_i - \\bftheta^T \\xx_i)^2\\right]\\\\\n&= n  \\log \\frac{1}{\\sqrt{2 \\pi}\\sigma} - \\frac{1}{2 \\sigma^2} \\sum_{i=1}^{n} (y_i - \\bftheta^T \\xx_i)^2\n\\end{align*}\n\\]\nObservar en esta última expresión que maximizar \\(\\ell(\\theta)\\) es equivalente a minimizar\n\\[\nJ(\\bftheta)=\\frac{1}{2} \\sum_{i=1}^{n} (y_i - \\bftheta^T \\xx_i)^2,\n\\]\nlo cual es la función de costo asociada a mínimos cuadrados. El paramétro \\(\\hat{\\bftheta}\\) que minimiza dicha función está dado en forma cerrada por\n\\[\\hat{\\bftheta}=(X^TX)^{-1}X^T\\vec{y},\\]\ndonde \\(X\\in\\RR^{n\\times p}\\) es la matriz de datos y \\(\\vec{y}\\) el vector de respuestas."
  },
  {
    "objectID": "CAPITULO_2/01_reg_lineal_logistica.html#algoritmo-lms",
    "href": "CAPITULO_2/01_reg_lineal_logistica.html#algoritmo-lms",
    "title": "Regresión lineal y logística",
    "section": "1.2 Algoritmo LMS",
    "text": "1.2 Algoritmo LMS\nUn algoritmo de búsqueda del minimizador de \\(J(\\bftheta)\\) comienza con un valor inicial para \\(\\bftheta\\) y, luego, realiza una actualización iterativa de \\(\\bftheta\\) que pretende hacer el valor de \\(J(\\bftheta)\\) cada vez más pequeño. Específicamente, consideremos el algoritmo de descenso de gradiente que utiliza la siguiente actualización:\n\\[\n\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\bftheta).\n\\]\n(Esta actualización se realiza simultáneamente para todos los valores de \\(j = 0, \\dots, p\\)). Aquí, \\(\\alpha\\) se llama tasa de aprendizaje. Este es un algoritmo natural que repetidamente da un paso en la dirección del descenso más pronunciado de \\(J\\).\nPara implementar este algoritmo, necesitamos calcular el término de la derivada parcial en el lado derecho. Primero trabajemos en el caso donde solo tenemos un ejemplo de entrenamiento \\((\\xx, y)\\), de manera que podamos omitir la suma en la definición de \\(J\\). Tenemos:\n\\[\n\\frac{\\partial}{\\partial \\theta_j} J(\\bftheta) = \\frac{\\partial}{\\partial \\theta_j}\\left[ \\frac{1}{2} (y-\\bftheta^T\\xx)^2\\right] = (y-\\bftheta^T\\xx) x_j.\n\\]\nEsto da la regla de actualización:\n\\[\n\\theta_j := \\theta_j + \\alpha (y-\\bftheta^T\\xx) x_j.\n\\]\nEsta regla se llama la regla de actualización LMS (least mean squares), y también se conoce como la regla de aprendizaje Widrow-Hoff.\nLa magnitud de la actualización es proporcional al término de error \\((y - \\bftheta^T\\xx)\\). Si encontramos un ejemplo de entrenamiento en el que nuestra predicción coincide casi con el valor real de \\(y\\), entonces el cambio en los parámetros será mínimo.\nPara un conjunto de entrenamiento \\(\\{\\xx_i,y_i\\}_{i=1}^n\\), es fácil ver que la regla se puede escribir vectorialmente como\n\\[\n\\bftheta := \\bftheta + \\alpha \\sum_{i=1}^{n} (y_i - \\bftheta^T\\xx_i) \\xx_i.\n\\]\nEste método analiza cada ejemplo en todo el conjunto de entrenamiento en cada paso y se denomina descenso de gradiente por lotes (batch gradient descent)."
  },
  {
    "objectID": "CAPITULO_2/01_reg_lineal_logistica.html#ejemplo",
    "href": "CAPITULO_2/01_reg_lineal_logistica.html#ejemplo",
    "title": "Regresión lineal y logística",
    "section": "1.3 Ejemplo",
    "text": "1.3 Ejemplo\nEl conjunto de datos California Housing incluye información detallada sobre diversas características socioeconómicas y geográficas de diferentes áreas residenciales en California. El objetivo es predecir el valor medio de las viviendas en cada zona, a partir de factores como la densidad poblacional y los ingresos medios de los hogares, entre otros.\n\n\n\n\n\n\nLa clase LinearRegression usa la ecuación normal \\(\\hat{\\bftheta}=(X^TX)^{-1}X^T\\vec{y}\\). Para usar descenso de gradiente, la alternativa es la clase SGDRegressor: esta efectúa descenso de gradiente estocástico, que consiste en calcular el gradiente utilizando un dato a la vez."
  },
  {
    "objectID": "CAPITULO_2/01_reg_lineal_logistica.html#función-logística",
    "href": "CAPITULO_2/01_reg_lineal_logistica.html#función-logística",
    "title": "Regresión lineal y logística",
    "section": "2.1 Función logística",
    "text": "2.1 Función logística\nEn vez de trabajar con \\(h(\\xx)=\\bftheta^T\\xx\\) (regresión lineal), utilizaremos la siguiente expresión:\n\\[\nh_\\bftheta(\\xx) = g(\\bftheta^T \\xx) = \\frac{1}{1 + e^{-\\bftheta^T \\xx}},\n\\]\ndonde\n\\[ g(z) = \\frac{1}{1 + e^{-z}} \\]\nse llama función logística o función sigmoide.\n\n2.1.1 Gráfico de la función logística\nLa siguiente es una representación de \\(g(z)\\). Observar que \\(g(z)\\) tiende hacia 1 cuando \\(z \\to \\infty\\), mientras que tiende hacia 0 cuando \\(z \\to -\\infty\\). Además, \\(g(z)\\) está acotada entre 0 y 1."
  },
  {
    "objectID": "CAPITULO_2/01_reg_lineal_logistica.html#estimación-de-parámetros",
    "href": "CAPITULO_2/01_reg_lineal_logistica.html#estimación-de-parámetros",
    "title": "Regresión lineal y logística",
    "section": "2.2 Estimación de parámetros",
    "text": "2.2 Estimación de parámetros\nEntonces, dado el modelo de regresión logística, ¿cómo ajustamos \\(\\bftheta\\)? Como antes, obtengamos el estimador de máxima verosimilitud a partir de un conjunto de supuestos probabilísticos. Supongamos que\n\\[\n\\begin{align*}\nP(y = 1 | \\xx; \\bftheta) &= h_\\theta(\\xx),\n\\\\\nP(y = 0 | \\xx; \\bftheta) &= 1 - h_\\theta(\\xx).\n\\end{align*}\n\\]\nEsto puede escribirse de manera más compacta como:\n\\[\np(y | \\xx; \\bftheta) = \\left( h_\\bftheta(\\xx) \\right)^y \\left( 1 - h_\\bftheta(\\xx) \\right)^{1-y}.\n\\]\n\n2.2.1 Función de verosimilitud\nPara un conjunto de datos \\(\\{\\xx_i, y_i\\}_{i=1}^n\\) i.i.d., la función de verosimilitud es\n\\[\n\\begin{align*}\nL(\\bftheta) = \\prod_{i=1}^n \\left( h_\\bftheta(\\xx_i) \\right)^{y_i} \\left( 1 - h_\\bftheta(\\xx_i) \\right)^{1-y_i}.\n\\end{align*}\n\\]\nComo antes, será más sencillo maximizar el logaritmo de la verosimilitud:\n\\[\n\\begin{align*}\n\\ell(\\bftheta) &= \\log L(\\bftheta) \\\\\n&= \\sum_{i=1}^n \\left[y_i \\log h_\\theta(\\xx_i) + (1 - y_i) \\log \\left( 1 - h_\\theta(\\xx_i) \\right)\\right].\n\\end{align*}\n\\]\n\n\n2.2.2 Regla de ascenso por gradiente\n¿Cómo maximizamos la verosimilitud? Similar a nuestra derivación en el caso de la regresión lineal, podemos usar ascenso por gradiente. Las actualizaciones estarán dadas por\n\\[\n\\bftheta := \\bftheta + \\alpha \\nabla_\\bftheta \\ell(\\bftheta).\n\\]\nComo antes, comencemos trabajando con un único ejemplo de entrenamiento \\((\\xx,y)\\). Resulta: \\[\n\\begin{align*}\n\\frac{\\partial}{\\partial \\theta_j} \\ell(\\bftheta) & = \\frac{\\partial}{\\partial\\theta_j}\\left[y\\log h_{\\bftheta}(\\xx)+(1-y)\\log\\left(1-h_{\\bftheta}(\\xx)\\right)\\right]\\\\\n&=\n\\left[ y \\frac{1}{g(\\bftheta^T \\xx)} - (1 - y) \\frac{1}{1 - g(\\bftheta^T \\xx)} \\right]\n\\frac{\\partial}{\\partial \\theta_j} g(\\bftheta^T \\xx)\n\\\\\n&=\n\\left[ y \\frac{1}{g(\\bftheta^T \\xx)} - (1 - y) \\frac{1}{1 - g(\\bftheta^T \\xx)} \\right]\ng'(\\bftheta^T\\xx)x_j\n\\\\\n&= \\left[ y \\frac{1}{g(\\bftheta^T \\xx)} - (1 - y) \\frac{1}{1 - g(\\bftheta^T \\xx)} \\right]\ng(\\bftheta^T \\xx) (1 - g(\\bftheta^T \\xx)) x_j\n\\\\\n&= \\left[ y (1 - g(\\bftheta^T \\xx)) - (1 - y) g(\\bftheta^T \\xx) \\right] x_j\n\\\\\n&= (y - h_\\bftheta(\\xx)) x_j.\n\\end{align*}\n\\]\nEn los cálculos anteriores hemos usado el hecho de que \\(g'(z) = g(z)(1 - g(z))\\). Esto nos da la regla de ascenso de gradiente estocástico:\n\\[\n\\theta_j := \\theta_j + \\alpha \\left( y_i - h_\\bftheta(\\xx_i) \\right) x_{ij},\n\\]\ndonde \\(x_{ij}\\) es el \\(j\\)-ésimo elemento de la observación \\(\\xx_i\\). Si comparamos esto con la regla de actualización de LMS, podemos notar una cierta similaridad entre los factores \\((y_i-\\bftheta^T\\xx_i)\\) y \\((y_i-h_\\bftheta(\\xx))\\). ¿Es esto coincidencia? Veremos este hecho cuando veamos los modelos lineales generalizados."
  },
  {
    "objectID": "CAPITULO_2/01_reg_lineal_logistica.html#ejemplo-1",
    "href": "CAPITULO_2/01_reg_lineal_logistica.html#ejemplo-1",
    "title": "Regresión lineal y logística",
    "section": "2.3 Ejemplo",
    "text": "2.3 Ejemplo\nEl conjunto de datos Breast Cancer contiene información sobre 569 muestras de tejido mamario, con 30 características que describen propiedades de las células nucleares de los tumores. El objetivo principal es clasificar los tumores como benignos o malignos.\n\n\n\n\n\n\n\n\nEjercicios\n\n\nProbar que la derivada de la función logística \\(g(z)=1/(1+e^{-z})\\) verifica \\[g'(z)(1-g(z)).\\]\nMostrar que en los GLMs la derivada de la log-likelihood es \\[\n\\nabla_\\theta \\ell(\\theta) = \\sum_{i=1}^n (T(y^{(i)}) - \\mathbb{E}[T(y) | \\xx^{(i)}; \\theta]) \\xx^{(i)}.\n\\]"
  },
  {
    "objectID": "CAPITULO_2/03_optimizacion_ML.html",
    "href": "CAPITULO_2/03_optimizacion_ML.html",
    "title": "Optimización en regresión y clasificación",
    "section": "",
    "text": "\\[\n\\def\\RR{\\mathbb{R}}\n\\def\\media{\\mathbb{E}}\n\\def\\xx{{\\bf x}}\n\\def\\XX{{\\bf X}}\n\\def\\TT{{\\bf T}}\n\\def\\bftheta{\\boldsymbol{\\theta}}\n\\def\\bfeta{\\boldsymbol{\\eta}}\n\\def\\bfone{\\mathbf{1}}\n\\]\n\n\n\n1 First"
  },
  {
    "objectID": "CAPITULO_2/05_clustering.html",
    "href": "CAPITULO_2/05_clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "\\[\n\\def\\RR{\\mathbb{R}}\n\\def\\media{\\mathbb{E}}\n\\def\\xx{{\\bf x}}\n\\def\\XX{{\\bf X}}\n\\def\\TT{{\\bf T}}\n\\def\\bftheta{\\boldsymbol{\\theta}}\n\\def\\bfeta{\\boldsymbol{\\eta}}\n\\def\\bfone{\\mathbf{1}}\n\\]\n\n\n\n1 First"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Página de Inicio",
    "section": "",
    "text": "Bienvenidos al curso de Optimización. Aquí encontrarás los capítulos y secciones.\n\n\n\nSección 1: A1_intro_optimizacion\nSección 2: A2_optimalidad"
  },
  {
    "objectID": "index.html#capítulo-1",
    "href": "index.html#capítulo-1",
    "title": "Página de Inicio",
    "section": "",
    "text": "Sección 1: A1_intro_optimizacion\nSección 2: A2_optimalidad"
  }
]