[
  {
    "objectID": "macros.html",
    "href": "macros.html",
    "title": "Mi Sitio en Quarto",
    "section": "",
    "text": "\\[\n\\def\\RR{\\mathbb{R}}\n\\def\\media{\\mathbb{E}}\n\\def\\calL{\\mathcal{L}}\n\\def\\calG{\\mathcal{G}}\n\\def\\bb{{\\bf b}}\n\\def\\xx{{\\bf x}}\n\\def\\yy{{\\bf y}}\n\\def\\vv{{\\bf v}}\n\\def\\XX{{\\bf X}}\n\\def\\TT{{\\bf T}}\n\\def\\SS{{\\bf S}}\n\\def\\bfa{\\boldsymbol{a}}\n\\def\\bfb{\\boldsymbol{b}}\n\\def\\bftheta{\\boldsymbol{\\theta}}\n\\def\\bflambda{\\boldsymbol{\\lambda}}\n\\def\\bfeta{\\boldsymbol{\\eta}}\n\\def\\bfmu{\\boldsymbol{\\mu}}\n\\def\\bfnu{\\boldsymbol{\\nu}}\n\\def\\bfSigma{\\boldsymbol{\\Sigma}}\n\\def\\bfone{\\mathbf{1}}\n\\def\\argmin{\\mathop{\\mathrm{arg\\,min\\,}}}\n\\def\\argmax{\\mathop{\\mathrm{arg\\,max\\,}}}\n\\]"
  },
  {
    "objectID": "CAPITULO_2/06_EM.html",
    "href": "CAPITULO_2/06_EM.html",
    "title": "Algoritmo EM",
    "section": "",
    "text": "\\[\n\\def\\RR{\\mathbb{R}}\n\\def\\media{\\mathbb{E}}\n\\def\\xx{{\\bf x}}\n\\def\\XX{{\\bf X}}\n\\def\\TT{{\\bf T}}\n\\def\\bftheta{\\boldsymbol{\\theta}}\n\\def\\bfeta{\\boldsymbol{\\eta}}\n\\def\\bfone{\\mathbf{1}}\n\\]\n\n\n\n1 First"
  },
  {
    "objectID": "CAPITULO_2/04_modelos_generativos.html",
    "href": "CAPITULO_2/04_modelos_generativos.html",
    "title": "Modelos generativos",
    "section": "",
    "text": "\\[\n\\def\\RR{\\mathbb{R}}\n\\def\\media{\\mathbb{E}}\n\\def\\xx{{\\bf x}}\n\\def\\XX{{\\bf X}}\n\\def\\TT{{\\bf T}}\n\\def\\bftheta{\\boldsymbol{\\theta}}\n\\def\\bfeta{\\boldsymbol{\\eta}}\n\\def\\bfmu{\\boldsymbol{\\mu}}\n\\def\\bfSigma{\\boldsymbol{\\Sigma}}\n\\def\\bfone{\\mathbf{1}}\n\\def\\argmin{\\mathop{\\mathrm{arg\\,min\\,}}}\n\\def\\argmax{\\mathop{\\mathrm{arg\\,max\\,}}}\n\\]\nHasta ahora, hemos hablado principalmente sobre algoritmos de aprendizaje que modelan \\(p(y|\\xx; \\bftheta)\\); esto es, la distribución condicional de \\(y\\) dado \\(\\xx\\) parametrizada por \\(\\bftheta\\). El objetivo es aprender un clasificador que asocie a una entrada \\(\\xx\\) una etiqueta. Nos referiremos a estos algoritmos como aprendizaje discriminativo.\nEn esta sección, hablaremos sobre algoritmos de aprendizaje generativo, en el cual se pone foco en \\(p(\\xx|y)\\). Para entender la idea, consideremos un problema de clasificación en el que queremos aprender a distinguir entre elefantes (\\(y=1\\)) y perros (\\(y=0\\)), basado en algunas características del animal. Una posibilidad es construir un modelo de cómo son los elefantes, \\(p(\\xx|y=1)\\), y un modelo separado para los perros, \\(p(\\xx|y=0)\\), de tal manera que al final del día clasificar un nuevo animal dependa de si se parece más a los elefantes o más a los perros del conjunto de entrenamiento.\nDespués de modelar \\(p(\\xx|y)\\) y también la distribución a priori \\(p(y)\\), la regla de Bayes permite obtener la distribución a posteriori:\n\\[\np(y|\\xx) = \\frac{p(\\xx|y)p(y)}{p(\\xx)}.\n\\]\nObservar que el denominador se obtiene mediante la regla \\[p(\\xx) = p(\\xx|y=1)p(y=1) + p(\\xx|y=0)p(y=0).\\]\nSin embargo, al momento de hacer una predicción el denominador no es necesario ya que \\[\n\\argmax_y p(y|\\xx) = \\argmax_y \\frac{p(\\xx|y)p(y)}{p(\\xx)} = \\argmax_y p(\\xx|y)p(y).\n\\]"
  },
  {
    "objectID": "CAPITULO_2/04_modelos_generativos.html#análisis-discriminante-lineal-lda",
    "href": "CAPITULO_2/04_modelos_generativos.html#análisis-discriminante-lineal-lda",
    "title": "Modelos generativos",
    "section": "1.1 Análisis discriminante lineal (LDA)",
    "text": "1.1 Análisis discriminante lineal (LDA)\nEl modelo es:\n\\[\n\\begin{align*}\ny &\\sim \\text{Bernoulli}(\\phi) \\\\\n\\xx|y=0 &\\sim \\mathcal{N}(\\bfmu_0, \\bfSigma) \\\\\nx|y=1 &\\sim \\mathcal{N}(\\bfmu_1, \\bfSigma)\n\\end{align*}\n\\]\nLos parámetros del modelo son \\(\\phi\\), \\(\\bfmu_0\\), \\(\\bfmu_1\\) y \\(\\bfSigma\\).\n\n\n\n\n\n\n\n\n\nEn la figura se muestran datos de entrenamiento correspondientes a dos clases y las curvas de nivel de las gaussianas que generan ambas clases. Se puede observar que ambas gaussianas tienen la misma forma y orientación, ya que comparten una matriz de covarianza \\(\\bfSigma\\) en común. También se muestra en la figura la línea recta que representa la frontera de decisión al momento de clasificar un nuevo dato. Dicha frontera corresponde a la ecuación \\(p(y=1|\\xx) = 0.5\\).\n\n1.1.1 Función de verosimilitud\nPara un conjunto de datos \\(\\{\\xx^{(i)}, y^{(i)}\\}_{i=1}^n\\) i.i.d., la función de log-verosimilitud es\n\\[\n\\begin{align*}\n\\ell(\\phi, \\bfmu_0, \\bfmu_1, \\bfSigma) &= \\log \\prod_{i=1}^n p(\\xx^{(i)}, y^{(i)}; \\phi, \\bfmu_0, \\bfmu_1, \\bfSigma)\\\\\n&= \\log \\prod_{i=1}^n p(\\xx^{(i)}| y^{(i)}; \\phi, \\bfmu_0, \\bfmu_1, \\bfSigma) p(y^{(i)}; \\phi)\\\\\n\\end{align*}\n\\]\nMaximizando \\(\\ell\\) con respecto a los parámetros, encontramos las estimaciones de máxima verosimilitud:\n\\[\n\\begin{align*}\n\\hat{\\phi} &= \\frac{1}{n} \\sum_{i=1}^n \\bfone\\{y^{(i)} = 1\\} \\\\\n\\hat{\\bfmu}_0 &= \\frac{\\sum_{i=1}^n \\bfone\\{y^{(i)} = 0\\} \\xx^{(i)}}{\\sum_{i=1}^n \\bfone\\{y^{(i)} = 0\\}} \\\\\n\\hat{\\bfmu}_1 &= \\frac{\\sum_{i=1}^n \\bfone\\{y^{(i)} = 1\\} \\xx^{(i)}}{\\sum_{i=1}^n \\bfone\\{y^{(i)} = 1\\}} \\\\\n\\hat{\\bfSigma} &= \\frac{1}{n} \\sum_{i=1}^n (\\xx^{(i)} - \\bfmu_{y^{(i)}})(\\xx^{(i)} - \\bfmu_{y^{(i)}})^T\n\\end{align*}\n\\]\n\n\n1.1.2 Clasificador final\nUna observación \\(\\xx\\) se etiquetará con \\(y=0\\) si\n\\[\n\\begin{align*}\n\\log\\left(\\frac{p(y=0|\\xx)}{p(y=1|\\xx)}\\right)&&gt;0\\\\\n\\log\\left(\\frac{p(\\xx|y=0)p(y=0)}{p(\\xx|y=1)p(y=1)}\\right)&&gt;0\\\\\n\\log\\left(\\frac{(1-\\hat{\\phi})\\,\\mathcal{N}(\\xx|\\hat{\\bfmu}_0,\\hat{\\bfSigma})}{\\hat{\\phi}\\,\\mathcal{N}(\\xx|\\hat{\\bfmu}_1,\\hat{\\bfSigma})}\\right)&&gt;0\n\\end{align*}.\n\\]\nSi se desarrolla el argumento del logaritmo en la expresión anterior, es decir el cociente de verosimilitudes, se puede deducir que la frontera de decisión es efectivamente una función lineal. ¿Ejercicio?"
  },
  {
    "objectID": "CAPITULO_2/04_modelos_generativos.html#análisis-discriminante-cuadrático-qda",
    "href": "CAPITULO_2/04_modelos_generativos.html#análisis-discriminante-cuadrático-qda",
    "title": "Modelos generativos",
    "section": "1.2 Análisis discriminante cuadrático (QDA)",
    "text": "1.2 Análisis discriminante cuadrático (QDA)\nEl modelo es:\n\\[\n\\begin{align*}\ny &\\sim \\text{Bernoulli}(\\phi) \\\\\n\\xx|y=0 &\\sim \\mathcal{N}(\\bfmu_0, \\bfSigma_0) \\\\\nx|y=1 &\\sim \\mathcal{N}(\\bfmu_1, \\bfSigma_1)\n\\end{align*}\n\\]\nLos parámetros del modelo son \\(\\phi\\), \\(\\bfmu_0\\), \\(\\bfmu_1\\), \\(\\bfSigma_0\\) y \\(\\bfSigma_1\\).\n\n\n\n\n\n\n\n\n\nEn la figura se observa como en este caso, a diferencia de LDA, las gaussianas tienen diferente forma y orientación debido a que se asume que las matrices de covarianza no son iguales. Además, la frontera de decisión no es lineal.\n\n1.2.1 Función de verosimilitud\nPara un conjunto de datos \\(\\{\\xx^{(i)}, y^{(i)}\\}_{i=1}^n\\) i.i.d., la función de log-verosimilitud es\n\\[\n\\ell(\\phi, \\bfmu_0, \\bfmu_1, \\bfSigma_0,\\bfSigma_1) = \\log \\prod_{i=1}^n p(\\xx^{(i)}| y^{(i)}; \\phi, \\bfmu_0, \\bfmu_1, \\bfSigma_1,\\bfSigma_2) p(y^{(i)}; \\phi).\n\\]\nLos estimadores de máxima verosimilitud de \\(\\phi\\), \\(\\bfmu_0\\) y \\(\\bfmu_1\\) son idénticos al que se obtiene en LDA. Para las matrices de covarianza, resulta\n\\[\n\\hat{\\bfSigma}_0 = \\frac{\\sum_{i=1}^n \\bfone\\{y^{(i)}=0\\}(\\xx^{(i)} - \\hat{\\bfmu}_0)(\\xx^{(i)} - \\hat{\\bfmu}_0)^T}{\\sum_{i=1}^n\\bfone\\{y^{(i)}=0\\}}\n\\]\n\\[\n\\hat{\\bfSigma}_1 = \\frac{\\sum_{i=1}^n \\bfone\\{y^{(i)}=1\\}(\\xx^{(i)} - \\hat{\\bfmu}_1)(\\xx^{(i)} - \\hat{\\bfmu}_1)^T}{\\sum_{i=1}^n\\bfone\\{y^{(i)}=1\\}}\n\\]\n\n\n1.2.2 Clasificador final\nUna observación \\(\\xx\\) se etiquetará con \\(y=0\\) si\n\\[\\log\\left(\\frac{(1-\\hat{\\phi})\\,\\mathcal{N}(\\xx|\\hat{\\bfmu}_0,\\hat{\\bfSigma}_0)}{\\hat{\\phi}\\,\\mathcal{N}(\\xx|\\hat{\\bfmu}_1,\\hat{\\bfSigma}_1)}\\right)&gt;0.\\]\nEn este caso, el desarrollo del cociente de verosimilitudes permite deducir que la frontera de decisión es una función cuadrática. ¿Ejercicio?"
  },
  {
    "objectID": "CAPITULO_2/04_modelos_generativos.html#tipos-de-clasificadores-naive-bayes",
    "href": "CAPITULO_2/04_modelos_generativos.html#tipos-de-clasificadores-naive-bayes",
    "title": "Modelos generativos",
    "section": "2.1 Tipos de clasificadores Naive Bayes",
    "text": "2.1 Tipos de clasificadores Naive Bayes\nNo existe solo un tipo de clasificador Naïve Bayes. Los tipos más comunes se diferencian según las distribuciones de los valores de las características. Algunos de estos son:\n\nNaive Bayes Gaussiano: Este es un tipo de clasificador Naïve Bayes que se utiliza con distribuciones gaussianas, es decir, distribuciones normales y variables continuas. Este modelo se ajusta encontrando la media y la desviación estándar de cada clase.\nNaive Bayes Multinomial: Este tipo de clasificador Naïve Bayes asume que las características provienen de distribuciones multinomiales. Esta variante es útil cuando se trabaja con datos discretos, como conteos de frecuencia, y se aplica típicamente en casos de uso de procesamiento de lenguaje natural, como la clasificación de spam.\nNaïve Bayes Bernoulli: Esta es otra variante del clasificador Naïve Bayes, que se utiliza con variables booleanas, es decir, variables con dos valores, como Verdadero y Falso o 1 y 0.\n\n\n2.1.1 Naive Bayes Bernoulli\nLas suposiciones son:\n\\[y\\sim\\text{Bernoulli}(\\phi),\\] \\[x_j|y=0\\sim\\text{Bernoulli}(\\phi_{j|y=0}),\\] \\[x_j|y=1\\sim\\text{Bernoulli}(\\phi_{j|y=1}),\\]\nLos parámetros del modelo son \\[\\phi=p(y=1),\\quad \\phi_{j|y=0}=p(x_j=1|y=0),\\quad \\phi_{j|y=1}=p(x_j=1|y=1).\\]\n\n2.1.1.1 Función de verosimilitud\nPara un conjunto de datos \\(\\{\\xx^{(i)}, y^{(i)}\\}_{i=1}^n\\) i.i.d., la función de verosimilitud es \\[\n\\mathcal{L}(\\phi, \\phi_{j|y=0}, \\phi_{j|y=1}) = \\prod_{i=1}^n p(\\xx^{(i)}, y^{(i)})\n\\]\nLos estimadores de máxima verosimilitud resultan:\n\\[\\phi = \\frac{\\sum_{i=1}^n \\bfone\\{y^{(i)} = 1\\}}{n},\\] \\[\\phi_{j|y=1} = \\frac{\\sum_{i=1}^n \\bfone\\{\\xx_j^{(i)} = 1 \\land y^{(i)} = 1\\}}{\\sum_{i=1}^n \\bfone\\{y^{(i)} = 1\\}},\\] \\[\\phi_{j|y=0} = \\frac{\\sum_{i=1}^n \\bfone\\{\\xx_j^{(i)} = 1 \\land y^{(i)} = 0\\}}{\\sum_{i=1}^n \\bfone\\{y^{(i)} = 0\\}}.\\]\nA pesar de que pueda parecer engorroso a simple vista, estas estimaciones tienen una interpretación muy natural. Por ejemplo, \\(\\phi_{j|y=1}\\) es simplemente la fracción de correos etiquetados como spam (\\(y=1\\)) en los que aparece la palabra \\(j\\).\n\n\nEjercicios"
  },
  {
    "objectID": "CAPITULO_2/02_GLM.html",
    "href": "CAPITULO_2/02_GLM.html",
    "title": "Modelos lineales generalizados",
    "section": "",
    "text": "\\[\n\\def\\RR{\\mathbb{R}}\n\\def\\media{\\mathbb{E}}\n\\def\\xx{{\\bf x}}\n\\def\\XX{{\\bf X}}\n\\def\\TT{{\\bf T}}\n\\def\\bftheta{\\boldsymbol{\\theta}}\n\\def\\bfeta{\\boldsymbol{\\eta}}\n\\def\\bfone{\\mathbf{1}}\n\\]\nTanto en el modelo de regresión lineal como en el de regresión logística, asumimos una cierta distribución condicional para \\(Y|\\XX\\) que depende de un conjunto de parámetros \\(\\bftheta\\).\nEn esta sección mostraremos que ambos métodos son casos especiales de una familia más amplia de modelos, llamados Modelos Lineales Generalizados (GLMs). También mostraremos cómo otros modelos en GLM pueden derivarse y aplicarse a otros problemas de regresión y clasificación."
  },
  {
    "objectID": "CAPITULO_2/02_GLM.html#ejemplos",
    "href": "CAPITULO_2/02_GLM.html#ejemplos",
    "title": "Modelos lineales generalizados",
    "section": "1.1 Ejemplos",
    "text": "1.1 Ejemplos\nMostraremos que las distribuciones Bernoulli y Normal son ejemplos de distribuciones de la familia exponencial.\n\n1.1.1 Distribución Bernoulli\nSea \\(Y\\in\\{0,1\\}\\) tal que \\(Y\\sim\\text{Bernoulli}(\\phi)\\). Entonces\n\\[\n\\begin{align*}\np(y; \\phi) &= \\phi^y (1 - \\phi)^{1-y} \\\\\n& = \\exp\\left( y \\log\\phi + (1-y)\\log(1-\\phi) \\right) \\\\\n& = \\exp\\left(\\log\\left(\\frac{\\phi}{1-\\phi}\\right)y+\\log(1-\\phi)\\right) \\\\\n& = \\exp\\left(\\log\\left(\\frac{\\phi}{1-\\phi}\\right)y+\\log(1-\\phi)\\right)\n\\end{align*}\n\\]\nReconocemos el parámetro natural \\[\\eta=\\log\\left(\\frac{\\phi}{1-\\phi}\\right)\\]\nmientras que el resto de los elementos son: \\[T(y)=y, \\qquad a(\\eta)=-\\log(1-\\phi)=\\log(1+e^\\eta), \\qquad b(y)=1.\\]\n\n\n1.1.2 Distribución Gaussiana\nSea \\(Y\\in\\RR\\) tal que \\(Y\\sim\\mathcal{N}(\\mu,\\sigma^2)\\). Entonces\n\\[\n\\begin{align*}\np(y; \\mu, \\sigma^2) & = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(y - \\mu)^2}{2\\sigma^2}\\right) \\\\\n& = \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2\\sigma^2}y^2+\\frac{\\mu}{\\sigma^2}y-\\frac{\\mu^2}{2\\sigma^2}-\\log\\sigma\\right) \\\\\n& = \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(\\left(\\frac{\\mu}{\\sigma^2},-\\frac{1}{2\\sigma^2}\\right)^T(y,y^2)-\\frac{\\mu^2}{2\\sigma^2}-\\log\\sigma\\right)\n\\end{align*}\n\\]\nEl parámetro natural es el vector\n\\[\\bfeta=\\left(\\frac{\\mu}{\\sigma^2},-\\frac{1}{2\\sigma^2}\\right)\\]\ny el resto de los elementos son \\[\\TT(y)=(y,y^2), \\qquad a(\\bfeta)=\\frac{\\mu^2}{2\\sigma^2}+\\log\\sigma=-\\frac{\\eta_1^2}{4\\eta_2}+\\frac{1}{2}\\log(-2\\eta_2), \\qquad b(y)=\\frac{1}{\\sqrt{2\\pi}}.\\]\n\nExisten muchas otras distribuciones que son miembros de la familia exponencial: la multinomial (que veremos más adelante), la Poisson (para modelar datos de conteo), la gamma y la exponencial (para modelar variables continuas no negativas, como intervalos de tiempo); la beta y la Dirichlet (para distribuciones sobre probabilidades); y muchas más.\nEn la próxima sección, describiremos una “receta” general para construir modelos en los cuales \\(y\\) (dado \\(\\xx\\) y \\(\\theta\\)) proviene de cualquiera de estas distribuciones."
  },
  {
    "objectID": "CAPITULO_2/02_GLM.html#terminología",
    "href": "CAPITULO_2/02_GLM.html#terminología",
    "title": "Modelos lineales generalizados",
    "section": "2.1 Terminología",
    "text": "2.1 Terminología\n\nLa función \\(g(\\bfeta) = \\mathbb{E}[T(y); \\bfeta]\\) se denomina función de respuesta canónica.\nLa inversa de esta función, \\(g^{-1}\\), se llama la función de enlace canónica."
  },
  {
    "objectID": "CAPITULO_2/02_GLM.html#el-principio-de-máxima-verosimilitud",
    "href": "CAPITULO_2/02_GLM.html#el-principio-de-máxima-verosimilitud",
    "title": "Modelos lineales generalizados",
    "section": "3.1 El principio de máxima verosimilitud",
    "text": "3.1 El principio de máxima verosimilitud\nEl ajuste de parámetros en los GLMs se basa en maximizar la log-verosimilitud de los datos observados. Dado un conjunto de entrenamiento \\(\\{(\\xx^{(i)}, y^{(i)})\\}_{i=1}^n\\), la función de log-verosimilitud está dada por:\n\\[\n\\ell(\\theta) = \\log L(\\theta) = \\sum_{i=1}^n \\log p(y^{(i)} | \\xx^{(i)}; \\theta).\n\\]\nPara maximizar \\(\\ell(\\theta)\\), derivamos con respecto a \\(\\theta\\) para obtener el gradiente. En GLMs, las propiedades de la familia exponencial simplifican esta derivada, que resulta ser:\n\\[\n\\nabla_\\theta \\ell(\\theta) = \\sum_{i=1}^n (T(y^{(i)}) - \\mathbb{E}[T(y) | \\xx^{(i)}; \\theta]) \\xx^{(i)}.\n\\]\nA continuación describiremos dos métodos iterativos para realizar la tarea de maximizar la log-verosimilitud.\n\n3.1.1 El método de gradiente descendente\nEste método optimiza \\(\\ell(\\bftheta)\\) iterativamente ajustando los parámetros en la dirección opuesta al gradiente de la función objetivo, con un paso de tamaño controlado por una tasa de aprendizaje \\(\\alpha\\):\n\\[\n\\bftheta^{(t+1)} = \\bftheta^{(t)} - \\alpha \\nabla_\\theta \\ell(\\bftheta^{(t)}).\n\\]\nEl gradiente descendente es computacionalmente barato, pero su convergencia puede ser lenta, especialmente si no se ajusta adecuadamente la tasa de aprendizaje.\n\n\n3.1.2 El método de Newton-Raphson\nEste método requiere calcular la Hessiana de \\(\\ell(\\theta)\\), que es la matriz de segundas derivadas\n\\[\nH(\\bftheta) = \\sum_{i=1}^n \\left(-\\text{Var}[T(y) | \\xx^{(i)}; \\bftheta]\\right) \\xx^{(i)} (\\xx^{(i)})^T,\n\\]\nEl método de Newton-Raphson actualiza \\(\\bftheta\\) como:\n\\[\n\\bftheta^{(t+1)} = \\bftheta^{(t)} - H(\\bftheta^{(t)})^{-1} \\nabla_\\theta \\ell(\\bftheta^{(t)}).\n\\]\nAunque este método converge rápidamente en muchas aplicaciones, calcular la Hessiana e invertirla puede ser computacionalmente costoso para modelos de alta dimensionalidad."
  },
  {
    "objectID": "CAPITULO_2/02_GLM.html#la-distribución-multinomial-como-familia-exponencial",
    "href": "CAPITULO_2/02_GLM.html#la-distribución-multinomial-como-familia-exponencial",
    "title": "Modelos lineales generalizados",
    "section": "4.1 La distribución multinomial como familia exponencial",
    "text": "4.1 La distribución multinomial como familia exponencial\nSupongamos que \\(Y\\in\\{1, 2, \\cdots, k\\}\\) se distribuye mediante una distribución multinomial con \\(k-1\\) parámetros \\(\\phi_1, \\dots, \\phi_{k-1}\\), y sea \\(\\phi_k=1-\\sum_{i=1}^{k-1}\\phi_i\\), tal que \\(\\sum_{i=1}^k \\phi_i = 1\\). Para expresar la multinomial como una distribución de la familia exponencial, definimos\n\\[\nT(1) = \\begin{pmatrix} 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix},\n\\quad\nT(2) = \\begin{pmatrix} 0 \\\\ 1 \\\\ \\vdots \\\\ 0 \\end{pmatrix},\n\\quad \\dots, \\quad\nT(k-1) = \\begin{pmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 1 \\end{pmatrix},\n\\quad\nT(k) = \\begin{pmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix}.\n\\]\nEn lo que sigue, \\(\\bfone\\{\\cdot\\}\\) es la función indicatriz, que vale 1 si su argumento es cierto y 0 en caso contrario. Resulta\n\\[\n\\begin{align*}\np(y; \\mathbf{\\phi})& =\n\\phi_1^{\\bfone\\{y=1\\}} \\phi_2^{\\bfone\\{y=2\\}} \\cdots \\phi_k^{\\bfone\\{y=k\\}}\\\\\n&= \\phi_1^{(T(y))_1} \\phi_2^{(T(y))_2} \\cdots \\phi_{k-1}^{(T(y))_{k-1}} \\left(1 - \\sum_{i=1}^{k-1} \\phi_i\\right)^{1-\\sum_{i=1}^{k-1}(T(y))_i}\n\\\\\n&= \\exp\\left((T(y))_1 \\log(\\phi_1) + (T(y))_2 \\log(\\phi_2) + \\dots + \\left(1 - \\sum_{i=1}^{k-1}(T(y))_i\\right)\\log\\left(1 - \\sum_{i=1}^{k-1} \\phi_i\\right)\\right).\n\\end{align*}\n\\]\nAsí, \\(Y\\) se distribuye en la familia exponencial\n\\[\np(y; \\phi) = b(y) \\exp\\left(\\eta^T T(y) - a(\\eta)\\right),\n\\]\ndonde:\n\\[\n\\begin{align*}\n\\eta &= \\begin{bmatrix}\n\\log\\left(\\frac{\\phi_1}{\\phi_k}\\right) \\\\\n\\log\\left(\\frac{\\phi_2}{\\phi_k}\\right) \\\\\n\\vdots \\\\\n\\log\\left(\\frac{\\phi_{k-1}}{\\phi_k}\\right)\n\\end{bmatrix},\n\\\\\na(\\eta) &= -\\log(\\phi_k),\n\\\\\nb(y)& = 1.\n\\end{align*}\n\\]\nEsto completa nuestra formulación de la multinomial como una distribución de la familia exponencial.\nLa función que mapea () a () se llama la función softmax:"
  },
  {
    "objectID": "CAPITULO_2/02_GLM.html#función-de-enlace-y-función-de-respuesta",
    "href": "CAPITULO_2/02_GLM.html#función-de-enlace-y-función-de-respuesta",
    "title": "Modelos lineales generalizados",
    "section": "4.2 Función de Enlace y Función de Respuesta",
    "text": "4.2 Función de Enlace y Función de Respuesta\nLa función de enlace está dada (para \\(i = 1, \\dots, k\\)) por:\n\\[\n\\eta_i = \\log\\left(\\frac{\\phi_i}{\\phi_k}\\right).\n\\]\nPara conveniencia, también definimos \\(\\eta_k = \\log(\\phi_k / \\phi_k) = 0\\). Para invertir la función de enlace y derivar la función de respuesta, tenemos que:\n\\[\ne^{\\eta_i} = \\frac{\\phi_i}{\\phi_k},\n\\]\nlo que implica:\n\\[\n\\phi_k \\sum_{i=1}^k e^{\\eta_i} = \\sum_{i=1}^k \\phi_i = 1.\n\\]\nPor lo tanto:\n\\[\n\\phi_k = \\frac{1}{\\sum_{i=1}^k e^{\\eta_i}},\n\\]\ny al sustituir en la ecuación obtenemos la función de respuesta:\n\\[\n\\phi_i = \\frac{e^{\\eta_i}}{\\sum_{j=1}^k e^{\\eta_j}}.\n\\]"
  },
  {
    "objectID": "CAPITULO_2/02_GLM.html#softmax-function-y-modelo-de-clasificación",
    "href": "CAPITULO_2/02_GLM.html#softmax-function-y-modelo-de-clasificación",
    "title": "Modelos lineales generalizados",
    "section": "4.3 Softmax Function y Modelo de Clasificación",
    "text": "4.3 Softmax Function y Modelo de Clasificación\nLa función que mapea de \\(\\eta\\) a \\(\\phi\\) se llama la función .\nPara completar nuestro modelo, usamos la Asunción 3, mencionada previamente, donde los \\(\\eta_i\\) están relacionados linealmente con los \\(x\\)’s. Así, tenemos \\(\\eta_i = \\theta_i^T x\\) (para \\(i = 1, \\dots, k - 1\\)), donde \\(\\theta_1, \\dots, \\theta_{k-1} \\in \\mathbb{R}^{d+1}\\) son los parámetros de nuestro modelo. Por notación, podemos definir \\(\\theta_k = 0\\), de modo que \\(\\eta_k = \\theta_k^T x = 0\\), como se definió anteriormente. Por lo tanto, nuestro modelo asume que la distribución condicional de \\(y\\) dado \\(x\\) está dada por:\n\\[\n\\begin{align*}\np(y = i | x; \\theta) &= \\phi_i\\\\\n&= \\frac{e^{\\eta_i}}{\\sum_{j=1}^k e^{\\eta_j}}\\\\\n&= \\frac{e^{\\theta_i^T x}}{\\sum_{j=1}^k e^{\\theta_j^T x}}, \\quad (5)\n\\end{align*}\n\\]\ndonde este modelo, que aplica a problemas de clasificación donde \\(y \\in \\{1, \\dots, k\\}\\), se llama . Es una generalización de la regresión logística.\n\n4.3.1 Hipótesis del Modelo\nNuestra hipótesis producirá:\n\\[\n\\begin{align*}\nh_\\theta(x) &= \\mathbb{E}[T(y) | x; \\theta]\n\\\\\n&= \\mathbb{E}\n\\begin{bmatrix}\n1\\{y = 1\\} \\\\\n1\\{y = 2\\} \\\\\n\\vdots \\\\\n1\\{y = k - 1\\}\n\\end{bmatrix}\n\\\\&=\n\\begin{bmatrix}\n\\phi_1 \\\\\n\\phi_2 \\\\\n\\vdots \\\\\n\\phi_{k-1}\n\\end{bmatrix}\n\\\\&=\n\\begin{bmatrix}\n\\frac{\\exp(\\theta_1^T x)}{\\sum_{j=1}^k \\exp(\\theta_j^T x)} \\\\\n\\frac{\\exp(\\theta_2^T x)}{\\sum_{j=1}^k \\exp(\\theta_j^T x)} \\\\\n\\vdots \\\\\n\\frac{\\exp(\\theta_{k-1}^T x)}{\\sum_{j=1}^k \\exp(\\theta_j^T x)}\n\\end{bmatrix}.\n\\end{align*}\n\\]\nEn otras palabras, nuestra hipótesis producirá la probabilidad estimada de que \\(p(y = i | x; \\theta)\\) para cada valor \\(i = 1, \\dots, k\\). (Aunque \\(h_\\theta(x)\\), como se definió arriba, tiene solo \\(k - 1\\) dimensiones, claramente \\(p(y = k | x; \\theta)\\) puede obtenerse como \\(1 - \\sum_{i=1}^{k-1} \\phi_i\\))."
  },
  {
    "objectID": "CAPITULO_2/02_GLM.html#ajuste-de-parámetros-1",
    "href": "CAPITULO_2/02_GLM.html#ajuste-de-parámetros-1",
    "title": "Modelos lineales generalizados",
    "section": "4.4 Ajuste de Parámetros",
    "text": "4.4 Ajuste de Parámetros\nPor último, discutamos el ajuste de parámetros. Similar a nuestra derivación original de mínimos cuadrados ordinarios y regresión logística, si tenemos un conjunto de entrenamiento de \\(n\\) ejemplos \\(\\{(x^{(i)}, y^{(i)}); i = 1, \\dots, n\\}\\) y deseamos aprender los parámetros \\(\\theta_i\\) de este modelo, comenzaríamos escribiendo la log-verosimilitud:\n\\[\n\\begin{align*}\n\\ell(\\theta) &= \\sum_{i=1}^n \\log p(y^{(i)} | x^{(i)}; \\theta)\n\\\\\n&= \\sum_{i=1}^n \\log \\prod_{l=1}^k \\left( \\frac{e^{\\theta_l^T x^{(i)}}}{\\sum_{j=1}^k e^{\\theta_j^T x^{(i)}}} \\right)^{1\\{y^{(i)} = l\\}}.\n\\end{align*}\n\\]\nPara obtener la segunda línea anterior, usamos la definición de \\(p(y | x; \\theta)\\) dada en la Ecuación (5). Ahora podemos obtener la estimación de máxima verosimilitud de los parámetros maximizando \\(\\ell(\\theta)\\) en términos de \\(\\theta\\), utilizando un método como ascenso de gradiente o el método de Newton.\nACA FALTA AJUSTE DE PARAMETROS PARA EL MODELO GLM EN GENERAL eso esta en el video de el viejo del 2008 , no esta en el nuevo. seria importante poner algo\n\n4.4.1 Ejemplo Práctico en Python\nImplementemos un ejemplo práctico para un problema de clasificación multiclasificación.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# Generar datos simulados para clasificación multiclase\nX, y = make_classification(\n    n_samples=300, n_features=2, n_informative=2, n_redundant=0,\n    n_classes=3, n_clusters_per_class=1, random_state=42\n)\n\n# Visualizar los datos\nplt.figure(figsize=(8, 6))\nscatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap=\"viridis\")\nplt.title(\"Datos de Clasificación Multiclase\")\nplt.xlabel(\"Característica 1\")\nplt.ylabel(\"Característica 2\")\nplt.colorbar(scatter, label=\"Clases\")\nplt.grid(True)\nplt.show()\n\n# Dividir los datos en entrenamiento y prueba\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Ajustar el modelo de regresión softmax\nmodel = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", max_iter=200)\nmodel.fit(X_train, y_train)\n\n# Predicciones\ny_pred = model.predict(X_test)\n\n# Frontera de decisión\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n                     np.arange(y_min, y_max, 0.01))\nZ = model.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nplt.figure(figsize=(8, 6))\nplt.contourf(xx, yy, Z, alpha=0.8, cmap=\"viridis\")\nscatter = plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor=\"k\", cmap=\"viridis\")\nplt.title(\"Frontera de Decisión: Regresión Softmax\")\nplt.xlabel(\"Característica 1\")\nplt.ylabel(\"Característica 2\")\nplt.colorbar(scatter, label=\"Clases\")\nplt.grid(True)\nplt.show()\n\n# Evaluación del modelo\nprint(\"Reporte de Clasificación:\")\nprint(classification_report(y_test, y_pred))\nprint(\"Matriz de Confusión:\")\nprint(confusion_matrix(y_test, y_pred))\n\n\n\n\n\n\n\n\nC:\\Users\\isaias\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\nReporte de Clasificación:\n              precision    recall  f1-score   support\n\n           0       0.75      0.89      0.81        27\n           1       0.92      0.89      0.91        38\n           2       1.00      0.84      0.91        25\n\n    accuracy                           0.88        90\n   macro avg       0.89      0.87      0.88        90\nweighted avg       0.89      0.88      0.88        90\n\nMatriz de Confusión:\n[[24  3  0]\n [ 4 34  0]\n [ 4  0 21]]"
  },
  {
    "objectID": "CAPITULO_1/A3_metodos_optimizacion.html",
    "href": "CAPITULO_1/A3_metodos_optimizacion.html",
    "title": "Métodos de optimización",
    "section": "",
    "text": "Comenzaremos explorando métodos de optimización de primer orden para problemas no lineales. En principio, consideraremos el caso sin restricciones \\[\n\\min_{\\xx\\in\\RR^n} f(\\xx),\n\\]\ncon \\(f\\) una función diferenciable. En este caso, ya hemos visto que \\(\\xx^*\\in\\RR^n\\) resuelve el problema de optimización solo si \\(\\nabla f(\\xx^*)=\\mathbf{0}\\). Más aún, cuando \\(f\\) es convexa, la condición es también suficiente.\nUna idea fundamental para construir un algoritmo de optimización es aproximar \\(f\\) con modelos más simples y manejables. En particular, los métodos de primer orden se basan en la aproximación de Taylor de primer orden, alrededor de un punto \\(\\xx_t\\), a saber: \\[\nf(\\xx) \\approx f(\\xx_{t}) + \\langle \\nabla f(\\xx_{t}), \\xx - \\xx_{t} \\rangle.\n\\]"
  },
  {
    "objectID": "CAPITULO_1/A3_metodos_optimizacion.html#condición-de-lipschitz-para-el-gradiente",
    "href": "CAPITULO_1/A3_metodos_optimizacion.html#condición-de-lipschitz-para-el-gradiente",
    "title": "Métodos de optimización",
    "section": "1.1 Condición de Lipschitz para el gradiente",
    "text": "1.1 Condición de Lipschitz para el gradiente\nEspecíficamente, requeriremos que el gradiente \\(\\nabla f(x)\\) sea \\(L\\)-Lipschitz continuo para alguna constante \\(L \\geq 0\\). Esta condición a menudo se llama \\(L\\)-suavidad en la literatura. La presentamos ahora para funciones generales con dominios convexos arbitrarios \\(\\Omega\\); hoy solo nos importará el caso \\(\\Omega = \\mathbb{R}^{n}\\).\n\nDefinición 1. (\\(L\\)-suavidad) Sea \\(\\Omega\\) un conjunto convexo. Una función diferenciable \\(f: \\Omega \\to \\RR\\) es \\(L\\)-suave si su gradiente es \\(L\\)-Lipschitz continuo. Es decir, se verifica \\[\n\\|\\nabla f(\\xx_1) - \\nabla f(\\xx_2)\\|_{2} \\leq L \\|\\xx_1 - \\xx_2\\|_{2}, \\qquad \\forall \\xx_1, \\xx_2 \\in \\Omega.\n\\]\n\nUna consecuencia inmediata de la \\(L\\)-suavidad es que la función admite una cota superior cuadrática. Esta propiedad será extremadamente útil en el análisis.\nTeorema 1.12.1 (Cota superior cuadrática). Sea \\(f: \\Omega \\to \\mathbb{R}\\) una función \\(L\\)-suave en un dominio convexo \\(\\Omega\\). Entonces, podemos acotar superiormente la función \\(f\\) como:\n\\[\nf(y) \\leq f(x) + \\langle \\nabla f(x), y - x \\rangle + \\frac{L}{2} \\|y - x\\|_{2}^{2} \\quad \\forall x, y \\in \\Omega. \\quad \\text{(2)}\n\\]\nDemostración. La idea es simple: expresamos el crecimiento \\(f(y) - f(x)\\) como la integral del gradiente en la línea que conecta \\(x\\) con \\(y\\), y luego usamos el límite de Lipschitz en el crecimiento del gradiente \\(\\nabla f\\) para acotar el crecimiento:\n\\[\\begin{align*}\nf(y) - f(x) &= \\int_{0}^{1} \\langle \\nabla f(x + t \\cdot (y - x)), y - x \\rangle \\, \\mathrm{d}t\\\\&\n= \\left( \\int_{0}^{1} \\langle \\nabla f(x + t \\cdot (y - x)) - \\nabla f(x), y - x \\rangle \\, \\mathrm{d}t \\right) + \\langle \\nabla f(x), y - x \\rangle\\\\\n&\n\\leq \\left( \\int_{0}^{1} \\|\\nabla f(x + t \\cdot (y - x)) - \\nabla f(x)\\|_{2} \\cdot \\|y - x\\|_{2} \\, \\mathrm{d}t \\right) + \\langle \\nabla f(x), y - x \\rangle\\\\\n&\n\\leq \\left( \\int_{0}^{1} t L \\|y - x\\|_{2}^{2} \\, \\mathrm{d}t \\right) + \\langle \\nabla f(x), y - x \\rangle\\\\\n&= \\frac{L}{2} \\|y - x\\|_{2}^{2} + \\langle \\nabla f(x), y - x \\rangle.\n\\end{align*}\\] Reordenando, obtenemos el enunciado. \\(\\blacksquare\\)\nTambién mencionamos la siguiente caracterización.\nTeorema 1.12.2. Para funciones dos veces diferenciables \\(f: \\Omega \\to \\mathbb{R}\\) definidas en un conjunto abierto \\(\\Omega \\subseteq \\mathbb{R}^{n}\\), una condición equivalente de \\(L\\)-suavidad es:\n\\[\n-L I \\preceq \\nabla^{2} f(x) \\preceq L I \\quad \\forall x \\in \\Omega,\n\\]\no equivalentemente,\n\\[\n|v^{\\top} \\nabla^{2} f(x) v| \\leq L \\quad \\forall x \\in \\Omega, v \\in \\mathbb{R}^{n}: \\|v\\|_{2} = 1.\n\\]"
  },
  {
    "objectID": "CAPITULO_1/A1_intro_optimizacion.html#definiciones-básicas",
    "href": "CAPITULO_1/A1_intro_optimizacion.html#definiciones-básicas",
    "title": "Introducción a la Optimización",
    "section": "1.1 Definiciones básicas",
    "text": "1.1 Definiciones básicas\nEn esta sección introduciremos tres tipos fundamentales de conjuntos que aparecen en optimización: conjuntos afines, conjuntos convexos y conos. Antes, es necesario recordar la forma parámetrica de rectas y segmentos de recta.\nSean \\(\\xx_{1} \\neq \\xx_{2}\\) dos puntos en \\(\\RR^{n}\\). La recta que pasa por \\(\\xx_1\\) y \\(\\xx_2\\) queda determinada por \\[\n\\yy = \\theta \\xx_{1} + (1 - \\theta) \\xx_{2},\\qquad \\theta\\in\\RR.\n\\]\nMientras que, si restringimos el valor del parámetro a \\(0\\leq \\theta\\leq 1\\), obtenemos el segmento de recta (cerrado) entre ambos puntos. Una expresión alternativa es\n\\[\n\\yy=\\xx_2+\\theta(\\xx_1-\\xx_2),\n\\]\nla cual permite interpretar a \\(\\yy\\) en términos del punto inicial \\(\\xx_2\\) y la dirección \\(\\xx_1-\\xx_2\\). Así, \\(\\theta\\) indica la fracción del camino desde \\(\\xx_2\\) hasta \\(\\xx_1\\) donde se encuentra \\(\\yy\\). Esto se ilustra en la (figura 2.1).\n\n1.1.1 Conjuntos afines\n\nDefinición 1. (Conjunto afín) Un conjunto \\(C \\subseteq \\RR^{n}\\) es afín si la línea que pasa por cualquier par de puntos distintos en \\(C\\) está contenida en \\(C\\). Es decir, si se verifica \\[\n\\theta \\xx_1+(1-\\theta)\\xx_2\\in C\\qquad\\forall \\xx_1,\\xx_2\\in C, \\forall\\theta\\in\\RR.\n\\]\n\nObservar que \\(\\theta \\xx_1+(1-\\theta)\\xx_2\\) es una combinación lineal entre los puntos \\(\\xx_1\\) y \\(\\xx_2\\) que verifica que la suma de sus coeficientes sea uno. Esta idea se puede generalizar a más de dos puntos: un punto de la forma \\[\n\\theta_1\\xx_1+\\cdots+\\theta_k\\xx_k,\\qquad \\sum_{i=1}^k\\theta_i=1\n\\] se denomina combinación afín de los puntos \\(\\xx_1,\\ldots,\\xx_k\\). Es fácil ver que un conjunto afín \\(C\\) contiene todas las combinaciones afines de sus puntos.\nPor otro lado, dado un conjunto afín \\(C\\) y un punto \\(\\xx_0\\in C\\), el conjunto \\[\nV:= C-\\xx_0=\\{\\xx-\\xx_0 \\mid \\xx\\in C\\}\n\\]\nes un subespacio vectorial, que permite expresar el conjunto afín \\(C\\) como \\[\nC=V+\\xx_0=\\{v+\\xx_0 \\mid v\\in V\\}.\n\\]\nEl subespacio \\(V\\) asociado con el conjunto afín \\(C\\) no depende de la elección de \\(x_{0}\\), por lo que \\(x_{0}\\) puede elegirse como cualquier punto en \\(C\\). Definimos la dimensión de \\(C\\) como \\[\n\\dim C:=\\dim V=\\dim C-\\xx_0,\\quad \\xx_0\\in C.\n\\]\n\n\n1.1.2 Conjuntos convexos\n\nDefinición 1. (Conjunto convexo) Un conjunto \\(C \\subseteq \\RR^{n}\\) es convexo si el segmento de línea entre cualquier par de puntos distintos en \\(C\\) está contenido en \\(C\\). Es decir, si se verifica \\[\n\\theta \\xx_1+(1-\\theta)\\xx_2\\in C\\qquad\\forall \\xx_1,\\xx_2\\in C, \\forall\\theta\\in[0,1].\n\\]\n\nClaramente, todo conjunto afín también es convexo, ya que contiene toda la recta que pasa por cualquier par de puntos distintos en él y, por lo tanto, también el segmento de línea entre los puntos. La (figura 2.2) ilustra algunos conjuntos convexos y no convexos en \\(\\RR^{2}\\).\nLlamamos combinación convexa a un punto de la forma \\[\n\\theta_{1} \\xx_{1} + \\cdots + \\theta_{k} \\xx_{k},\\qquad \\sum_{i=1}^k\\theta_i = 1, \\theta_{i} \\geq 0.\n\\]\nA diferencia de una combinación afín, una combinación convexa requiere la no negatividad de los coeficientes \\(\\theta_i\\), lo cual significa que puede ser interpretada como un promedio ponderado de los puntos \\(\\xx_i\\).\n\nDefinición 1. (Envolvente convexa) La envolvente convexa de un conjunto \\(C\\) es el conjunto de todas las combinaciones convexas de puntos en \\(C\\). Esto es \\[\n\\text{conv}\\, C = \\left\\{\\theta_{1} x_{1} + \\cdots + \\theta_{k} x_{k}\\;\\Big|\\; x_{i} \\in C, \\theta_{i} \\geq 0, \\sum_{i=1}^k\\theta_{i} = 1\\right\\}.\n\\]\n\nComo su nombre lo indica, la envolvente convexa \\(\\text{conv}\\, C\\) es siempre un conjunto convexo. Más aún, es el conjunto convexo más pequeño que contiene a \\(C\\): si \\(B\\) es cualquier conjunto convexo que contiene a \\(C\\), entonces \\(\\text{conv}\\, C \\subseteq B\\). La (figura 2.3) ilustra la definición de casco convexo.\n\n\n1.1.3 Conos\n\nDefinición 1. (Cono) Un conjunto \\(C\\in\\RR^n\\) se denomina cono si verifica \\[\n\\theta\\xx\\in C,\\qquad\\forall \\xx\\in C, \\forall \\theta\\geq 0.\n\\]\n\nUn conjunto \\(C\\) es un cono convexo si es convexo y es un cono, lo que significa que \\[\n\\theta_1\\xx_1+\\theta_2\\xx_2\\in C,\\qquad\\forall \\xx_1,\\xx_2\\in C, \\forall \\theta_1, \\theta_2\\geq 0.\n\\]\nLos puntos de esta forma pueden describirse geométricamente como formando una “rebanada de pastel” bidimensional con vértice en \\(0\\) y bordes que pasan por \\(x_{1}\\) y \\(x_{2}\\). (figura 2.4).\nLlamamos combinación cónica a un punto de la forma \\[\n\\theta_1\\xx_1+\\cdot+\\theta_k\\xx_j,\\qquad \\theta_i\\geq 0.\n\\]\n\nDefinición 1. (Envolvente cónica) La envolvente cónica de un conjunto \\(C\\) es el conjunto de todas las combinaciones cónicas de puntos en \\(C\\). Esto es \\[\n\\text{cone}\\, C = \\left\\{\\theta_{1} \\xx_{1} + \\cdots + \\theta_{k} \\xx_{k}\\;\\Big|\\; x_{i} \\in C, \\theta_{i} \\geq 0\\right\\}.\n\\]\n\nDe manera análoga a la envolvente convexa, la envolvente cónica de un conjunto \\(C\\) es el conjunto de todas las combinación cónicas de puntos en \\(C\\) y, además, es el cono convexo más pequeño que contiene a \\(C\\). figura 2.5)"
  },
  {
    "objectID": "CAPITULO_1/A1_intro_optimizacion.html#operaciones-que-preservan-convexidad",
    "href": "CAPITULO_1/A1_intro_optimizacion.html#operaciones-que-preservan-convexidad",
    "title": "Introducción a la Optimización",
    "section": "1.2 Operaciones que preservan convexidad",
    "text": "1.2 Operaciones que preservan convexidad\nEn esta sección describimos algunas operaciones que preservan la convexidad de conjuntos o nos permiten construir conjuntos convexos a partir de otros. Estas operaciones, junto con los ejemplos simples descritos en §2.2, forman un cálculo de conjuntos convexos que es útil para determinar o establecer la convexidad de conjuntos.\n\n1.2.1 Intersección\nLa convexidad se preserva bajo intersección: si \\(S_{1}\\) y \\(S_{2}\\) son convexos, entonces \\(S_{1} \\cap S_{2}\\) es convexo. Esta propiedad se extiende a la intersección de un número infinito de conjuntos: si \\(S_{\\alpha}\\) es convexo para cada \\(\\alpha \\in \\mathcal{A}\\), entonces \\(\\bigcap_{\\alpha \\in \\mathcal{A}} S_{\\alpha}\\) es convexo. (Los subespacios, conjuntos afines y conos convexos también son cerrados bajo intersecciones arbitrarias). Como ejemplo simple, un poliedro es la intersección de semiespacios e hiperplanos (que son convexos), y por lo tanto es convexo.\nEjemplo 2.7: El cono semidefinido positivo \\(\\mathbb{S}^{n}_{+}\\) puede expresarse como\n\\[\n\\bigcap_{z \\neq 0} \\{X \\in \\mathbb{S}^{n} \\mid z^{T} X z \\geq 0\\}.\n\\]\nPara cada \\(z \\neq 0\\), \\(z^{T} X z\\) es una función lineal (no idénticamente cero) de \\(X\\), por lo que los conjuntos\n\\[\n\\{X \\in \\mathbb{S}^{n} \\mid z^{T} X z \\geq 0\\}\n\\]\nson, de hecho, semiespacios en \\(\\mathbb{S}^{n}\\). Por lo tanto, el cono semidefinido positivo es la intersección de un número infinito de semiespacios, y por lo tanto es convexo.\nEjemplo 2.8: Consideramos el conjunto\n\\[\nS = \\{x \\in \\mathbf{R}^{m} \\mid |p(t)| \\leq 1 \\text{ para } |t| \\leq \\pi/3\\}, \\quad (2.10)\n\\]\ndonde \\(p(t) = \\sum_{k=1}^{m} x_{k} \\cos kt\\). El conjunto \\(S\\) puede expresarse como la intersección de un número infinito de lajas: \\(S = \\bigcap_{|t| \\leq \\pi/3} S_{t}\\), donde\n\\[\nS_{t} = \\{x \\mid -1 \\leq (\\cos t, \\ldots, \\cos mt)^{T} x \\leq 1\\},\n\\]\ny por lo tanto es convexo. La definición y el conjunto se ilustran en las figuras 2.13 y 2.14, para \\(m = 2\\).\nEn los ejemplos anteriores establecemos la convexidad de un conjunto expresándolo como una intersección (posiblemente infinita) de semiespacios. Veremos en §2.5.1 que se cumple lo contrario: todo conjunto convexo cerrado \\(S\\) es una intersección (generalmente infinita) de semiespacios. De hecho, un conjunto convexo cerrado \\(S\\) es la intersección de todos los semiespacios que lo contienen:\n\\[\nS = \\bigcap \\{\\mathcal{H} \\mid \\mathcal{H} \\text{ semiespacio, } S \\subseteq \\mathcal{H}\\}.\n\\]\n\n\n1.2.2 2.3.2 Funciones afines\nRecordemos que una función \\(f: \\mathbf{R}^{n} \\rightarrow \\mathbf{R}^{m}\\) es afín si es una suma de una función lineal y una constante, es decir, si tiene la forma \\(f(x) = A x + b\\), donde \\(A \\in \\mathbf{R}^{m \\times n}\\) y \\(b \\in \\mathbf{R}^{m}\\). Supongamos que \\(S \\subseteq \\mathbf{R}^{n}\\) es convexo y \\(f: \\mathbf{R}^{n} \\rightarrow \\mathbf{R}^{m}\\) es una función afín. Entonces, la imagen de \\(S\\) bajo \\(f\\),\n\n\n\n\n\n\\[\nf(S) = \\{f(x) \\mid x \\in S\\},\n\\]\nes convexa. De manera similar, si \\(f: \\mathbf{R}^{k} \\rightarrow \\mathbf{R}^{n}\\) es una función afín, la imagen inversa de \\(S\\) bajo \\(f\\),\n\\[\nf^{-1}(S) = \\{x \\mid f(x) \\in S\\},\n\\]\nes convexa.\nDos ejemplos simples son la escalación y la traslación. Si \\(S \\subseteq \\mathbf{R}^{n}\\) es convexo, \\(\\alpha \\in \\mathbf{R}\\), y \\(a \\in \\mathbf{R}^{n}\\), entonces los conjuntos \\(\\alpha S\\) y \\(S + a\\) son convexos, donde\n\\[\n\\alpha S = \\{\\alpha x \\mid x \\in S\\}, \\qquad S + a = \\{x + a \\mid x \\in S\\}.\n\\]\nLa proyección de un conjunto convexo sobre algunas de sus coordenadas es convexa: si \\(S \\subseteq \\mathbf{R}^{m} \\times \\mathbf{R}^{n}\\) es convexo, entonces\n\\[\nT = \\{x_{1} \\in \\mathbf{R}^{m} \\mid (x_{1}, x_{2}) \\in S \\text{ para algún } x_{2} \\in \\mathbf{R}^{n}\\}\n\\]\nes convexo.\nLa suma de dos conjuntos se define como\n\\[\nS_{1} + S_{2} = \\{x + y \\mid x \\in S_{1}, y \\in S_{2}\\}.\n\\]\nSi \\(S_{1}\\) y \\(S_{2}\\) son convexos, entonces \\(S_{1} + S_{2}\\) es convexo. Para ver esto, si \\(S_{1}\\) y \\(S_{2}\\) son convexos, entonces el producto directo o cartesiano\n\\[\nS_{1} \\times S_{2} = \\{(x_{1}, x_{2}) \\mid x_{1} \\in S_{1}, x_{2} \\in S_{2}\\}\n\\]\ntambién es convexo. La imagen de este conjunto bajo la función lineal \\(f(x_{1}, x_{2}) = x_{1} + x_{2}\\) es la suma \\(S_{1} + S_{2}\\).\nTambién podemos considerar la suma parcial de \\(S_{1}, S_{2} \\in \\mathbf{R}^{n} \\times \\mathbf{R}^{m}\\), definida como\n\\[\nS = \\{(x, y_{1} + y_{2}) \\mid (x, y_{1}) \\in S_{1}, (x, y_{2}) \\in S_{2}\\},\n\\]\ndonde \\(x \\in \\mathbf{R}^{n}\\) y \\(y_{i} \\in \\mathbf{R}^{m}\\). Para \\(m = 0\\), la suma parcial da la intersección de \\(S_{1}\\) y \\(S_{2}\\); para \\(n = 0\\), es la suma de conjuntos. Las sumas parciales de conjuntos convexos son convexas (ver ejercicio 2.16).\nEjemplo 2.9: Poliedro. El poliedro \\(\\{x \\mid A x \\preceq b, C x = d\\}\\) puede expresarse como la imagen inversa del producto cartesiano del ortante no negativo y el origen bajo la función afín \\(f(x) = (b - A x, d - C x)\\):\n\\[\n\\{x \\mid A x \\preceq b, C x = d\\} = \\{x \\mid f(x) \\in \\mathbf{R}_{+}^{m} \\times \\{0\\}\\}.\n\\]\nEjemplo 2.10: Conjunto solución de una desigualdad matricial lineal. La condición\n\\[\nA(x) = x_{1} A_{1} + \\cdots + x_{n} A_{n} \\preceq B, \\quad (2.11)\n\\]\ndonde \\(B, A_{i} \\in \\mathbf{S}^{m}\\), se llama desigualdad matricial lineal (LMI) en \\(x\\) (nota la similitud con una desigualdad lineal ordinaria,\n\\[\na^{T} x = x_{1} a_{1} + \\cdots + x_{n} a_{n} \\leq b,\n\\]\ncon \\(b, a_{i} \\in \\mathbf{R}\\)).\nEl conjunto solución de una desigualdad matricial lineal, \\(\\{x \\mid A(x) \\preceq B\\}\\), es convexo. De hecho, es la imagen inversa del cono semidefinido positivo bajo la función afín \\(f: \\mathbf{R}^{n} \\rightarrow \\mathbf{S}^{m}\\) dada por \\(f(x) = B - A(x)\\).\n\n\n1.2.3 2.3.3 Funciones lineales-fraccionales y perspectivas\nEn esta sección exploramos una clase de funciones, llamadas lineales-fraccionales, que es más general que las funciones afines pero aún preserva la convexidad.\n\n\n\n\n\n\n1.2.3.1 Función perspectiva\nDefinimos la función perspectiva \\(P: \\mathbf{R}^{n+1} \\rightarrow \\mathbf{R}^{n}\\), con dominio \\(\\mathbf{dom}\\, P = \\mathbf{R}^{n} \\times \\mathbf{R}_{++}\\), como \\(P(z, t) = z / t\\). (Aquí \\(\\mathbf{R}_{++}\\) denota el conjunto de números positivos: \\(\\mathbf{R}_{++} = \\{x \\in \\mathbf{R} \\mid x &gt; 0\\}\\)). La función perspectiva escala o normaliza vectores de modo que la última componente sea uno, y luego elimina la última componente.\nObservación 2.1: Podemos interpretar la función perspectiva como la acción de una cámara estenopeica. Una cámara estenopeica (en \\(\\mathbf{R}^{3}\\)) consiste en un plano horizontal opaco \\(x_{3} = 0\\), con un único agujero en el origen, a través del cual puede pasar la luz, y un plano de imagen horizontal \\(x_{3} = -1\\). Un objeto en \\(x\\), por encima de la cámara (es decir, con \\(x_{3} &gt; 0\\)), forma una imagen en el punto \\(-(x_{1} / x_{3}, x_{2} / x_{3}, 1)\\) en el plano de imagen. Eliminando la última componente del punto de imagen (ya que siempre es \\(-1\\)), la imagen de un punto en \\(x\\) aparece en \\(y = -(x_{1} / x_{3}, x_{2} / x_{3}) = -P(x)\\) en el plano de imagen. Esto se ilustra en la figura 2.15.\nSi \\(C \\subseteq \\mathbf{dom}\\, P\\) es convexo, entonces su imagen\n\\[\nP(C) = \\{P(x) \\mid x \\in C\\}\n\\]\n\n\n\n\n\nes convexa. Este resultado es ciertamente intuitivo: un objeto convexo, visto a través de una cámara estenopeica, produce una imagen convexa. Para establecer este hecho, mostramos que los segmentos de línea se mapean a segmentos de línea bajo la función perspectiva. (Esto también tiene sentido: un segmento de línea, visto a través de una cámara estenopeica, produce una imagen de segmento de línea). Supongamos que \\(x = (\\tilde{x}, x_{n+1})\\), \\(y = (\\tilde{y}, y_{n+1}) \\in \\mathbf{R}^{n+1}\\) con \\(x_{n+1} &gt; 0\\), \\(y_{n+1} &gt; 0\\). Entonces, para \\(0 \\leq \\theta \\leq 1\\),\n\\[\nP(\\theta x + (1 - \\theta) y) = \\frac{\\theta \\tilde{x} + (1 - \\theta) \\tilde{y}}{\\theta x_{n+1} + (1 - \\theta) y_{n+1}} = \\mu P(x) + (1 - \\mu) P(y),\n\\]\ndonde\n\\[\n\\mu = \\frac{\\theta x_{n+1}}{\\theta x_{n+1} + (1 - \\theta) y_{n+1}} \\in [0, 1].\n\\]\nEsta correspondencia entre \\(\\theta\\) y \\(\\mu\\) es monótona: a medida que \\(\\theta\\) varía entre 0 y 1 (lo que recorre el segmento de línea \\([x, y]\\)), \\(\\mu\\) varía entre 0 y 1 (lo que recorre el segmento de línea \\([P(x), P(y)]\\)). Esto muestra que \\(P([x, y]) = [P(x), P(y)]\\).\nAhora supongamos que \\(C\\) es convexo con \\(C \\subseteq \\mathbf{dom}\\, P\\) (es decir, \\(x_{n+1} &gt; 0\\) para todo \\(x \\in C\\)), y \\(x, y \\in C\\). Para establecer la convexidad de \\(P(C)\\), necesitamos mostrar que el segmento de línea \\([P(x), P(y)]\\) está en \\(P(C)\\). Pero este segmento de línea es la imagen del segmento de línea \\([x, y]\\) bajo \\(P\\), y por lo tanto está en \\(P(C)\\).\nLa imagen inversa de un conjunto convexo bajo la función perspectiva también es convexa: si \\(C \\subseteq \\mathbf{R}^{n}\\) es convexo, entonces\n\\[\nP^{-1}(C) = \\{(x, t) \\in \\mathbf{R}^{n+1} \\mid x / t \\in C, t &gt; 0\\}\n\\]\nes convexo. Para mostrar esto, supongamos que \\((x, t) \\in P^{-1}(C)\\), \\((y, s) \\in P^{-1}(C)\\), y \\(0 \\leq \\theta \\leq 1\\). Necesitamos mostrar que\n\\[\n\\theta (x, t) + (1 - \\theta) (y, s) \\in P^{-1}(C),\n\\]\nes decir, que\n\\[\n\\frac{\\theta x + (1 - \\theta) y}{\\theta t + (1 - \\theta) s} \\in C\n\\]\n(\\(\\theta t + (1 - \\theta) s &gt; 0\\) es obvio). Esto se sigue de\n\\[\n\\frac{\\theta x + (1 - \\theta) y}{\\theta t + (1 - \\theta) s} = \\mu (x / t) + (1 - \\mu) (y / s),\n\\]\ndonde\n\\[\n\\mu = \\frac{\\theta t}{\\theta t + (1 - \\theta) s} \\in [0, 1].\n\\]\n\n\n1.2.3.2 Funciones lineales-fraccionales\nUna función lineal-fraccional se forma componiendo la función perspectiva con una función afín. Supongamos que \\(g: \\mathbf{R}^{n} \\rightarrow \\mathbf{R}^{m+1}\\) es afín, es decir,\n\\[\ng(x) = \\left[\\begin{array}{c}A\\\\ c^{T}\\end{array}\\right] x + \\left[\\begin{array}{c}b\\\\ d\\end{array}\\right], \\quad (2.12)\n\\]\ndonde \\(A \\in \\mathbf{R}^{m \\times n}\\), \\(b \\in \\mathbf{R}^{m}\\), \\(c \\in \\mathbf{R}^{n}\\), y \\(d \\in \\mathbf{R}\\). La función \\(f: \\mathbf{R}^{n} \\rightarrow \\mathbf{R}^{m}\\) dada por \\(f = P \\circ g\\), es decir,\n\\[\nf(x) = (A x + b) / (c^{T} x + d), \\qquad \\mathbf{dom}\\, f = \\{x \\mid c^{T} x + d &gt; 0\\}, \\quad (2.13)\n\\]\nse llama función lineal-fraccional (o proyectiva). Si \\(c = 0\\) y \\(d &gt; 0\\), el dominio de \\(f\\) es \\(\\mathbf{R}^{n}\\), y \\(f\\) es una función afín. Por lo tanto, podemos pensar en las funciones afines y lineales como casos especiales de funciones lineales-fraccionales.\nObservación 2.2: Interpretación proyectiva. A menudo es conveniente representar una función lineal-fraccional como una matriz\n\\[\nQ = \\left[\\begin{array}{cc}A & b\\\\ c^{T} & d\\end{array}\\right] \\in \\mathbf{R}^{(m+1) \\times (n+1)} \\quad (2.14)\n\\]\nque actúa sobre (multiplica) puntos de la forma \\((x, 1)\\), lo que produce \\((A x + b, c^{T} x + d)\\). Este resultado se escala o normaliza de modo que su última componente sea uno, lo que produce \\((f(x), 1)\\).\nEsta representación puede interpretarse geométricamente asociando \\(\\mathbf{R}^{n}\\) con un conjunto de rayos en \\(\\mathbf{R}^{n+1}\\) de la siguiente manera. Con cada punto \\(z\\) en \\(\\mathbf{R}^{n}\\) asociamos el rayo (abierto) \\(\\mathcal{P}(z) = \\{(t z, 1) \\mid t &gt; 0\\}\\) en \\(\\mathbf{R}^{n+1}\\). La última componente de este rayo toma valores positivos. A la inversa, cualquier rayo en \\(\\mathbf{R}^{n+1}\\), con base en el origen y última componente que toma valores positivos, puede escribirse como \\(\\mathcal{P}(v) = \\{t (v, 1) \\mid t \\geq 0\\}\\) para algún \\(v \\in \\mathbf{R}^{n}\\). Esta correspondencia (proyectiva) \\(\\mathcal{P}\\) entre \\(\\mathbf{R}^{n}\\) y el semiespacio de rayos con última componente positiva es biyectiva.\nLa función lineal-fraccional (2.13) puede expresarse como\n\\[\nf(x) = \\mathcal{P}^{-1}(Q \\mathcal{P}(x)).\n\\]\nAsí, comenzamos con \\(x \\in \\mathbf{dom}\\, f\\), es decir, \\(c^{T} x + d &gt; 0\\). Luego formamos el rayo \\(\\mathcal{P}(x)\\) en \\(\\mathbf{R}^{n+1}\\). La transformación lineal con matriz \\(Q\\) actúa sobre este rayo para producir otro rayo \\(Q \\mathcal{P}(x)\\). Dado que \\(x \\in \\mathbf{dom}\\, f\\), la última componente de este rayo asume valores positivos. Finalmente, tomamos la transformación proyectiva inversa para recuperar \\(f(x)\\)\nAl igual que la función de perspectiva, las funciones lineales-fraccionales preservan la convexidad. Si \\(C\\) es convexo y se encuentra en el dominio de \\(f\\) (, \\(c^T x + d &gt; 0\\) para \\(x \\in C\\)), entonces la imagen \\(f(C)\\) es convexa. Esto se sigue inmediatamente de los resultados anteriores: la imagen de \\(C\\) bajo la transformación afín (2.12) es convexa, y la imagen del conjunto resultante bajo la función de perspectiva \\(P\\), que produce \\(f(C)\\), es convexa. De manera similar, si \\(C \\subseteq \\mathbb{R}^m\\) es convexo, entonces la imagen inversa \\(f^{-1}(C)\\) es convexa.\nEjemplo 2.13 Probabilidades condicionales. Supongamos que \\(u\\) y \\(v\\) son variables aleatorias que toman valores en \\(\\{1, \\dots, n\\}\\) y \\(\\{1, \\dots, m\\}\\), respectivamente, y que \\(p_{ij}\\) denota \\(\\operatorname{prob}(u = i, v = j)\\). Entonces, la probabilidad condicional \\(f_{ij} = \\operatorname{prob}(u = i | v = j)\\) está dada por\n\\[\nf_{ij} = \\frac{p_{ij}}{\\sum_{k=1}^{n} p_{kj}}.\n\\]\nAsí, \\(f\\) se obtiene a partir de una transformación lineal-fraccional de \\(p\\).\nSe sigue que si \\(C\\) es un conjunto convexo de probabilidades conjuntas para \\((u, v)\\), entonces el conjunto asociado de probabilidades condicionales de \\(u\\) dado \\(v\\) también es convexo.\nLa Figura 2.16 muestra un conjunto \\(C \\subseteq \\mathbb{R}^2\\) y su imagen bajo la función lineal-fraccional\n\\[\nf(x) = \\frac{1}{x_1 + x_2 + 1} x,\n\\]\n\\[\n\\operatorname{dom} f = \\{ (x_1, x_2) \\mid x_1 + x_2 + 1 &gt; 0 \\}.\n\\]"
  },
  {
    "objectID": "CAPITULO_1/A1_intro_optimizacion.html#condiciones-de-convexidad",
    "href": "CAPITULO_1/A1_intro_optimizacion.html#condiciones-de-convexidad",
    "title": "Introducción a la Optimización",
    "section": "2.1 Condiciones de convexidad",
    "text": "2.1 Condiciones de convexidad\nPara verificar si una función es convexa, además de recurrir a la definición, existen criterios basados en sus derivadas. Estos permiten caracterizar la convexidad en términos del comportamiento local de la función, ya sea a través del gradiente (primera derivada) o de la matriz hessiana (segunda derivada). Para ello, debemos imponer condiciones a \\(f\\) en relación con su diferenciabilidad.\nImportante Diremos que \\(f\\) es diferenciable si su gradiente \\(\\nabla f\\) existe en todo su dominio, y que es dos veces diferenciable si además su hessiana \\(\\nabla^2 f\\) existe en todo el dominio. En ambos casos, asumimos que dicho dominio es un conjunto abierto.\n\nTeorema 1. (Condición de primer orden para convexidad) Sea \\(f:\\RR^n\\to\\RR\\) una función diferenciable. \\(f\\) es convexa si y solo si \\(\\text{dom}\\,f\\) es convexo y \\[\nf(\\xx_2)\\geq f(\\xx_1)+\\langle\\nabla f(\\xx_1),\\xx_2-\\xx_1\\rangle,\\qquad\\forall\\xx_1,\\xx_2\\in\\text{dom}\\,f.\n\\]\n\n\n\nMostrar detalles\n\n\n\nInterpretación geométricaObservacionesDemostración\n\n\nLa función afín de \\(\\xx_2\\), definida por \\(f(\\xx_1) + \\langle \\nabla f(\\xx_1), \\xx_2 - \\xx_1 \\rangle\\), es la aproximación lineal de \\(f\\) cerca de \\(\\xx_1\\). En consecuencia, la desigualdad del teorema implica que la aproximación lineal de \\(f\\) en \\(\\xx_1\\) es un subestimador global de \\(f\\).\n\n\n\nGrafo de una función convexa \\(f\\) y su aproximación lineal.\n\n\n\n\n\nObservar que, si \\(\\nabla f(\\xx_1) = 0\\), entonces la desigualdad del teorema implica \\(f(\\xx_2) \\geq f(\\xx_1)\\) para todo \\(\\xx_2 \\in \\text{dom} f\\). Es decir, en tal caso \\(\\xx_1\\) es un minimizador global de la función \\(f\\).\nLa convexidad estricta también puede caracterizarse por una condición de primer orden: \\(f\\) es estrictamente convexa si y solo si \\(\\text{dom} f\\) es convexo y \\[\nf(\\xx_2) &gt; f(\\xx_1) + \\langle \\nabla f(\\xx_1), \\xx_2 - \\xx_1 \\rangle\n\\] para todo \\(\\xx_1, \\xx_2 \\in \\text{dom} f\\), con \\(\\xx_1 \\neq \\xx_2\\).\nPara funciones cóncavas, tenemos la caracterización correspondiente: \\(f\\) es cóncava si y solo si \\(\\text{dom} f\\) es convexo y \\[\nf(\\xx_2) \\leq f(\\xx_1) + \\langle \\nabla f(\\xx_1), \\xx_2 - \\xx_1 \\rangle\n\\] para todo \\(\\xx_1, \\xx_2 \\in \\text{dom} f\\).\n\n\n\nSupongamos que \\(f\\) es convexa y sean \\(\\xx_1,\\xx_2\\in\\text{dom}\\,f\\). Entonces, por definición, \\(\\text{dom}\\,f\\) es convexo; en consecuencia \\(\\xx_1+t(\\xx_2-\\xx_1)\\in\\text{dom}\\,f\\) para todo \\(0&lt;t\\leq 1\\). La convexidad de \\(f\\) nos permite escribir\n\\[\nf(\\xx_1+t(\\xx_2-\\xx_1))=f((1-t)\\xx_1+t\\xx_2)\\leq (1-t)f(\\xx_1)+tf(\\xx_2).\n\\]\nDiviendo ambos lados de la desigualdad por \\(t\\), resulta\n\\[\nf(\\xx_2)\\geq f(\\xx_1)+\\frac{f(\\xx_1+t(\\xx_2-\\xx_1))-f(\\xx_1)}{t}.\n\\]\nAl tomar límite cuando \\(t\\to 0\\), el cociente incremental del lado derecho es la derivada direccional de \\(f\\) en \\(\\xx_1\\) en la dirección del vector \\(\\xx_2-\\xx_1\\), lo cual es equivalente a \\(\\langle \\nabla f(\\xx_1),\\xx_2-\\xx_1\\rangle\\) en virtud de la diferenciabilidad de \\(f\\). Luego,\n\\[\nf(\\xx_2)\\geq f(\\xx_1)+\\langle\\nabla f(\\xx_1),\\xx_2-\\xx_1\\rangle.\n\\]\nPara probar la suficiencia, vamos a asumir que sí se cumple para funciones de una variable (se deja como ejercicio). Sean \\(\\xx_1,\\xx_2\\in\\text{dom}\\,f\\) y \\(t,\\tilde{t}\\in(0,1]\\). Partimos de la desigualdad\n\\[\nf(t\\xx_2+(1-t)\\xx_1)\\geq f(\\tilde{t}\\xx_2+(1-\\tilde{t})\\xx_1)+\\langle \\nabla f(\\tilde{t}\\xx_2+(1-\\tilde{t})\\xx_1),(\\xx_2-\\xx_1)\\rangle (t-\\tilde{t}).\n\\]\nSi restringimos \\(f\\) a la recta entre \\(\\xx_1\\) y \\(\\xx_2\\), tenemos la función de una variable \\(g(t)=f(t\\xx_2+(1-t)\\xx_1)\\). Por regla de la cadena, tenemos \\(g'(t)\\langle \\nabla f(t\\xx_2+(1-t)\\xx_1),\\xx_2-\\xx_1\\rangle\\). En consecuencia, la desigualdad anterior puede reescribirse como\n\\[\ng(t)\\geq g(\\tilde{t})+g'(\\tilde{t})(t-\\tilde{t}).\n\\]\nLuego, \\(g\\) es convexa y, dado que la restricción sobre una recta es arbitraria (ver observaciones de la definición de funciones convexas), \\(f\\) es convexa. \\(\\blacksquare\\)\n\n\n\n\n\n\nTeorema 2. (Condición de segundo orden para convexidad) Sea \\(f:\\RR^n\\to\\RR\\) una función dos veces diferenciable. \\(f\\) es convexa si y solo si \\(\\text{dom}\\,f\\) es convexo y \\[\n\\nabla^2 f(\\xx)\\succeq 0,\\qquad\\forall\\xx\\in\\text{dom}\\,f.\n\\]\n\n\n\\(A\\succeq 0\\) significa que \\(A\\) es semidefinida positiva. Esto es, \\(\\xx^T A\\xx\\geq 0\\) para todo \\(\\xx\\in\\RR^n\\).\n\n\n\nMostrar detalles\n\n\n\nObservaciones\n\n\n\nPara funciones de una variable, la condición se reduce a \\(f''(x)\\geq 0\\), lo cual implica que la derivada \\(f'(x)\\) es no decreciente.\nLa convexidad estricta también puede caracterizarse por una condición de primer orden, pero de manera parcial: si \\(\\text{dom}\\,f\\) es convexo y \\(\\nabla^2 f\\succ 0\\) para todo \\(\\xx\\in\\text{dom}\\,f\\), entonces \\(f\\) es estrictamente convexa. El recíproco no es cierto: por ejemplo, \\(f(x)=x^4\\) es estrictamente convexa pero \\(f''(0)=0\\).\nPara funciones cóncavas, tenemos la caracterización correspondiente: \\(f\\) es cóncava si y solo si \\(\\text{dom} f\\) es convexo y \\(\\nabla^2 f(\\xx)\\preceq 0\\) para todo \\(\\xx\\in\\text{dom}\\,f\\).\n\n\n\n\n\n\n\nEjemplo 1 Consideremos la función cuadrática \\(f:\\RR^n\\to\\RR\\) definida por\n\\[\nf(\\xx)=\\frac{1}{2}\\xx^TA\\xx+\\bb^T\\xx+c,\n\\]\ndonde \\(A\\in\\SS^n:=\\{X\\in\\RR^{n\\times n} \\mid X=X^T\\}\\), \\(\\bb\\in\\RR^n\\) y \\(c\\in\\RR\\). Dado que \\(\\nabla^2 f(\\xx)=A\\) para todo \\(\\xx\\), podemos afirmar que \\(f\\) es convexa si y sólo si \\(A\\succeq 0\\).\nAsí, por ejemplo, la función \\(f:\\RR^2\\to\\RR\\) definida por\n\\[\nf(x,y)=\\frac{1}{2}\\begin{pmatrix}x&y\\end{pmatrix}\\begin{pmatrix}2&1\\\\1&3\\end{pmatrix}\\begin{pmatrix}x\\\\ y\\end{pmatrix}=\\frac{1}{2}(2x^2+2xy+3y^2)\n\\]\nes convexa, ya que \\(\\xx^TA\\xx=2x^2+2xy+3y^2=x^2+(x+y)^2+2y^2\\geq 0\\) para todo \\(\\xx\\in\\RR^n\\), lo cual significa que \\(A\\succeq 0\\).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D  # para 3D\n\n# Definir la matriz A (simétrica)\nA = np.array([[2, 1],\n              [1, 3]])\n\n# Crear una malla de puntos en R^2\nx = np.linspace(-3, 3, 100)\ny = np.linspace(-3, 3, 100)\nX, Y = np.meshgrid(x, y)\n\n# Evaluar la función f(x) = 1/2 x^T A x\nZ = 0.5 * (A[0,0]*X**2 + (A[0,1] + A[1,0])*X*Y + A[1,1]*Y**2)\n\n# Graficar\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)\nax.set_xlabel('x')\nax.set_ylabel('y')\nplt.show()"
  },
  {
    "objectID": "CAPITULO_1/A1_intro_optimizacion.html#operaciones-que-preservan-convexidad-1",
    "href": "CAPITULO_1/A1_intro_optimizacion.html#operaciones-que-preservan-convexidad-1",
    "title": "Introducción a la Optimización",
    "section": "2.2 Operaciones que preservan convexidad",
    "text": "2.2 Operaciones que preservan convexidad\nEn esta sección describimos algunas operaciones que preservan la convexidad o concavidad de funciones, o que permiten construir nuevas funciones convexas y cóncavas. Las verificaciones se dejan como ejercicio.\n\nSumas ponderadas no negativas. Si \\(f\\) es una función convexa y \\(\\alpha \\geq 0\\), entonces la función \\(\\alpha f\\) es convexa. Si \\(f_1\\) y \\(f_2\\) son ambas funciones convexas, entonces su suma \\(f_1 + f_2\\) también lo es. Combinando estas dos propiedades, se obtiene que el conjunto de funciones convexas es en sí mismo un cono convexo: una suma ponderada no negativa de funciones convexas, \\[\nf = w_1 f_1 + \\cdots + w_m f_m,\n\\] es convexa. De manera similar, una suma ponderada no negativa de funciones cóncavas es cóncava.\nComposición con una aplicación afín. Sean \\(f:\\RR^n\\to\\RR\\), \\(A\\in\\RR^{n\\times m}\\) y \\(\\bb\\in\\RR^n\\). Definimos \\(g:\\RR^m\\to\\RR\\) como \\[\ng(\\xx):=f(A\\xx+\\bb)\n\\] con \\(\\text{dom}\\,g = \\{\\xx \\mid A\\xx + \\bb \\in \\text{dom}\\, f\\}\\). Entonces, \\(g\\) conserva la convexidad o concavidad de \\(f\\).\nMáximo puntual. Si \\(f_1\\) y \\(f_2\\) son funciones convexas, entonces su máximo puntual \\(f\\), definido por \\[\nf(\\xx) = \\max\\{f_1(\\xx), f_2(\\xx)\\},\n\\] con \\(\\text{dom}\\,f = \\text{dom}\\,f_1 \\cap \\text{dom}\\,f_2\\), también es convexa. Además, este resultado se puede extender a: si \\(f_1,\\dots,f_m\\) son funciones convexas, entonces su máximo puntual, definido por \\(f(\\xx)=\\max\\{f_1(\\xx), \\ldots, f_m(\\xx)\\}\\), también lo es.\nSupremo puntual. La propiedad del máximo puntual se extiende al supremo puntual sobre un conjunto infinito de funciones convexas. Si para $i, donde \\(\\mathcal{A}\\) es un conjunto de índices, se tiene que \\(f_i(\\xx)\\) es convexa, entonces la función \\(f\\) definida por \\[\nf(\\xx) = \\sup_{i \\in \\mathcal{A}} f_i(\\xx)\n\\] también es convexa en \\(\\xx\\). Aquí, el dominio de \\(f\\) es \\[\n\\text{dom}\\, f = \\{\\xx \\mid \\xx \\in \\text{dom}\\, f_i \\text{ para todo } i \\in \\mathcal{A}, \\sup_{i \\in \\mathcal{A}} f_i(\\xx) &lt; \\infty\\}.\n\\] De manera similar, el ínfimo puntual de un conjunto de funciones cóncavas es una función cóncava."
  },
  {
    "objectID": "CAPITULO_1/A2_optimalidad.html",
    "href": "CAPITULO_1/A2_optimalidad.html",
    "title": "Condiciones de optimalidad",
    "section": "",
    "text": "En un problema de optimización general \\[\n\\min_{x} f(x)\n\\] \\[\n\\text{s.t.}\\; x\\in\\Omega=\\left\\{x\\in\\RR^n\\left|\\begin{array}{rl}\ng_i(x)\\leq 0,& i=1,\\cdots,r.\\\\\nh_j(x)=0, & j=1,\\cdots,m.\n\\end{array}\\right.\\right\\},\n\\]\nlas condiciones de optimalidad definen los requisitos que deben cumplir los puntos óptimos. En lo que sigue, asumiremos que trabajamos con funciones diferenciables."
  },
  {
    "objectID": "CAPITULO_1/A2_optimalidad.html#optimización-sin-restricciones",
    "href": "CAPITULO_1/A2_optimalidad.html#optimización-sin-restricciones",
    "title": "Condiciones de optimalidad",
    "section": "1.1 Optimización sin restricciones",
    "text": "1.1 Optimización sin restricciones\nDe cursos anteriores recordemos que, cuando se pretende optimizar una función \\(f\\) respecto a \\(x\\in\\RR^n\\), una condición necesaria para que un punto sea óptimo es que verifique \\[\n\\nabla f(x)=\\mathbf{0}.\n\\]\nPero cuidado: es solo una condición necesaria que todos los puntos óptimos deben cumplir, pero no implica que cualquier punto que la satisfaga sea automáticamente óptimo. En otras palabras, las soluciones de \\(\\nabla f(x) = 0\\) forman una lista de puntos candidatos para minimizar, llamados puntos críticos.\nDe inmediato surgen dos preguntas claves:\n\n¿Cuál es la generalización correcta de la condición necesaria \\(\\nabla f(x) = 0\\) cuando enfrentamos un problema de optimización con restricciones?\n¿Bajo qué circunstancias \\(\\nabla f(x) = 0\\) también se convierte en una condición suficiente para la optimalidad?\n\nAntes, veamos cómo surge la condición \\(\\nabla f(x)=\\mathbf{0}\\) para problemas de optimización sin restricciones.\n\n1.1.1 Prueba de condición necesaria \\(\\nabla f(x)=\\mathbf{0}\\)\nSea \\(x\\in\\RR^n\\) es un minimizador de la función \\(f: \\RR^n \\to \\RR\\) y sea \\(d\\in\\RR^n\\) una dirección arbitraria. Entonces se cumple\n\\[\nf(x + t \\cdot d) \\geq f(x) \\quad \\text{para todo } t \\geq 0.\n\\]\nEn consecuencia, la derivada direccional de \\(f\\) en \\(x\\) a lo largo de \\(d\\) es\n\\[\n\\frac{\\partial f}{\\partial d}(x)= \\lim_{t \\to 0} \\frac{f(x + t \\cdot d) - f(x)}{t} \\geq 0.\n\\]\nAhora bien, como \\(f\\) es diferenciable, se verifica la propiedad \\(\\frac{\\partial f}{\\partial d}(x)=\\langle \\nabla f(x),d\\rangle\\), por lo que la desigualdad anterior se puede reescribir como:\n\\[\n\\langle \\nabla f(x), d \\rangle \\geq 0 \\quad \\forall d \\in {R}^n.\n\\]\nEn particular, eligiendo \\(d = -\\nabla f(x)\\), resulta \\(-\\|\\nabla f(x)\\|^2\\geq 0\\), lo cual es cierto si y sólo si \\(\\nabla f(x)=\\mathbf{0}\\)."
  },
  {
    "objectID": "CAPITULO_1/A2_optimalidad.html#optimización-con-restricciones",
    "href": "CAPITULO_1/A2_optimalidad.html#optimización-con-restricciones",
    "title": "Condiciones de optimalidad",
    "section": "1.2 Optimización con restricciones",
    "text": "1.2 Optimización con restricciones\nLa clave para generalizar la condición \\(\\nabla f(x)=\\mathbf{0}\\) al caso de optimización con restricciones surge de la prueba anterior, más precisamente de la desigualdad \\(\\langle\\nabla f(x),d\\rangle\\geq 0\\).\nLa diferencia principal con el caso sin restricciones es que, en un conjunto restringido, podríamos estar limitados en la elección de las direcciones \\(d\\) a lo largo de las cuales podemos aproximarnos a \\(x\\) sin salirnos del conjunto.\nNo obstante, para cualquier dirección \\(d\\) tal que \\(x + t \\cdot d \\in \\Omega\\) para todo \\(t \\geq 0\\) suficientemente pequeño, el mismo argumento aplicado anteriormente sigue siendo válido. Por lo tanto, podemos concluir que necesariamente:\n\\[\n\\langle \\nabla f(x), d \\rangle \\geq 0 \\quad \\text{para todo } d \\in  {R}^n \\text{ que permanezca en } \\Omega \\text{ desde } x.\n\\]\nPara aplicar esta condición, se requieren dos pasos:\n\nDeterminar el conjunto de direcciones \\(d\\) que permanecen en \\(\\Omega\\) desde \\(x\\)*.\nA partir de esas direcciones, ver de qué manera imponen restricciones sobre \\(\\nabla f(x)\\).\n\nDe estos dos pasos, el primero suele ser el más sencillo. En todos los casos de interés, podemos determinar el conjunto de direcciones permitidas simplemente considerando cualquier otro punto \\(y \\in \\Omega\\) y observando la dirección de \\(x\\) a \\(y\\). Esta propiedad se cumple trivialmente si todos los segmentos de línea entre \\(x\\) y cualquier punto en \\(\\Omega\\) están contenidos en \\(\\Omega\\), lo cual es siempre cierto para conjuntos convexos.\n\nTeorema 1. (Condición necesaria de optimalidad de primer orden para un conjunto factible convexo) Sea \\(\\Omega \\subseteq  {R}^n\\) convexo y sea \\(f:  {R}^n \\to  {R}\\) una función diferenciable. Si \\(x \\in \\Omega\\) es un minimizador de \\(f\\) sobre \\(\\Omega\\), entonces \\[\n\\langle \\nabla f(x), y - x \\rangle \\geq 0 \\quad \\forall y \\in \\Omega.\n\\]\n\n\n\nInterpretación geométrica\n\n\nLa condición del Teorema 1 se verifica si el vector gradiente de \\(f\\) en una solución \\(x\\in\\Omega\\) forma un ángulo agudo con todas las direcciones \\(y-x\\), donde \\(y\\in\\Omega\\).\nAhora bien, a la hora de buscar un minimizador, la dirección que realmente nos importa es \\(-\\nabla f(x)\\). Así, el Teorema 1 nos dice que \\(-\\nabla f(x)\\) debe formar un ángulo obtuso con las direcciones \\(y-x\\). En otras palabras, la condición dada es equivalente a escribir\n\\[\n-\\nabla f(x)\\in N_{\\Omega}(x),\n\\]\ndonde \\(N_{\\Omega}(x)\\) es el cono normal a \\(\\Omega\\) en \\(x\\).\nEs importante observar que si \\(x\\) es un punto interior de \\(\\Omega\\), sabemos que \\(N_{\\Omega}(x)=\\{\\mathbf{0}\\}\\) y, en consecuencia, debe ser \\(\\nabla f(x)=\\mathbf{0}\\). Esto es consistente con la condición de optimalidad de los problemas sin restricciones.\nNo obstante, la importancia del Teorema 1 radica en cómo tratar los puntos en la frontera de \\(\\Omega\\). A partir de los ejemplos previamente estudiados sobre el cono normal, podemos ver que la interpretación del Teorema 1 es bastante intuitiva. Por ejemplo, si \\(\\Omega\\) es un polígono y \\(x\\in\\Omega\\) es una solución, el vector \\(-\\nabla f(x)\\) es perpendicular a \\(\\Omega\\) y apunta hacia afuera, lo cual significa que no hay posibilidad de “moverse” en dirección opuesta al gradiente sin salirse de \\(\\Omega\\).\n\n\n¡Cuidado! La condición provista por el Teorema 1 es necesaria pero no suficiente. Podría verificarse la condición en otros puntos críticos de \\(\\Omega\\) que no sean un mínimo global de \\(f\\) en dicho conjunto. Sin embargo, para el notable caso de las funciones convexas dicha condición sí es suficiente.\n\nTeorema 2. (Condición necesaria de optimalidad de primer orden para un conjunto factible convexo y una función objetivo convexa) Sea \\(\\Omega \\subseteq  {R}^n\\) convexo y sea \\(f:  {R}^n \\to  {R}\\) una función diferenciable convexa. Entonces \\[\n-\\nabla f(x)\\in N_{\\Omega}(x)\\Leftrightarrow x \\text{ es un minimizador de }f\\text{ en }\\Omega.\n\\]"
  },
  {
    "objectID": "CAPITULO_1/A2_optimalidad.html#el-caso-particular-de-restricciones-lineales",
    "href": "CAPITULO_1/A2_optimalidad.html#el-caso-particular-de-restricciones-lineales",
    "title": "Condiciones de optimalidad",
    "section": "2.1 El caso particular de restricciones lineales",
    "text": "2.1 El caso particular de restricciones lineales\nVamos a considerar el caso en que \\(\\Omega\\) un polítopo convexo, esto es, el conjunto convexo definido por la intersección de un número finito de medios espacios (desigualdades lineales):\n\\[\n\\Omega=\\{\\xx\\in\\RR^n|A\\xx\\leq \\bfb\\},\n\\]\ndonde \\(A\\in\\RR^{m\\times n}\\) es una matriz, cuyas filas denotaremos con \\(\\bfa_j\\), \\(j=1,\\cdots,m\\), y \\(\\bfb\\in\\RR^m\\).\nEn el ejemplo … (Agregar ejemplo 3.6 de Lecture 2 a ‘Intro’) hemos visto que el cono normal en un punto en la intersección de dos medios espacios es la envolvente cónica de las direcciones ortogonales a dichos subespacios.\n\n\n\nEnvolvente cónica de la intersección de dos subespacios \\(\\bfa_1^T\\xx\\leq b_1\\) y \\(\\bfa_2^T\\xx\\leq b_2\\).\n\n\nPor otra parte, los otros casos también ya han sido vistos:\n\nSi \\(\\xx\\) pertenece al interior de \\(\\Omega\\), entonces \\(N_{\\Omega}(\\xx)=\\{\\mathbf{0}\\}\\).\nSi \\(\\xx\\) pertenece a la frontera de un único medio espacio, a saber \\(\\bfa_k^T\\xx=b_k\\), entonces \\(N_{\\Omega}(\\xx)=\\{\\lambda \\bfa_k:\\lambda\\geq 0\\}\\).\n\nLa generalización al caso de \\(m\\) medios espacios se presenta en el siguiente teorema.\n\nTeorema 3. Sea \\(\\Omega=\\{x\\in\\RR^n\\}\\) la intersección de \\(m\\) medios espacios \\(\\bfa_j^T\\xx\\leq b_j\\). Dado un punto \\(x\\in\\Omega\\), se define el conjunto de índices de las restricciones activas mediante\n\\[\nI(\\xx):=\\left\\{j\\in\\{1,\\cdots,m\\}:\\bfa_j^T\\xx=b_j\\right\\}.\n\\] Entonces, el cono normal en \\(\\xx\\) está dado por\n\\[\nN_{\\Omega}(\\xx)=\\left\\{\\sum_{j\\in I(\\xx)}\\lambda_j\\bfa_j:\\lambda_j\\geq 0\\right\\}.\n\\]\n\n\n\nInterpretación\n\n\nSi \\(\\xx\\) pertenece al interior de \\(\\Omega\\), entonces no hay restricciones activas. Esto se corresponde con el hecho de que el cono normal es \\(N_{\\Omega}(\\xx)=\\{\\mathbf{0}\\}\\).\nPor su parte, si \\(\\xx\\) pertenece a la frontera de \\(\\Omega\\), entonces el cono normal es la envolvente cónica de las direcciones ortogonales a las restricciones activas.\nObservar que la condición de optimalidad \\(-\\nabla f(\\xx)\\in N_{\\Omega}(\\xx)\\) se traduce como\n\\[\n-\\nabla f(\\xx)=\\sum_{j\\in I(\\xx)}\\lambda_j\\bfa_j,\n\\]\ny, en consecuencia, se puede escribir\n\\[\n\\nabla f(\\xx)-\\sum_{j\\in I(\\xx)}\\lambda_j \\nabla g_j(\\xx)=0,\n\\]\ncon \\(g_j(\\xx)=\\bfa_j^T\\xx-b\\). Los coeficientes \\(\\lambda_j\\) se denominan tipicamente multiplicadores de Lagrange.\n\n\nLa suma en la expresión del cono normal puede ser reescrita sin restringir \\(j\\in I(\\xx)\\), simplemente imponiendo \\(\\lambda_j=0\\) para todo \\(j\\notin I(\\xx)\\). Esta imposición queda ímplicita de forma inmediata si se escribe\n\\[\n\\sum_{j=1}^m\\lambda_j\\left(\\bfa_j^T\\xx-b_j\\right)=0.\n\\]\nEn forma vectorial, esto es \\(\\bflambda^T\\left(A\\xx-\\bfb\\right)=0\\), donde \\(\\bflambda=(\\lambda_1,\\cdots,\\lambda_m)^T\\). Con esta notación, el resultado del Teorema 3 puede reescribirse como\n\\[\nN_{\\Omega}(\\xx)=\\left\\{A^T\\bflambda:\\bflambda^T(A\\xx-\\bfb)=0,\\bflambda\\in\\RR^m_{\\geq 0}\\right\\}.\n\\]\nPara concluir este análisis, y teniendo en cuenta lo expuesto hasta aquí, podemos hacer énfasis en tres condiciones necesarias para que una solución sea óptima en este caso particular de restricciones lineales.\n\nPara que \\(\\xx\\in\\Omega\\) sea un minimizador de \\(f\\) sobre \\(\\Omega\\), debe cumplir:\n\nEstacionariedad: El gradiente debe ser una combinación lineal de los gradientes de las restricciones activas. \\[\n\\nabla f(\\xx)-\\sum_{j=1}^{m}\\lambda_j(\\bfa_j^T\\xx-b_j).\n\\]\nFactibilidad dual: Los multiplicadores de Lagrange asociados a las restricciones de desigualdad deben ser no negativos. \\[\n\\lambda_j\\geq 0 \\quad \\forall j=1,\\cdots,m.\n\\]\nHolgura complementaria: Los multiplicadores de Lagrange solo pueden ser positivos si la restricción está activa. \\[\n\\lambda_j\\left(\\bfa_j^T\\xx-b_j\\right)=0 \\quad \\forall j=1,\\cdots,m.\n\\]\n\n\n Las condiciones mencionadas se conocen como condiciones de Karush-Kuhn-Tucker y son una consecuencia de la caracterización del cono normal para conjuntos definidos como interseccion de restricciones lineales, provista por el Teorema 3. Ya estamos en condiciones de abordar el problema de optimización general."
  },
  {
    "objectID": "CAPITULO_1/A2_optimalidad.html#generalización",
    "href": "CAPITULO_1/A2_optimalidad.html#generalización",
    "title": "Condiciones de optimalidad",
    "section": "2.2 Generalización",
    "text": "2.2 Generalización\nConsideremos el problema de optimización general \\[\n\\min_{x} f(x)\n\\] \\[\n\\text{s.t.}\\; x\\in\\Omega=\\left\\{x\\in\\RR^n\\left|\\begin{array}{rl}\nh_i(x)= 0,& i=1,\\cdots,r.\\\\\ng_j(x)\\leq 0, & j=1,\\cdots,m.\n\\end{array}\\right.\\right\\},\n\\]\ndonde \\(\\Omega\\) está definido como una intersección de restricciones funcionales diferenciables.\nSupongamos que \\(\\xx^*\\) es un punto óptimo en la frontera del conjunto factible \\(\\Omega\\) correspondiente a tres condiciones de desigualdad \\(g_i(x) \\le 0\\) para \\(i=1,2,3\\) (ver figura abajo). La idea principal es la siguiente:\n\nEl conjunto de direcciones que forma un ángulo obtuso con todas las direcciones desde \\(\\xx^*\\) que permanecen en \\(\\Omega\\) coincide con el cono normal de la linearización de las restricciones activas en \\(\\xx^*\\).\n\n\n\n\nEnvolvente cónica de la intersección de restricciones \\(g_i(\\xx)\\leq 0\\), para \\(i=1,2,3\\).\n\n\nSi aplicamos la idea anterior a \\(-\\nabla f(\\xx)\\) y consideramos el Teorema 3, obtenemos la generalización de las condiciones de optimalidad de Karush-Kuhn-Tucker (KKT). Denominemos \\(\\tilde{\\Omega}\\) la linearización de \\(\\Omega\\), dado por la linearización de las restricciones en un punto óptimo \\(\\xx^*\\):\n\\[\nh_i(\\xx^*)+\\nabla h_i(\\xx^*)\\cdot (\\xx-\\xx^*)=0,\n\\]\n\\[\ng_i(\\xx^*)+\\nabla g_i(\\xx^*)\\cdot (\\xx-\\xx^*)\\leq 0.\n\\]\nEntonces, por Teorema 3, resulta\n\\[\nN_{\\tilde{\\Omega}}(\\xx^*)=\\left\\{\\sum_{i=1}^m\\lambda_i\\nabla h_i(\\xx^*)+\\sum_{j\\in I(\\xx^*)}\\mu_j\\nabla g_j(\\xx^*): \\lambda_i\\in\\RR, \\mu_j\\in\\RR_{\\geq 0}\\right\\},\n\\]\ndonde \\(I(\\xx^*):=\\left\\{j\\in\\{1,\\cdots,m\\}: g_j(\\xx^*)=0\\right\\}\\). Por supuesto, puede reformularse la condición $jI(^*) utilizando holgura complementaria, tal como antes.\nSi bien no estamos aún en condiciones de extender el Teorema 3, puesto que el desarrollo previo fue más bien intuitivo, podemos establecer las condiciones KKT mediante una definición formal de la siguiente manera.\n\nDefinición 1. (Condiciones de Karush-Kuhn-Tucker (KKT) Sea un problema de optimización no lineal con función objetivo diferenciable y restricciones funcionales, de la forma \\[\n\\text{s.t.}\\; x\\in\\Omega=\\left\\{x\\in\\RR^n\\left|\\begin{array}{rl}\nh_i(x)= 0,& i=1,\\cdots,r.\\\\\ng_j(x)\\leq 0, & j=1,\\cdots,m.\n\\end{array}\\right.\\right\\},\n\\]\ny sea \\(\\xx\\in\\Omega\\) (factibilidad primal). Las condiciones KKT en \\(\\xx\\) están dadas por:\n\nEstacionariedad: \\[\n-\\nabla f(\\xx)=\\sum_{i=1}^r\\lambda_i\\nabla h_i(\\xx)+\\sum_{j=1}^m\\mu_j\\nabla g_j(\\xx).\n\\]\nFactibilidad dual: \\[\n\\lambda_i\\in\\RR, \\mu_j\\in\\RR_{\\geq 0}\\quad\\forall i=1,\\cdots,r, \\forall j=1,\\cdots, m.\n\\]\nHolgura complementaria: \\[\n\\mu_j g_j(\\xx)=0\\quad \\forall j=1,\\cdots,m.\n\\]\n\n\nEs importante remarcar que:\n\nLas condiciones KKT indican que -\\(\\nabla f(\\xx)\\) debe estar en el cono normal a la linearización del conjunto de restricciones.\nLa condición de holgura compelementaria se resume en “si la restricción \\(g_j(\\xx)\\leq 0\\) no está activa, entonces \\(\\mu_j=0\\)”.\nLas condiciones KKT a menudo son necesarias para la optimalidad, pero no siempre.\n\nA continuación, veremos un ejemplo donde las condiciones KKT fallan. Luego, en la siguiente sección veremos en qué escenarios las condiciones KKT son efectivamente una condición necesaria de optimalidad."
  },
  {
    "objectID": "CAPITULO_1/A2_optimalidad.html#cualificación-de-restricciones",
    "href": "CAPITULO_1/A2_optimalidad.html#cualificación-de-restricciones",
    "title": "Condiciones de optimalidad",
    "section": "2.3 Cualificación de restricciones",
    "text": "2.3 Cualificación de restricciones"
  },
  {
    "objectID": "CAPITULO_1/A2_optimalidad.html#función-dual-de-lagrange",
    "href": "CAPITULO_1/A2_optimalidad.html#función-dual-de-lagrange",
    "title": "Condiciones de optimalidad",
    "section": "3.1 Función dual de Lagrange",
    "text": "3.1 Función dual de Lagrange\n\nDefinición 2. (Lagrangiano) Sea el problema de optimización general \\[\n\\min f(\\xx)\n\\] \\[\n\\text{sujeto a}\\; \\xx\\in\\Omega=\\left\\{\\xx\\in\\RR^p\\left|\\begin{array}{rl}\ng_i(\\xx)\\leq 0,& i=1,\\cdots,q\\\\\nh_j(\\xx)= 0, & j=1,\\cdots,r\n\\end{array}\\right.\\right\\}.\n\\]\nSe denomina Lagrangiano a la función \\(\\calL:\\RR^p\\times\\RR^q\\times\\RR^r\\to\\RR\\) definida por \\[\n\\calL(\\xx,\\bflambda,\\bfnu)=f(\\xx)+\\sum_{i=1}^q\\lambda_i g_i(\\xx)+\\sum_{j=1}^r\\nu_j h_j(\\xx).\n\\]\n\nLos vectores \\(\\bflambda\\) y \\(\\bfnu\\), cuyas componentes son multiplicadores de Lagrange, se denominan variables duales del problema de optimización y son el argumento de la función definida a continuación. A su vez, al problema de optimización original se lo denomina problema primal.\n\nDefinición 3. (Función dual) Sea \\(\\calL\\) el Lagrangiano de la Definición 2. Se denomina función dual de Lagrange a \\(\\calG:\\RR^q\\times\\RR^r\\to\\RR\\) definida por \\[\n\\calG(\\bflambda,\\bfmu)=\\inf_{\\xx}\\calL(\\xx,\\bflambda,\\bfnu).\n\\]\nCuando el Lagrangiano no está acotado inferiormente en \\(\\xx\\), se asume el valor \\(-\\infty\\).\n\nDado que la función dual es el ínfimo puntual de una familia de funciones afínes de \\((\\bflambda,\\bfnu)\\), es cóncava, aún cuando el problema primal no sea convexo [ver Ejercicio …].\nLa propiedad más importante de la función dual es que es una cota inferior del valor óptimo \\(p^*\\) del problema primal. Es decir, para todo \\(\\bflambda\\geq 0\\) y cualquier \\(\\bfnu\\), resulta \\[\n\\calG(\\bflambda,\\bfnu)\\leq p^*.\n\\]\n\n\nVerificación\n\n\nSea \\(\\tilde{\\xx}\\) un punto factible del problema primal. Entonces \\(g_i(\\tilde{\\xx})\\leq 0\\) y \\(h_j(\\tilde{\\xx})=0\\) para todo \\(i=1,\\cdots,q\\) y \\(j=1,\\cdots,r\\), respectivamente. Por lo tanto, para \\(\\bflambda\\geq 0\\) se tiene que \\[\n\\sum_{i=1}^q\\lambda_ig_i(\\tilde{\\xx})+\\sum_{j=1}^r\\nu_j h_j(\\tilde{\\xx})\\geq 0.\n\\]\nSumando \\(f(\\tilde{\\xx})\\) a ambos miembros, se obtiene \\(\\calL(\\xx,\\bflambda,\\bfnu)\\leq f(\\tilde{\\xx})\\). Luego, por propiedad del ínfimo, resulta \\[\n\\calG(\\bflambda,\\bfnu)\\leq f(\\tilde{\\xx}).\n\\]\nFinalmente, dado que \\(\\tilde{\\xx}\\) es cualquier punto factible, en particular la desigualdad anterior es cierta para un punto óptimo \\(\\xx^*\\) tal que \\(f(\\xx^*)=p^*\\)."
  },
  {
    "objectID": "CAPITULO_1/A2_optimalidad.html#el-problema-dual-de-lagrange",
    "href": "CAPITULO_1/A2_optimalidad.html#el-problema-dual-de-lagrange",
    "title": "Condiciones de optimalidad",
    "section": "3.2 El problema dual de Lagrange",
    "text": "3.2 El problema dual de Lagrange\nInmediatamente nos podemos preguntar: ¿cual es el valor máximo \\(d^*\\) de \\(\\calG(\\bflambda,\\bfnu)\\) si asumimos \\(\\bflambda\\geq 0\\)? Esto da lugar al problema que definiremos a continuación. Observar que la desigualdad \\(\\calG(\\bflambda,\\bfnu)\\leq p^*\\) implica \\[\nd^*\\leq p^*.\n\\]\n\nDefinición 3. (Problema dual) Sea \\(\\calG\\) la función dual de Lagrange asociado al problema primal de la Definición 2. El problema dual de Lagrange es \\[\n\\max \\calG(\\bflambda,\\bfnu)\n\\] \\[\n\\text{sujeto a }\\; \\bflambda\\geq 0.\n\\]\n\nEn línea con la notación anterior, denominaremos multiplicadores óptimos a un par \\((\\bflambda^*,\\bfnu^*)\\) que es solución del problema dual. Es decir, verifican \\(\\calG(\\bflambda^*,\\bfnu^*)=d^*\\).\nEl problema dual de Lagrange es un problema de optimización convexo, independientemente de que el problema primal sea convexo o no. Esto se debe a que la función a maximizar es cóncava y la restricción es un conjunto convexo."
  },
  {
    "objectID": "CAPITULO_1/A2_optimalidad.html#suboptimalidad-y-criterio-de-parada",
    "href": "CAPITULO_1/A2_optimalidad.html#suboptimalidad-y-criterio-de-parada",
    "title": "Condiciones de optimalidad",
    "section": "3.3 Suboptimalidad y criterio de parada",
    "text": "3.3 Suboptimalidad y criterio de parada\nLos puntos factibles duales nos permiten acotar cuán subóptimo es un punto factible dado, sin conocer el valor exacto de \\(p^{*}\\). De hecho, si \\(\\xx\\) es factible primal y \\((\\bflambda, \\bfnu)\\) es factible dual, entonces se cumple la siguiente desigualdad: \\[\nf(\\xx) - p^{*} \\leq f(\\xx)-\\calG(\\bflambda, \\bfnu).\n\\]\nEn particular, esto establece que \\(\\xx\\) es \\(\\epsilon\\)-subóptimo, con \\[\\epsilon = f(\\xx) - g(\\bflambda, \\bfnu).\n\\]\nLa brecha \\(\\epsilon\\) se conoce como brecha de dualidad para los puntos factibles \\(\\xx\\) y \\((\\bflambda,\\bfnu)\\). Para dichos puntos, los valores óptimos de los problemas primal y dual verifican \\[\np^*,d^*\\in\\left[g(\\bflambda,\\bfnu), f(\\xx)\\right],\n\\] donde el ancho del intervalo es justamente la brecha de dualidad. Observar que si \\(\\epsilon=0\\), entonces \\(\\xx\\) es óptimo primal y \\((\\bflambda,\\bfnu)\\) es óptimo dual.\nEl concepto de brecha de dualidad puede usarse en algoritmos de optimización para proporcionar criterios de parada no heurísticos. Supongamos que un algoritmo produce una secuencia de puntos factibles primales \\(\\xx^{(k)}\\) y puntos factibles duales \\((\\bflambda^{(k)}, \\bfnu^{(k)})\\), para \\(k = 1, 2, \\ldots\\), y sea \\(\\epsilon_{\\text{abs}} &gt; 0\\) una precisión absoluta requerida. Entonces, el criterio de parada \\[\nf(\\xx^{(k)}) - g(\\bflambda^{(k)}, \\bfnu^{(k)}) \\leq \\epsilon_{\\text{abs}}\n\\]\ngarantiza que, cuando el algoritmo termina, \\(\\xx^{(k)}\\) es \\(\\epsilon_{\\text{abs}}\\)-subóptimo. Por supuesto, la dualidad fuerte debe cumplirse si se pretende que este método funcione para tolerancias \\(\\epsilon_{\\text{abs}}\\) arbitrariamente pequeñas."
  },
  {
    "objectID": "CAPITULO_1/A2_optimalidad.html#holgura-complementaria",
    "href": "CAPITULO_1/A2_optimalidad.html#holgura-complementaria",
    "title": "Condiciones de optimalidad",
    "section": "3.4 Holgura complementaria",
    "text": "3.4 Holgura complementaria\nSupongamos que los valores óptimos primal y dual se alcanzan y son iguales. Esto es, se cumple la desigualdad fuerte \\(p^*=d^*\\). Esto significa que para puntos óptimos \\(\\xx^*\\) y \\((\\bflambda^*,\\bfnu^*)\\) se verifica \\[\n\\begin{align*}\nf(\\xx^{*}) &= \\calG(\\bflambda^{*}, \\bfnu^{*}) \\\\\n&= \\inf_{x} \\left( f(\\xx) + \\sum_{i=1}^{q} \\lambda_{i}^{*} g_{i}(x) + \\sum_{j=1}^{r} \\nu_{j}^{*} h_{j}(\\xx) \\right)\\\\\n& \\leq f(\\xx^{*}) + \\sum_{i=1}^{q} \\lambda_{i}^{*} g_{i}(\\xx^{*}) + \\sum_{j=1}^{r} \\nu_{j}^{*} h_{j}(\\xx^{*}) \\\\&\\leq f(\\xx^{*}).\n\\end{align*}\n\\]\nLa justificación de cada uno los pasos es sencilla [Ejercicio …]. Concluimos que las dos desigualdades en esta cadena se cumplen con igualdad. En particular, la última igualdad\n\\[\nf(\\xx^*)+ \\sum_{i=1}^{q} \\lambda_{i}^{*} g_{i}(\\xx^{*}) + \\sum_{j=1}^{r} \\nu_{j}^{*} h_{j}(\\xx^{*})=f(\\xx^*)\n\\]\nimplica que\n\\[\n\\sum_{i=1}^{q} \\lambda_{i}^{*} g_{i}(\\xx^{*}) = 0.\n\\]\nPero cada término en esta suma es no positivo, por lo tanto debe verificarse\n\\[\n\\boxed{\\lambda_{i}^{*} g_{i}(\\xx^{*}) = 0, \\quad \\forall i = 1, \\ldots, q.}\n\\]\nEsta es la condición de holgura complementaria que hemos visto en la sección anterior. Se cumple para cualquier punto óptimo primal \\(\\xx^{*}\\) y cualquier punto óptimo dual \\((\\bflambda^{*}, \\bfnu^{*})\\) bajo dualidad fuerte. Recordemos que, en términos generales, esta condición significa que el \\(i\\)-ésimo multiplicador de Lagrange óptimo es cero salvo que la \\(i\\)-ésima restricción esté activa."
  },
  {
    "objectID": "CAPITULO_1/A2_optimalidad.html#relación-entre-dualidad-y-kkt",
    "href": "CAPITULO_1/A2_optimalidad.html#relación-entre-dualidad-y-kkt",
    "title": "Condiciones de optimalidad",
    "section": "3.5 Relación entre dualidad y KKT",
    "text": "3.5 Relación entre dualidad y KKT\nLos conceptos de Lagrangiano y su función dual han permitido caracterizar la relación entre los valores óptimos \\(p^*\\) y \\(d^*\\). En particular, las condiciones de factibilidad dual y de holgura complementaria de la Definición 1 (Condiciones de Karush-Kuhn-Tucker) han permitido primero escribir la desigualdad \\(\\calG(\\bflambda,\\bfmu)\\leq p^*\\) y luego analizar qué sucede cuando \\(p^*=q^*\\).\nVeamos ahora cómo surge la estacionariedad en este contexto. Podemos afirmar que\n\\[\nf(\\xx^*)\\leq f(\\xx)+\\sum_{i=1}^{q} \\lambda_{i}^{*} g_{i}(\\xx) + \\sum_{j=1}^{r} \\nu_{j}^{*} h_{j}(\\xx)\n\\]\npara todo punto factible \\(\\xx\\), con igualdad si \\(\\xx=\\xx^*\\). El lado derecho es el Lagrangiano \\(L(\\xx,\\bflambda^*,\\bfnu^*)\\), del cual podemos afirmar que \\(\\xx\\) es un minimizador. En consecuencia, debe verificarse\n\\[\n\\begin{align*}\n  \\nabla L(\\xx^*,\\bflambda^*,\\bfnu^*)&=0\\\\\n  \\nabla f(\\xx^*)+\\sum_{i=1}^q\\lambda_i^*\\nabla g_i(\\xx^*)+\\sum_{j=1}^r\\nu_j^*\\nabla h_j(\\xx^*)&=0.\n\\end{align*}\n\\]\n\nTeorema 4. (Dualidad y KKT) Sea \\(\\xx^*\\) un punto óptimo primal y \\((\\bflambda^*,\\bfnu^*)\\) un punto óptimo dual. Si \\(p^*=q^*\\), entonces \\((\\xx^*,\\bflambda^*,\\bfnu^*)\\) satisfacen las condiciones de Karush-Kuhn-Tucker.\n\n\n\n\nEjercicios\n\n\nConsidere el problema \\[\n\\min x^2\\quad\\text{sujeto a}\\quad 2-x\\leq 0.\n\\] Obtenga la función dual de Lagrange y verifique graficamente su convexidad. ¿Cuál es el valor de \\(d^*\\)?\nProbar que la función dual de Lagrange \\(\\calG(\\bflambda,\\bfmu)\\) es cóncava.\nMostrar que, si \\(\\xx\\) es factible primal y \\((\\bflambda,\\bfnu)\\) es factible dual, entonces \\((\\bflambda,\\bfnu)\\) es \\(\\epsilon\\)-subóptimo para el problema dual."
  },
  {
    "objectID": "CAPITULO_2/01_reg_lineal_logistica.html",
    "href": "CAPITULO_2/01_reg_lineal_logistica.html",
    "title": "Regresión lineal y logística",
    "section": "",
    "text": "\\[\n\\def\\RR{\\mathbb{R}}\n\\def\\media{\\mathbb{E}}\n\\def\\xx{{\\bf x}}\n\\def\\XX{{\\bf X}}\n\\def\\TT{{\\bf T}}\n\\def\\bftheta{\\boldsymbol{\\theta}}\n\\def\\bfeta{\\boldsymbol{\\eta}}\n\\def\\bfone{\\mathbf{1}}\n\\]"
  },
  {
    "objectID": "CAPITULO_2/01_reg_lineal_logistica.html#interpretación-probabilística",
    "href": "CAPITULO_2/01_reg_lineal_logistica.html#interpretación-probabilística",
    "title": "Regresión lineal y logística",
    "section": "1.1 Interpretación probabilística",
    "text": "1.1 Interpretación probabilística\nEn el modelo de regresión lineal, las variables predictoras \\(\\XX\\in\\RR^p\\) se relacionan con la variable respuesta \\(Y\\in\\RR\\) mediante la ecuación\n\\[\nY = \\theta_0+\\theta_1 X_1+\\theta_2 X_2+\\cdots+\\theta_p X_p + \\varepsilon,\n\\]\ndonde \\(\\bftheta=(\\theta_0,\\theta_1,\\cdots,\\theta_p)\\in\\RR^{p+1}\\) es el conjunto de parámetros del modelo y \\(\\varepsilon\\) es un término de error que captura efectos no modelados (como características omitidas o ruido aleatorio). Podemos escribir más abreviado el modelo bajo la convención \\(x_0=1\\), como:\n\\[\nY = \\bftheta^T \\XX + \\varepsilon,\n\\]\nAdemás, se asume que el error aleatorio \\(\\varepsilon\\) se distribuye según una distribución Normal con media cero y varianza \\(\\sigma^2\\), esto es\n\\[\n\\varepsilon^{(i)} \\sim \\mathcal{N}(0, \\sigma^2).\n\\]\nDe esta manera, la densidad de \\(\\varepsilon\\) es\n\\[\np(\\varepsilon) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{\\varepsilon^2}{2 \\sigma^2}\\right),\n\\]\nEsto implica que \\(Y|\\XX\\sim \\mathcal{N}(\\bftheta^T\\XX,\\sigma^2)\\) o, lo que es lo mismo, que la función de densidad condicional está dada por\n\\[\np(y |\\xx; \\bftheta) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{(y-\\bftheta^T \\xx)^2}{2 \\sigma^2}\\right).\n\\]\n\n\n1.1.1 Función de verosimilitud\nPara un conjunto de datos \\(\\{\\xx_i, y_i\\}_{i=1}^n\\) i.i.d., la función de verosimilitud es\n\\[\nL(\\bftheta) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{(y_i - \\bftheta^T\\xx_i)^2}{2 \\sigma^2}\\right).\n\\]\nAhora debemos aplicar el principio de máxima verosimilitud, que consiste en elegir \\(\\bftheta\\) de forma que los datos sean lo más probables posible (es decir, maximizar \\(L(\\bftheta)\\)). Sin embargo, es más conveniente maximizar el logaritmo de la verosimilitud, lo cual se denota con \\(\\ell(\\bftheta)\\). Resulta en este caso\n\\[\n\\begin{align*}\n\\ell(\\bftheta) &= \\log L(\\bftheta) \\\\\n&= \\log \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2 \\pi}\\sigma} \\exp\\left(-\\frac{(y_i - \\bftheta^T \\xx_i)^2}{2 \\sigma^2}\\right) \\\\\n&= \\sum_{i=1}^{n} \\left[\\log\\frac{1}{\\sqrt{2 \\pi}\\sigma} - \\frac{1}{2 \\sigma^2}  (y_i - \\bftheta^T \\xx_i)^2\\right]\\\\\n&= n  \\log \\frac{1}{\\sqrt{2 \\pi}\\sigma} - \\frac{1}{2 \\sigma^2} \\sum_{i=1}^{n} (y_i - \\bftheta^T \\xx_i)^2\n\\end{align*}\n\\]\nObservar en esta última expresión que maximizar \\(\\ell(\\theta)\\) es equivalente a minimizar\n\\[\nJ(\\bftheta)=\\frac{1}{2} \\sum_{i=1}^{n} (y_i - \\bftheta^T \\xx_i)^2,\n\\]\nlo cual es la función de costo asociada a mínimos cuadrados. El paramétro \\(\\hat{\\bftheta}\\) que minimiza dicha función está dado en forma cerrada por\n\\[\\hat{\\bftheta}=(X^TX)^{-1}X^T\\vec{y},\\]\ndonde \\(X\\in\\RR^{n\\times p}\\) es la matriz de datos y \\(\\vec{y}\\) el vector de respuestas."
  },
  {
    "objectID": "CAPITULO_2/01_reg_lineal_logistica.html#algoritmo-lms",
    "href": "CAPITULO_2/01_reg_lineal_logistica.html#algoritmo-lms",
    "title": "Regresión lineal y logística",
    "section": "1.2 Algoritmo LMS",
    "text": "1.2 Algoritmo LMS\nUn algoritmo de búsqueda del minimizador de \\(J(\\bftheta)\\) comienza con un valor inicial para \\(\\bftheta\\) y, luego, realiza una actualización iterativa de \\(\\bftheta\\) que pretende hacer el valor de \\(J(\\bftheta)\\) cada vez más pequeño. Específicamente, consideremos el algoritmo de descenso de gradiente que utiliza la siguiente actualización:\n\\[\n\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\bftheta).\n\\]\n(Esta actualización se realiza simultáneamente para todos los valores de \\(j = 0, \\dots, p\\)). Aquí, \\(\\alpha\\) se llama tasa de aprendizaje. Este es un algoritmo natural que repetidamente da un paso en la dirección del descenso más pronunciado de \\(J\\).\nPara implementar este algoritmo, necesitamos calcular el término de la derivada parcial en el lado derecho. Primero trabajemos en el caso donde solo tenemos un ejemplo de entrenamiento \\((\\xx, y)\\), de manera que podamos omitir la suma en la definición de \\(J\\). Tenemos:\n\\[\n\\frac{\\partial}{\\partial \\theta_j} J(\\bftheta) = \\frac{\\partial}{\\partial \\theta_j}\\left[ \\frac{1}{2} (y-\\bftheta^T\\xx)^2\\right] = (y-\\bftheta^T\\xx) x_j.\n\\]\nEsto da la regla de actualización:\n\\[\n\\theta_j := \\theta_j + \\alpha (y-\\bftheta^T\\xx) x_j.\n\\]\nEsta regla se llama la regla de actualización LMS (least mean squares), y también se conoce como la regla de aprendizaje Widrow-Hoff.\nLa magnitud de la actualización es proporcional al término de error \\((y - \\bftheta^T\\xx)\\). Si encontramos un ejemplo de entrenamiento en el que nuestra predicción coincide casi con el valor real de \\(y\\), entonces el cambio en los parámetros será mínimo.\nPara un conjunto de entrenamiento \\(\\{\\xx_i,y_i\\}_{i=1}^n\\), es fácil ver que la regla se puede escribir vectorialmente como\n\\[\n\\bftheta := \\bftheta + \\alpha \\sum_{i=1}^{n} (y_i - \\bftheta^T\\xx_i) \\xx_i.\n\\]\nEste método analiza cada ejemplo en todo el conjunto de entrenamiento en cada paso y se denomina descenso de gradiente por lotes (batch gradient descent)."
  },
  {
    "objectID": "CAPITULO_2/01_reg_lineal_logistica.html#ejemplo",
    "href": "CAPITULO_2/01_reg_lineal_logistica.html#ejemplo",
    "title": "Regresión lineal y logística",
    "section": "1.3 Ejemplo",
    "text": "1.3 Ejemplo\nEl conjunto de datos California Housing incluye información detallada sobre diversas características socioeconómicas y geográficas de diferentes áreas residenciales en California. El objetivo es predecir el valor medio de las viviendas en cada zona, a partir de factores como la densidad poblacional y los ingresos medios de los hogares, entre otros.\n\n\n\n\n\n\nLa clase LinearRegression usa la ecuación normal \\(\\hat{\\bftheta}=(X^TX)^{-1}X^T\\vec{y}\\). Para usar descenso de gradiente, la alternativa es la clase SGDRegressor: esta efectúa descenso de gradiente estocástico, que consiste en calcular el gradiente utilizando un dato a la vez."
  },
  {
    "objectID": "CAPITULO_2/01_reg_lineal_logistica.html#función-logística",
    "href": "CAPITULO_2/01_reg_lineal_logistica.html#función-logística",
    "title": "Regresión lineal y logística",
    "section": "2.1 Función logística",
    "text": "2.1 Función logística\nEn vez de trabajar con \\(h(\\xx)=\\bftheta^T\\xx\\) (regresión lineal), utilizaremos la siguiente expresión:\n\\[\nh_\\bftheta(\\xx) = g(\\bftheta^T \\xx) = \\frac{1}{1 + e^{-\\bftheta^T \\xx}},\n\\]\ndonde\n\\[ g(z) = \\frac{1}{1 + e^{-z}} \\]\nse llama función logística o función sigmoide.\n\n2.1.1 Gráfico de la función logística\nLa siguiente es una representación de \\(g(z)\\). Observar que \\(g(z)\\) tiende hacia 1 cuando \\(z \\to \\infty\\), mientras que tiende hacia 0 cuando \\(z \\to -\\infty\\). Además, \\(g(z)\\) está acotada entre 0 y 1."
  },
  {
    "objectID": "CAPITULO_2/01_reg_lineal_logistica.html#estimación-de-parámetros",
    "href": "CAPITULO_2/01_reg_lineal_logistica.html#estimación-de-parámetros",
    "title": "Regresión lineal y logística",
    "section": "2.2 Estimación de parámetros",
    "text": "2.2 Estimación de parámetros\nEntonces, dado el modelo de regresión logística, ¿cómo ajustamos \\(\\bftheta\\)? Como antes, obtengamos el estimador de máxima verosimilitud a partir de un conjunto de supuestos probabilísticos. Supongamos que\n\\[\n\\begin{align*}\nP(y = 1 | \\xx; \\bftheta) &= h_\\theta(\\xx),\n\\\\\nP(y = 0 | \\xx; \\bftheta) &= 1 - h_\\theta(\\xx).\n\\end{align*}\n\\]\nEsto puede escribirse de manera más compacta como:\n\\[\np(y | \\xx; \\bftheta) = \\left( h_\\bftheta(\\xx) \\right)^y \\left( 1 - h_\\bftheta(\\xx) \\right)^{1-y}.\n\\]\n\n2.2.1 Función de verosimilitud\nPara un conjunto de datos \\(\\{\\xx_i, y_i\\}_{i=1}^n\\) i.i.d., la función de verosimilitud es\n\\[\n\\begin{align*}\nL(\\bftheta) = \\prod_{i=1}^n \\left( h_\\bftheta(\\xx_i) \\right)^{y_i} \\left( 1 - h_\\bftheta(\\xx_i) \\right)^{1-y_i}.\n\\end{align*}\n\\]\nComo antes, será más sencillo maximizar el logaritmo de la verosimilitud:\n\\[\n\\begin{align*}\n\\ell(\\bftheta) &= \\log L(\\bftheta) \\\\\n&= \\sum_{i=1}^n \\left[y_i \\log h_\\theta(\\xx_i) + (1 - y_i) \\log \\left( 1 - h_\\theta(\\xx_i) \\right)\\right].\n\\end{align*}\n\\]\n\n\n2.2.2 Regla de ascenso por gradiente\n¿Cómo maximizamos la verosimilitud? Similar a nuestra derivación en el caso de la regresión lineal, podemos usar ascenso por gradiente. Las actualizaciones estarán dadas por\n\\[\n\\bftheta := \\bftheta + \\alpha \\nabla_\\bftheta \\ell(\\bftheta).\n\\]\nComo antes, comencemos trabajando con un único ejemplo de entrenamiento \\((\\xx,y)\\). Resulta: \\[\n\\begin{align*}\n\\frac{\\partial}{\\partial \\theta_j} \\ell(\\bftheta) & = \\frac{\\partial}{\\partial\\theta_j}\\left[y\\log h_{\\bftheta}(\\xx)+(1-y)\\log\\left(1-h_{\\bftheta}(\\xx)\\right)\\right]\\\\\n&=\n\\left[ y \\frac{1}{g(\\bftheta^T \\xx)} - (1 - y) \\frac{1}{1 - g(\\bftheta^T \\xx)} \\right]\n\\frac{\\partial}{\\partial \\theta_j} g(\\bftheta^T \\xx)\n\\\\\n&=\n\\left[ y \\frac{1}{g(\\bftheta^T \\xx)} - (1 - y) \\frac{1}{1 - g(\\bftheta^T \\xx)} \\right]\ng'(\\bftheta^T\\xx)x_j\n\\\\\n&= \\left[ y \\frac{1}{g(\\bftheta^T \\xx)} - (1 - y) \\frac{1}{1 - g(\\bftheta^T \\xx)} \\right]\ng(\\bftheta^T \\xx) (1 - g(\\bftheta^T \\xx)) x_j\n\\\\\n&= \\left[ y (1 - g(\\bftheta^T \\xx)) - (1 - y) g(\\bftheta^T \\xx) \\right] x_j\n\\\\\n&= (y - h_\\bftheta(\\xx)) x_j.\n\\end{align*}\n\\]\nEn los cálculos anteriores hemos usado el hecho de que \\(g'(z) = g(z)(1 - g(z))\\). Esto nos da la regla de ascenso de gradiente estocástico:\n\\[\n\\theta_j := \\theta_j + \\alpha \\left( y_i - h_\\bftheta(\\xx_i) \\right) x_{ij},\n\\]\ndonde \\(x_{ij}\\) es el \\(j\\)-ésimo elemento de la observación \\(\\xx_i\\). Si comparamos esto con la regla de actualización de LMS, podemos notar una cierta similaridad entre los factores \\((y_i-\\bftheta^T\\xx_i)\\) y \\((y_i-h_\\bftheta(\\xx))\\). ¿Es esto coincidencia? Veremos este hecho cuando veamos los modelos lineales generalizados."
  },
  {
    "objectID": "CAPITULO_2/01_reg_lineal_logistica.html#ejemplo-1",
    "href": "CAPITULO_2/01_reg_lineal_logistica.html#ejemplo-1",
    "title": "Regresión lineal y logística",
    "section": "2.3 Ejemplo",
    "text": "2.3 Ejemplo\nEl conjunto de datos Breast Cancer contiene información sobre 569 muestras de tejido mamario, con 30 características que describen propiedades de las células nucleares de los tumores. El objetivo principal es clasificar los tumores como benignos o malignos.\n\n\n\n\n\n\n\n\nEjercicios\n\n\nProbar que la derivada de la función logística \\(g(z)=1/(1+e^{-z})\\) verifica \\[g'(z)(1-g(z)).\\]\nMostrar que en los GLMs la derivada de la log-likelihood es \\[\n\\nabla_\\theta \\ell(\\theta) = \\sum_{i=1}^n (T(y^{(i)}) - \\mathbb{E}[T(y) | \\xx^{(i)}; \\theta]) \\xx^{(i)}.\n\\]"
  },
  {
    "objectID": "CAPITULO_2/03_optimizacion_ML.html",
    "href": "CAPITULO_2/03_optimizacion_ML.html",
    "title": "Optimización en regresión y clasificación",
    "section": "",
    "text": "\\[\n\\def\\RR{\\mathbb{R}}\n\\def\\media{\\mathbb{E}}\n\\def\\xx{{\\bf x}}\n\\def\\XX{{\\bf X}}\n\\def\\TT{{\\bf T}}\n\\def\\bftheta{\\boldsymbol{\\theta}}\n\\def\\bfeta{\\boldsymbol{\\eta}}\n\\def\\bfone{\\mathbf{1}}\n\\]\n\n\n\n1 First"
  },
  {
    "objectID": "CAPITULO_2/05_clustering.html",
    "href": "CAPITULO_2/05_clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "\\[\n\\def\\RR{\\mathbb{R}}\n\\def\\media{\\mathbb{E}}\n\\def\\xx{{\\bf x}}\n\\def\\XX{{\\bf X}}\n\\def\\TT{{\\bf T}}\n\\def\\bftheta{\\boldsymbol{\\theta}}\n\\def\\bfeta{\\boldsymbol{\\eta}}\n\\def\\bfone{\\mathbf{1}}\n\\]\n\n\n\n1 First"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Página de Inicio",
    "section": "",
    "text": "Bienvenidos al curso de Optimización. Aquí encontrarás los capítulos y secciones.\n\n\n\nSección 1: A1_intro_optimizacion\nSección 2: A2_optimalidad\nSección 3: A3_metodos_optimizacion"
  },
  {
    "objectID": "index.html#capítulo-1",
    "href": "index.html#capítulo-1",
    "title": "Página de Inicio",
    "section": "",
    "text": "Sección 1: A1_intro_optimizacion\nSección 2: A2_optimalidad\nSección 3: A3_metodos_optimizacion"
  }
]