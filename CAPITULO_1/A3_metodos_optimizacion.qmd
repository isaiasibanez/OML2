---
title: "Métodos de optimización"
---

<hr style="border: 1px solid rgba(50, 0, 0, 1);">

{{< include ../macros.qmd >}}



Comenzaremos explorando métodos de optimización de primer orden para problemas no lineales. En principio, consideraremos el caso sin restricciones
$$
\min_{\xx\in\RR^n} f(\xx),
$$

con $f$ una función diferenciable. En este caso, ya hemos visto que $\xx^*\in\RR^n$ resuelve el problema de optimización solo si $\nabla f(\xx^*)=\mathbf{0}$. Más aún, cuando $f$ es convexa, la condición es también suficiente.

Una idea fundamental para construir un algoritmo de optimización es aproximar $f$ con modelos más simples y manejables. En particular, los métodos de primer orden se basan en la aproximación de Taylor de primer orden, alrededor de un punto $\xx_t$, a saber:
$$
f(\xx) \approx f(\xx_{t}) + \langle \nabla f(\xx_{t}), \xx - \xx_{t} \rangle.
$$


# Método de gradiente descendente

La idea del descenso de gradiente consiste en actualizar la posición actual moviéndose en la dirección que reduce más rápidamente el valor de la función objetivo. Es decir, se avanza una distancia $\eta>0$ en la dirección $-\nabla f(\xx_t)$, que señala el sentido de máximo descenso de la función. En consecuencia, se define la iteración

$$
\boxed{\xx_{t+1} := \xx_{t} - \eta \nabla f(\xx_{t})}.
$$

El parámetro $\eta>0$ se denomina *tamaño de paso* o *tasa de aprendizaje*. Intuitivamente, la magnitud segura de $\eta$ debe determinarse en función de que tan cerca de $\xx_t$ la aproximación de primer orden es buena. La clave está dada por la *suavidad* de $f$: es preciso requerir una cota para la velocidad con la cual el vector gradiente puede cambiar al movernos ligeramente en alguna dirección. En otras palabras, si al comenzar a movernos en la dirección $-\nabla f(\xx_t)$, desde $\xx_t$, los nuevos gradientes que encontramos en el camino siguen estando mayormente alineados con $\nabla f(\xx_t)$, entonces $-\nabla f(\xx_t)$ seguría siendo una buena dirección de descenso por un tiempo. Esto sugiere que el tamaño de paso $\eta$ puede ser elegido con un valor significativo, no necesariamente muy cercano a cero, lo que favorece un avance eficiente en la optimización.


## Condición de Lipschitz para el gradiente

Específicamente, requeriremos que el gradiente $\nabla f(x)$ sea $L$-Lipschitz continuo para alguna constante $L \geq 0$. Esta condición a menudo se llama $L$_-suavidad_ en la literatura. La presentamos ahora para funciones generales con dominios convexos arbitrarios $\Omega$; hoy solo nos importará el caso $\Omega = \mathbb{R}^{n}$.


::: {.definicion}
**Definición 1.** ($L$-suavidad) Sea $\Omega$ un conjunto convexo. Una función diferenciable $f: \Omega \to \RR$ es $L$-suave si su gradiente es $L$-Lipschitz continuo. Es decir, se verifica
$$
\|\nabla f(\xx_1) - \nabla f(\xx_2)\|_{2} \leq L \|\xx_1 - \xx_2\|_{2}, \qquad \forall \xx_1, \xx_2 \in \Omega.
$$
:::

Una consecuencia inmediata de la $L$-suavidad es que la función admite una _cota superior cuadrática_. Esta propiedad será extremadamente útil en el análisis.

**Teorema 1.12.1** (Cota superior cuadrática). Sea $f: \Omega \to \mathbb{R}$ una función $L$-suave en un dominio convexo $\Omega$. Entonces, podemos acotar superiormente la función $f$ como:

$$
f(y) \leq f(x) + \langle \nabla f(x), y - x \rangle + \frac{L}{2} \|y - x\|_{2}^{2} \quad \forall x, y \in \Omega. \quad \text{(2)}
$$

**Demostración.** La idea es simple: expresamos el crecimiento $f(y) - f(x)$ como la integral del gradiente en la línea que conecta $x$ con $y$, y luego usamos el límite de Lipschitz en el crecimiento del gradiente $\nabla f$ para acotar el crecimiento:

\begin{align*}
f(y) - f(x) &= \int_{0}^{1} \langle \nabla f(x + t \cdot (y - x)), y - x \rangle \, \mathrm{d}t\\&
= \left( \int_{0}^{1} \langle \nabla f(x + t \cdot (y - x)) - \nabla f(x), y - x \rangle \, \mathrm{d}t \right) + \langle \nabla f(x), y - x \rangle\\
&
\leq \left( \int_{0}^{1} \|\nabla f(x + t \cdot (y - x)) - \nabla f(x)\|_{2} \cdot \|y - x\|_{2} \, \mathrm{d}t \right) + \langle \nabla f(x), y - x \rangle\\
&
\leq \left( \int_{0}^{1} t L \|y - x\|_{2}^{2} \, \mathrm{d}t \right) + \langle \nabla f(x), y - x \rangle\\
&= \frac{L}{2} \|y - x\|_{2}^{2} + \langle \nabla f(x), y - x \rangle.
\end{align*}
Reordenando, obtenemos el enunciado. $\blacksquare$

También mencionamos la siguiente caracterización.

**Teorema 1.12.2.** Para funciones dos veces diferenciables $f: \Omega \to \mathbb{R}$ definidas en un conjunto abierto $\Omega \subseteq \mathbb{R}^{n}$, una condición equivalente de $L$-suavidad es:

$$
-L I \preceq \nabla^{2} f(x) \preceq L I \quad \forall x \in \Omega,
$$

o equivalentemente,

$$
|v^{\top} \nabla^{2} f(x) v| \leq L \quad \forall x \in \Omega, v \in \mathbb{R}^{n}: \|v\|_{2} = 1.
$$





# Gradiente descendente estocástico





# Método de Newton














<div style="height:10cm;"></div>


<br>
<hr style="border: 2.5px solid black;">

<h2>Ejercicios</h2>

1. ...




