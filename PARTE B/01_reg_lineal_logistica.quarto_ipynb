{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Regresión lineal y logística\"\n",
        "lang: es\n",
        "format:\n",
        "  live-html:\n",
        "    pyodide:\n",
        "      cell-options:\n",
        "        edit: true\n",
        "        include: true\n",
        "    css: style.css\n",
        "    toc: true\n",
        "    toc-depth: 3\n",
        "    toc-location: left\n",
        "    toc-floating: true\n",
        "number-sections: true\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "::: {.hidden} \n",
        "$$\n",
        "\\def\\RR{\\mathbb{R}}\n",
        "\\def\\media{\\mathbb{E}}\n",
        "\\def\\xx{{\\bf x}}\n",
        "\\def\\XX{{\\bf X}}\n",
        "\\def\\TT{{\\bf T}}\n",
        "\\def\\bftheta{\\boldsymbol{\\theta}}\n",
        "\\def\\bfeta{\\boldsymbol{\\eta}}\n",
        "\\def\\bfone{\\mathbf{1}}\n",
        "$$ \n",
        ":::\n",
        "\n",
        "<hr style=\"border: 1px solid rgba(50, 0, 0, 1);\">\n",
        "\n",
        "\n",
        "\n",
        "# Regresión lineal\n",
        "\n",
        "## Interpretación probabilística\n",
        "\n",
        "En el modelo de regresión lineal, las variables predictoras $\\XX\\in\\RR^p$ se relacionan con la variable respuesta $Y\\in\\RR$ mediante la ecuación\n",
        "\n",
        "$$\n",
        "Y = \\theta_0+\\theta_1 X_1+\\theta_2 X_2+\\cdots+\\theta_p X_p + \\varepsilon,\n",
        "$$\n",
        "\n",
        "donde $\\bftheta=(\\theta_0,\\theta_1,\\cdots,\\theta_p)\\in\\RR^{p+1}$ es el conjunto de parámetros del modelo y $\\varepsilon$ es un término de error que captura efectos no modelados (como características omitidas o ruido aleatorio). Podemos escribir más abreviado el modelo bajo la convención $x_0=1$, como:\n",
        "\n",
        "$$\n",
        "Y = \\bftheta^T \\XX + \\varepsilon,\n",
        "$$\n",
        "\n",
        "Además, se asume que el error aleatorio $\\varepsilon$ se distribuye según una distribución Normal con media cero y varianza $\\sigma^2$, esto es\n",
        "\n",
        "$$\n",
        "\\varepsilon^{(i)} \\sim \\mathcal{N}(0, \\sigma^2).\n",
        "$$\n",
        "\n",
        "De esta manera, la densidad de $\\varepsilon$ es\n",
        "\n",
        "$$\n",
        "p(\\varepsilon) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{\\varepsilon^2}{2 \\sigma^2}\\right),\n",
        "$$\n",
        "\n",
        "Esto implica que $Y|\\XX\\sim \\mathcal{N}(\\bftheta^T\\XX,\\sigma^2)$ o, lo que es lo mismo, que la función de densidad condicional está dada por\n",
        "\n",
        "\n",
        "$$\n",
        "p(y |\\xx; \\bftheta) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{(y-\\bftheta^T \\xx)^2}{2 \\sigma^2}\\right).\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Función de verosimilitud\n",
        "\n",
        "Para un conjunto de datos $\\{\\xx_i, y_i\\}_{i=1}^n$ i.i.d., la función de verosimilitud es\n",
        "\n",
        "$$\n",
        "L(\\bftheta) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{(y_i - \\bftheta^T\\xx_i)^2}{2 \\sigma^2}\\right).\n",
        "$$\n",
        "\n",
        "Ahora debemos aplicar el principio de **máxima verosimilitud**, que consiste en elegir $\\bftheta$ de forma que los datos sean lo más probables posible (es decir, maximizar $L(\\bftheta)$). Sin embargo, es más conveniente maximizar el logaritmo de la verosimilitud, lo cual se denota con $\\ell(\\bftheta)$. Resulta en este caso\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\ell(\\bftheta) &= \\log L(\\bftheta) \\\\\n",
        "&= \\log \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2 \\pi}\\sigma} \\exp\\left(-\\frac{(y_i - \\bftheta^T \\xx_i)^2}{2 \\sigma^2}\\right) \\\\\n",
        "&= \\sum_{i=1}^{n} \\left[\\log\\frac{1}{\\sqrt{2 \\pi}\\sigma} - \\frac{1}{2 \\sigma^2}  (y_i - \\bftheta^T \\xx_i)^2\\right]\\\\\n",
        "&= n  \\log \\frac{1}{\\sqrt{2 \\pi}\\sigma} - \\frac{1}{2 \\sigma^2} \\sum_{i=1}^{n} (y_i - \\bftheta^T \\xx_i)^2\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Observar en esta última expresión que maximizar $\\ell(\\theta)$ es equivalente a minimizar\n",
        "\n",
        "$$\n",
        "J(\\bftheta)=\\frac{1}{2} \\sum_{i=1}^{n} (y_i - \\bftheta^T \\xx_i)^2,\n",
        "$$\n",
        "\n",
        "lo cual es la función de costo asociada a mínimos cuadrados. El paramétro $\\hat{\\bftheta}$ que minimiza dicha función está dado en forma cerrada por\n",
        "\n",
        "$$\\hat{\\bftheta}=(X^TX)^{-1}X^T\\vec{y},$$\n",
        "\n",
        "donde $X\\in\\RR^{n\\times p}$ es la matriz de datos y $\\vec{y}$ el vector de respuestas.\n",
        "\n",
        "\n",
        "## Algoritmo LMS\n",
        "\n",
        "Un algoritmo de búsqueda del minimizador de $J(\\bftheta)$ comienza con un valor inicial para $\\bftheta$ y, luego, realiza una actualización iterativa de $\\bftheta$ que pretende hacer el valor de $J(\\bftheta)$ cada vez más pequeño. Específicamente, consideremos el algoritmo de **descenso de gradiente** que utiliza la siguiente actualización:\n",
        "\n",
        "$$\n",
        "\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\bftheta).\n",
        "$$\n",
        "\n",
        "(Esta actualización se realiza simultáneamente para todos los valores de $j = 0, \\dots, p$). Aquí, $\\alpha$ se llama **tasa de aprendizaje**. Este es un algoritmo natural que repetidamente da un paso en la dirección del descenso más pronunciado de $J$.\n",
        "\n",
        "\n",
        "Para implementar este algoritmo, necesitamos calcular el término de la derivada parcial en el lado derecho. Primero trabajemos en el caso donde solo tenemos un ejemplo de entrenamiento $(\\xx, y)$, de manera que podamos omitir la suma en la definición de $J$. Tenemos:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial}{\\partial \\theta_j} J(\\bftheta) = \\frac{\\partial}{\\partial \\theta_j}\\left[ \\frac{1}{2} (y-\\bftheta^T\\xx)^2\\right] = (y-\\bftheta^T\\xx) x_j.\n",
        "$$\n",
        "\n",
        "Esto da la regla de actualización:\n",
        "\n",
        "$$\n",
        "\\theta_j := \\theta_j + \\alpha (y-\\bftheta^T\\xx) x_j.\n",
        "$$\n",
        "\n",
        "Esta regla se llama la **regla de actualización LMS** (*least mean squares*), y también se conoce como la **regla de aprendizaje Widrow-Hoff**.\n",
        "\n",
        "\n",
        "La magnitud de la actualización es proporcional al término de error $(y - \\bftheta^T\\xx)$. Si encontramos un ejemplo de entrenamiento en el que nuestra predicción coincide casi con el valor real de $y$, entonces el cambio en los parámetros será mínimo.\n",
        "\n",
        "Para un conjunto de entrenamiento $\\{\\xx_i,y_i\\}_{i=1}^n$, es fácil ver que la regla se puede escribir vectorialmente como\n",
        "\n",
        "$$\n",
        "\\bftheta := \\bftheta + \\alpha \\sum_{i=1}^{n} (y_i - \\bftheta^T\\xx_i) \\xx_i.\n",
        "$$\n",
        "\n",
        "Este método analiza cada ejemplo en todo el conjunto de entrenamiento en cada paso y se denomina **descenso de gradiente por lotes** (*batch gradient descent*).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Ejemplo\n",
        "\n",
        "El conjunto de datos `California Housing` incluye información detallada sobre diversas características socioeconómicas y geográficas de diferentes áreas residenciales en California. El objetivo es predecir el valor medio de las viviendas en cada zona, a partir de factores como la densidad poblacional y los ingresos medios de los hogares, entre otros.\n",
        "\n",
        "\n",
        "```{pyodide}\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
        "y = pd.Series(housing.target, name='MedHouseVal')\n",
        "\n",
        "\n",
        "# Train-test split y escalado:\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# Regresion lineal:\n",
        "modelo = LinearRegression()\n",
        "\n",
        "modelo.fit(X_train_scaled, y_train)  # Entrenamiento\n",
        "y_test_fit = modelo.predict(X_test_scaled)  # Prediccion\n",
        "MSE = mean_squared_error(y_test, y_test_fit)  # Evaluacion\n",
        "\n",
        "theta0 = modelo.intercept_\n",
        "theta = modelo.coef_\n",
        "\n",
        "print(f\"Cantidad de predictores: {X.shape[1]}\")\n",
        "print(f\"Intercepto: {theta0}\")\n",
        "print(f\"Coeficientes del modelo: {theta}\")\n",
        "```\n",
        "\n",
        "\n",
        "La clase `LinearRegression` usa la ecuación normal $\\hat{\\bftheta}=(X^TX)^{-1}X^T\\vec{y}$. Para usar descenso de gradiente, la alternativa es la clase `SGDRegressor`: esta efectúa **descenso de gradiente estocástico**, que consiste en calcular el gradiente utilizando un dato a la vez.\n",
        "\n",
        "\n",
        "```{pyodide}\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
        "y = pd.Series(housing.target, name='MedHouseVal')\n",
        "\n",
        "\n",
        "# Train-test split y escalado:\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# Regresion lineal con SGDRegressor:\n",
        "modelo = SGDRegressor(max_iter=1000, tol=1e-3, eta0=0.01, random_state=42)\n",
        "\n",
        "modelo.fit(X_train_scaled, y_train)  # Entrenamiento\n",
        "y_test_fit = modelo.predict(X_test_scaled)  # Prediccion\n",
        "MSE = mean_squared_error(y_test, y_test_fit)  # Evaluacion\n",
        "\n",
        "theta0 = modelo.intercept_\n",
        "theta = modelo.coef_\n",
        "\n",
        "print(f\"Cantidad de predictores: {X.shape[1]}\")\n",
        "print(f\"Intercepto: {theta0}\")\n",
        "print(f\"Coeficientes del modelo: {theta}\")\n",
        "```\n",
        "\n",
        "\n",
        "# Regresión logística\n",
        "\n",
        "Ahora hablemos sobre el problema de clasificación. En este caso, la variable respuesta toma valores discretos, es decir $Y\\in\\{0,1,\\cdots,r\\}$. En particular, nos enfocaremos en el problema de **clasificación binaria** en el que $Y\\in\\{0,1\\}$. \n",
        "\n",
        "Por ejemplo, si estamos tratando de construir un clasificador de spam para correos electrónicos, entonces una observación $\\xx$ contendrá características de un correo electrónico, mientras que su **etiqueta** $y$ será 1 si es spam o 0 en caso contrario. \n",
        "\n",
        "\n",
        "Podríamos abordar el problema de clasificación ignorando el hecho de que $y$ toma valores discretos, y usar nuestro antiguo algoritmo de regresión lineal para tratar de predecir $y$ dado $\\xx$. Sin embargo, esto suele tener un pobre desempeño y además no tiene sentido que la predicción para un valor de $\\xx$ sea un valor fuera del rango de 0 y 1.\n",
        "\n",
        "Para resolver este problema, podemos usar la **regresión logística**, donde la predicción se realiza con una función logística o sigmoide.\n",
        "\n",
        "\n",
        "## Función logística\n",
        "\n",
        "En vez de trabajar con $h(\\xx)=\\bftheta^T\\xx$ (regresión lineal), utilizaremos la siguiente expresión:\n",
        "\n",
        "$$\n",
        "h_\\bftheta(\\xx) = g(\\bftheta^T \\xx) = \\frac{1}{1 + e^{-\\bftheta^T \\xx}},\n",
        "$$\n",
        "\n",
        "donde\n",
        "\n",
        "$$ g(z) = \\frac{1}{1 + e^{-z}} $$\n",
        "\n",
        "se llama  **función logística** o **función sigmoide**.\n",
        "\n",
        "### Gráfico de la función logística\n",
        "\n",
        "La siguiente es una representación de $g(z)$. Observar que $g(z)$ tiende hacia 1 cuando $z \\to \\infty$, mientras que tiende hacia 0 cuando $z \\to -\\infty$. Además, $g(z)$ está acotada entre 0 y 1.\n",
        "\n",
        "\n",
        "```{pyodide}\n",
        "#| edit: false\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Definir la función sigmoide\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Crear valores de z para graficar\n",
        "z = np.linspace(-10, 10, 500)\n",
        "g_z = sigmoid(z)\n",
        "\n",
        "# Graficar la función sigmoide\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(z, g_z, label=\"$g(z) = \\\\frac{1}{1 + e^{-z}}$\")\n",
        "plt.axhline(1, color=\"gray\", linestyle=\"--\", linewidth=0.7)\n",
        "plt.axhline(0, color=\"gray\", linestyle=\"--\", linewidth=0.7)\n",
        "plt.axvline(0, color=\"gray\", linestyle=\"--\", linewidth=0.7)\n",
        "plt.title(\"Función Logística (Sigmoide)\")\n",
        "plt.xlabel(\"$z$\")\n",
        "plt.ylabel(\"$g(z)$\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Estimación de parámetros\n",
        "\n",
        "Entonces, dado el modelo de regresión logística, ¿cómo ajustamos $\\bftheta$? Como antes, obtengamos el estimador de máxima verosimilitud a partir de un conjunto de supuestos probabilísticos. Supongamos que\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "P(y = 1 | \\xx; \\bftheta) &= h_\\theta(\\xx),\n",
        "\\\\\n",
        "P(y = 0 | \\xx; \\bftheta) &= 1 - h_\\theta(\\xx).\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Esto puede escribirse de manera más compacta como:\n",
        "\n",
        "$$\n",
        "p(y | \\xx; \\bftheta) = \\left( h_\\bftheta(\\xx) \\right)^y \\left( 1 - h_\\bftheta(\\xx) \\right)^{1-y}.\n",
        "$$\n",
        "\n",
        "\n",
        "### Función de verosimilitud\n",
        "\n",
        "Para un conjunto de datos $\\{\\xx_i, y_i\\}_{i=1}^n$ i.i.d., la función de verosimilitud es\n",
        "\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "L(\\bftheta) = \\prod_{i=1}^n \\left( h_\\bftheta(\\xx_i) \\right)^{y_i} \\left( 1 - h_\\bftheta(\\xx_i) \\right)^{1-y_i}.\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Como antes, será más sencillo maximizar el logaritmo de la verosimilitud:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\ell(\\bftheta) &= \\log L(\\bftheta) \\\\\n",
        "&= \\sum_{i=1}^n \\left[y_i \\log h_\\theta(\\xx_i) + (1 - y_i) \\log \\left( 1 - h_\\theta(\\xx_i) \\right)\\right].\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "\n",
        "### Regla de ascenso por gradiente\n",
        "\n",
        "¿Cómo maximizamos la verosimilitud? Similar a nuestra derivación en el caso de la regresión lineal, podemos usar **ascenso por gradiente**. Las actualizaciones estarán dadas por\n",
        "\n",
        "$$\n",
        "\\bftheta := \\bftheta + \\alpha \\nabla_\\bftheta \\ell(\\bftheta).\n",
        "$$\n",
        "\n",
        "Como antes, comencemos trabajando con un único ejemplo de entrenamiento $(\\xx,y)$. Resulta:\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\frac{\\partial}{\\partial \\theta_j} \\ell(\\bftheta) & = \\frac{\\partial}{\\partial\\theta_j}\\left[y\\log h_{\\bftheta}(\\xx)+(1-y)\\log\\left(1-h_{\\bftheta}(\\xx)\\right)\\right]\\\\\n",
        "&= \n",
        "\\left[ y \\frac{1}{g(\\bftheta^T \\xx)} - (1 - y) \\frac{1}{1 - g(\\bftheta^T \\xx)} \\right]\n",
        "\\frac{\\partial}{\\partial \\theta_j} g(\\bftheta^T \\xx)\n",
        "\\\\\n",
        "&= \n",
        "\\left[ y \\frac{1}{g(\\bftheta^T \\xx)} - (1 - y) \\frac{1}{1 - g(\\bftheta^T \\xx)} \\right]\n",
        "g'(\\bftheta^T\\xx)x_j\n",
        "\\\\\n",
        "&= \\left[ y \\frac{1}{g(\\bftheta^T \\xx)} - (1 - y) \\frac{1}{1 - g(\\bftheta^T \\xx)} \\right]\n",
        "g(\\bftheta^T \\xx) (1 - g(\\bftheta^T \\xx)) x_j\n",
        "\\\\\n",
        "&= \\left[ y (1 - g(\\bftheta^T \\xx)) - (1 - y) g(\\bftheta^T \\xx) \\right] x_j\n",
        "\\\\\n",
        "&= (y - h_\\bftheta(\\xx)) x_j.\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "En los cálculos anteriores hemos usado el hecho de que $g'(z) = g(z)(1 - g(z))$. Esto nos da la regla de **ascenso de gradiente estocástico**:\n",
        "\n",
        "$$\n",
        "\\theta_j := \\theta_j + \\alpha \\left( y_i - h_\\bftheta(\\xx_i) \\right) x_{ij},\n",
        "$$\n",
        "\n",
        "donde $x_{ij}$ es el $j$-ésimo elemento de la observación $\\xx_i$. Si comparamos esto con la regla de actualización de LMS, podemos notar una cierta similaridad entre los factores $(y_i-\\bftheta^T\\xx_i)$ y $(y_i-h_\\bftheta(\\xx))$. ¿Es esto coincidencia? Veremos este hecho cuando veamos los modelos lineales generalizados.\n",
        "\n",
        "\n",
        "\n",
        "## Ejemplo \n",
        "\n",
        "El conjunto de datos `Breast Cancer` contiene información sobre 569 muestras de tejido mamario, con 30 características que describen propiedades de las células nucleares de los tumores. El objetivo principal es clasificar los tumores como benignos o malignos.\n",
        "\n",
        "\n",
        "```{pyodide}\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Cargar los datos\n",
        "cancer = load_breast_cancer()\n",
        "X = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
        "y = pd.Series(cancer.target, name='Malignant')\n",
        "\n",
        "# Train-test split y escalado:\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Regresión logística:\n",
        "modelo = LogisticRegression(max_iter=10000)\n",
        "\n",
        "modelo.fit(X_train_scaled, y_train)  # Entrenamiento\n",
        "y_test_fit = modelo.predict(X_test_scaled)  # Predicción\n",
        "\n",
        "# Evaluación:\n",
        "accuracy = accuracy_score(y_test, y_test_fit)\n",
        "\n",
        "# Resultados:\n",
        "coeficientes = modelo.coef_\n",
        "intercepto = modelo.intercept_\n",
        "\n",
        "print(f\"Cantidad de predictores: {X.shape[1]}\")\n",
        "print(f\"Intercepto: {intercepto}\")\n",
        "print(f\"Coeficientes del modelo: {coeficientes}\")\n",
        "print(f\"Precision del modelo: {accuracy}\")\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "<h2>Ejercicios</h2>\n",
        "\n",
        "1. Probar que la derivada de la función logística $g(z)=1/(1+e^{-z})$ verifica\n",
        "$$g'(z)(1-g(z)).$$\n",
        "\n",
        "1. Mostrar que en los GLMs la derivada de la log-likelihood es\n",
        "$$\n",
        "\\nabla_\\theta \\ell(\\theta) = \\sum_{i=1}^n (T(y^{(i)}) - \\mathbb{E}[T(y) | \\xx^{(i)}; \\theta]) \\xx^{(i)}.\n",
        "$$"
      ],
      "id": "975169e2"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\Users\\isaias\\AppData\\Local\\Programs\\Python\\Python313\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}